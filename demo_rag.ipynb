{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain.llms import AzureOpenAI\n",
    "# from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.chat_models import openai\n",
    "from openai import AzureOpenAI\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "import requests\n",
    "import os\n",
    "from datetime import datetime\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import faiss\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_KEY\"] = \"***\"\n",
    "os.environ[\"AZURE_OPENAI_KEY\"] = os.environ[\"OPENAI_KEY\"]\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = os.environ[\"OPENAI_KEY\"]\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://***.openai.azure.com/\"\n",
    "os.environ[\"AZURE_OPENAI_API_VERSION\"] = \"2024-02-01\"\n",
    "os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"] = \"gpt-35-turbo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AzureOpenAI(\n",
    "  azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"), \n",
    "  api_key=os.getenv(\"AZURE_OPENAI_KEY\"),  \n",
    "  api_version=\"2024-02-01\" #2024-02-15-preview ?\n",
    ")\n",
    "\n",
    "message_text = [{\"role\":\"system\",\"content\":\"You are an AI assistant that helps people find information.\"}]\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-35-turbo\", # model = \"deployment_name\"\n",
    "  messages = message_text,\n",
    "  temperature=0.7,\n",
    "  max_tokens=800,\n",
    "  top_p=0.95,\n",
    "  frequency_penalty=0,\n",
    "  presence_penalty=0,\n",
    "  stop=None\n",
    ")\n",
    "\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_version=\"2024-02-01\",\n",
    "    azure_deployment=\"gpt-35-turbo\",\n",
    ")\n",
    "\n",
    "llm4 = AzureChatOpenAI(\n",
    "    openai_api_version=\"2024-02-01\",\n",
    "    azure_deployment=\"gpt-4o\",\n",
    ")\n",
    "\n",
    "sentence_to_translate = \"I want to evaluate ChatGPT's answers.\"\n",
    "\n",
    "message = HumanMessage(\n",
    "    content=f\"Translate this sentence from English to French : {sentence_to_translate}\"\n",
    ")\n",
    "llm.invoke([message])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "def translate(sentence):\n",
    "\n",
    "    translate_prompt_template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a helpful assistant who translates the following sentence from english to french.\"),\n",
    "        (\"human\", \"{sentence}\"),\n",
    "    ])\n",
    "\n",
    "    translate_chain = translate_prompt_template | llm4\n",
    "\n",
    "    translation = translate_chain.invoke({\"sentence\": sentence}).content\n",
    "    # print(translation)\n",
    "    return translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(sentence_to_translate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install unstructured\n",
    "# !pip install \"unstructured[pdf]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "\n",
    "directory_path = \"microsoft_learn\"\n",
    "\n",
    "loader = DirectoryLoader(directory_path)\n",
    "pages = loader.load_and_split()\n",
    "pages\n",
    "\n",
    "# partition_pdf is not available. Install the pdf dependencies with pip install \"unstructured[pdf]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAG : recherche par similarité (en utilisant les embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract text from pdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pdfminer.six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pdfminer.high_level import extract_text\n",
    "\n",
    "\n",
    "# dirs = os.listdir(directory_path)\n",
    "# output = \"pdftotext.txt\"\n",
    "# # supprimer le fichier s'il existe déjà\n",
    "# if os.path.exists(output):  \n",
    "#     os.remove(output) \n",
    "# for file in dirs:\n",
    "#     path_file = os.path.join(directory_path, file)\n",
    "#     print(f\"PDF path file: {path_file}\")\n",
    "#     text = extract_text(path_file)\n",
    "#     with open(output, 'a', encoding='utf-8') as fichier:\n",
    "#         fichier.write(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader, DirectoryLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "\n",
    "# loader = TextLoader(\"pdftotext.txt\", encoding='utf-8')\n",
    "# loader = PyPDFLoader(\"microsoft_learn/intelligence_artificielle.pdf\")\n",
    "loader = DirectoryLoader(\"microsoft_learn/\", glob=\"**/*.pdf\", loader_cls=PyPDFLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader multi files types\n",
    "# https://github.com/langchain-ai/langchain/discussions/18559"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents= loader.load()\n",
    "for document in documents:\n",
    "    document.metadata[\"filename\"] = Path(document.metadata[\"source\"]).name\n",
    "    document.metadata[\"loading_date\"] = datetime.today().strftime('%Y%m%d')\n",
    "    document.metadata[\"user\"] = Path(document.metadata[\"source\"]).parent.name\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk settings  \n",
    "chunk_size = 200\n",
    "chunk_overlap = 30 # duplication des informations entre les vecteurs coupés "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "# text_splitter = CharacterTextSplitter(\n",
    "#                     chunk_size=chunk_size,\n",
    "#                     chunk_overlap=chunk_overlap,\n",
    "#                     separator=\"\\n\",\n",
    "#                 )\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "                    chunk_size=chunk_size,\n",
    "                    chunk_overlap=chunk_overlap,\n",
    "                    separators=[\"\\n\"],\n",
    "                )\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "nb_chunks = len(chunks)\n",
    "print(f\"Nb of chunks: {nb_chunks}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chunks[nb_chunks-1].metadata[\"user\"])\n",
    "print(chunks[nb_chunks-1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://stackoverflow.com/questions/76886954/multiple-file-loading-and-embeddings-with-openai\n",
    "\n",
    "# print(\"Loading data...\")\n",
    "# pdf_folder_path = \"content/\"\n",
    "# print(os.listdir(pdf_folder_path))\n",
    "\n",
    "# # Load multiple files\n",
    "# loaders = [UnstructuredPDFLoader(os.path.join(pdf_folder_path, fn)) for fn in os.listdir(pdf_folder_path)]\n",
    "\n",
    "# print(loaders)\n",
    "\n",
    "# all_documents = []\n",
    "\n",
    "# for loader in loaders:\n",
    "#     print(\"Loading raw document...\" + loader.file_path)\n",
    "#     raw_documents = loader.load()\n",
    "\n",
    "#     print(\"Splitting text...\")\n",
    "#     text_splitter = CharacterTextSplitter(\n",
    "#         separator=\"\\n\\n\",\n",
    "#         chunk_size=800,\n",
    "#         chunk_overlap=100,\n",
    "#         length_function=len,\n",
    "#     )\n",
    "#     documents = text_splitter.split_documents(raw_documents)\n",
    "#     all_documents.extend(documents)\n",
    "\n",
    "# print(\"Creating vectorstore...\")\n",
    "# embeddings = OpenAIEmbeddings()\n",
    "# vectorstore = FAISS.from_documents(all_documents, embeddings)\n",
    "\n",
    "# with open(\"vectorstore.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(vectorstore, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enregistrer les chunks en tant que vecteur en utilisant faiss  \n",
    "pour résoudre le problème des tokens appels nombreux API, on découpe les chunks\n",
    " en blocs , sleep \n",
    "save le tout en local pour ne pas relancer à chaque fois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U langchain-community faiss-cpu langchain-openai tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings import AzureOpenAIEmbeddings\n",
    "# https://github.com/facebookresearch/faiss/wiki/Special-operations-on-indexes\n",
    "\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=\"text-embedding-ada-002\",\n",
    "    api_key=os.environ[\"AZURE_OPENAI_KEY\"],\n",
    ")\n",
    "\n",
    "CALC_EMBEDDINGS = False\n",
    "BATCH_EMBEDDINGS = False\n",
    "\n",
    "if CALC_EMBEDDINGS:\n",
    "    if BATCH_EMBEDDINGS:\n",
    "\n",
    "        # Iterate over each chunk of documents\n",
    "        # https://github.com/langchain-ai/langchain/issues/7343\n",
    "        # https://stackoverflow.com/questions/76644262/adding-large-chunks-of-text-after-embedding-into-pinecone-without-openai-ratelim\n",
    "        batch_size = 50\n",
    "\n",
    "        for i in range(0, len(chunks), batch_size):\n",
    "            print(f\"batch {i} to {i+batch_size}\")\n",
    "            if 'faiss_vectorstore' not in locals() or 'faiss_vectorstore' not in globals():\n",
    "                faiss_vectorstore = FAISS.from_documents(chunks[i:i+batch_size], embeddings)\n",
    "            else:\n",
    "                faiss_vectorstore.add_documents(chunks[i:i+batch_size])\n",
    "                print(f\"...sleep 6s...\")\n",
    "                sleep(6)  # Add delay here\n",
    "    else:\n",
    "        faiss_vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "    faiss_vectorstore.save_local(\"faiss_index\")\n",
    "else:\n",
    "    faiss_vectorstore = FAISS.load_local(\"faiss_index\", embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "faiss_retriever = faiss_vectorstore.as_retriever()\n",
    "\n",
    "# TODO : optimiser la taille des chunks plus tard\n",
    "# LangChainDeprecationWarning: The class `AzureOpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import AzureOpenAIEmbeddings`.\n",
    "# The de-serialization relies loading a pickle file. Pickle files can be modified to deliver a malicious payload that results in execution of arbitrary code on your machine.You will need to set `allow_dangerous_deserialization` to `True` to enable deserialization. If you do this, make sure that you trust the source of the data. For example, if you are loading a file that you created, and know that no one else has modified the file, then this is safe to do. Do not set this to `True` if you are loading a file from an untrusted source (e.g., some random site on the internet.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# #with open(\"index.pkl\", \"ab\") as f:\n",
    "# with open(\"index.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(faiss_vectorstore, f)\n",
    "#     f.close()\n",
    "\n",
    "# # DEBUG : cannot pickle '_thread.RLock' object\n",
    "# # https://github.com/langchain-ai/langchain/issues/14581"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Qu'est ce que le service Azure OpenAI ?\"\n",
    "question = \"Citer trois services cognitifs Azure.\"\n",
    "# question = \"Ce document est-il autorisé ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searchDocs = faiss_vectorstore.similarity_search(question, filter={\"user\":\"Paul\"} )\n",
    "\n",
    "print(f\"Retrieve docs: {len(searchDocs)}\")\n",
    "for i in range(len(searchDocs)):\n",
    "    print(f\"- {i} from {searchDocs[i].metadata['filename']} : \\n'''{searchDocs[i].page_content}'''\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrievers are used to find relevant documents or passages that contain the answer to a given query.\n",
    "The system \"retrieves\" any documents that could be relevant in answering the question, and then passes those documents (along with the original question) to the language model for a \"generation\" step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La question est posée en utilisant le chat,\n",
    "toujours en utilisant la similarité sématique \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test utilisant LCEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Utilisez les éléments de contexte suivants pour répondre à la question contenue dans les 3 crochets à la fin. Si vous ne connaissez pas la réponse, dites simplement que vous ne savez pas, n'essayez pas d'inventer une réponse.\n",
    "Veuillez fournir une réponse correcte sur le plan factuel et basée sur les informations extraites du magasin de vecteurs.\n",
    "Veuillez également mentionner toute citation étayant la réponse, si elle est présente dans le contexte fourni, entre deux doubles guillemets \"\" .\n",
    "Ne relancez pas la conversation en demandant des précisions sur la question. Répondez exclusivement en français.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION:```{question}```\n",
    "REPONSE:\n",
    "\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\",\"question\"]\n",
    "  )\n",
    "\n",
    "chain_type_kwargs = {\"prompt\": PROMPT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt_template =  \"\"\"\n",
    "#                     Answer the question only according to the context: {context}\n",
    "#                     Question: {question}\n",
    "#                     \"\"\"\n",
    "\n",
    "# PROMPT = ChatPromptTemplate.from_template(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "retrieval_chain = (\n",
    "    {\"context\": faiss_retriever, \"question\": RunnablePassthrough()}\n",
    "    | PROMPT\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Qu'est ce que le service Azure OpenAI ?\"\n",
    "question = \"Citer trois services cognitifs Azure.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "\n",
    "filtered_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"filter\": {\"user\":'Paul'}})\n",
    "\n",
    "qa_stuff = RetrievalQA.from_chain_type( #creer une chain pour répondre à la question\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", # tous les documents d'un coup et les passe au LLM\n",
    "    # chain_type_kwargs=chain_type_kwargs,\n",
    "    retriever=filtered_retriever, \n",
    "    return_source_documents= True,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"Pourquoi utiliser Azure OpenAI ?\"\n",
    "question = \"Quel service utiliser pour de la traduction ?\"\n",
    "# question = \"Est-ce que le contenu du contexte est autorisé ?\"\n",
    "\n",
    "result = qa_stuff.invoke(question)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Préparer un golden dataset \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : translate questions and ground_truth\n",
    "# https://docs.ragas.io/en/stable/getstarted/testset_generation.html\n",
    "\n",
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "# generator with openai models\n",
    "generator_llm = llm\n",
    "critic_llm = llm4\n",
    "# embeddings = OpenAIEmbeddings()\n",
    "\n",
    "generator = TestsetGenerator.from_langchain(\n",
    "    generator_llm,\n",
    "    critic_llm,\n",
    "    embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate testset\n",
    "GENERATE_TESTSET = False\n",
    "\n",
    "if GENERATE_TESTSET:\n",
    "\n",
    "    testset = generator.generate_with_langchain_docs(\n",
    "                            documents,\n",
    "                            test_size=20,\n",
    "                            distributions={simple: 0.5, reasoning: 0.25, multi_context: 0.25}\n",
    "                            )\n",
    "    \n",
    "    testset_df = testset.to_pandas()\n",
    "    testset_df['question_en'] = testset_df['question']\n",
    "    testset_df['question'] = testset_df['question'].apply(translate)\n",
    "    testset_df.to_csv('testset.csv', index=False)  \n",
    "else :\n",
    "    testset_df = pd.read_csv('testset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset_df['question'] = testset_df['question_en'].apply(translate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(testset_df['ground_truth'] == 'nan').sum()\n",
    "\n",
    "# Replace \"nan\" in ground_truth by good answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset_df.iloc[2]['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset_df.iloc[2]['ground_truth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "\"Qu'est-ce que le service Azure OpenAI ?\",\n",
    "\"Comment fonctionne le service Azure OpenAI ?\",\n",
    "\"Quels sont les avantages du service Azure OpenAI ?\",\n",
    "\"Quels sont les cas d'utilisation du service Azure OpenAI ?\",\n",
    "\"Comment mettre en œuvre le service Azure OpenAI ?\",\n",
    "\"Quels sont les composants principaux du service Azure OpenAI ?\",\n",
    "\"Quels sont les modèles d'IA accessibles via le service Azure OpenAI ?\",\n",
    "\"Quelles tâches les modèles d'IA du service Azure OpenAI peuvent-ils accomplir ?\",\n",
    "\"Quelle est l'infrastructure sur laquelle repose le service Azure OpenAI ?\",\n",
    "\"Quels outils le service Azure OpenAI fournit-il pour la formation et le réglage des modèles d'IA ?\"\n",
    "]\n",
    "\n",
    "answers = [  \n",
    "\"Le service Azure OpenAI est un service cloud qui permet aux développeurs et aux chercheurs d'accéder à la plateforme OpenAI, une initiative qui vise à créer une intelligence artificielle générale (AGI) bénéfique pour l'humanité. Il offre la possibilité d'utiliser des modèles d'IA pré-entraînés ou personnalisés, de les déployer sur le cloud ou en périphérie, et de les intégrer à des applications et des scénarios variés.\",   \n",
    "\"Le service Azure OpenAI repose sur deux composants principaux : le portail Azure OpenAI et les API Azure OpenAI. Le portail est une interface web qui permet de créer, de gérer et de surveiller les ressources du service, tandis que les API sont des interfaces de programmation qui permettent d'interagir avec les modèles d'IA du service.\",   \n",
    "\"Le service Azure OpenAI présente plusieurs avantages, parmi lesquels l'accès à des modèles d'IA de pointe, une grande flexibilité, une haute performance grâce à l'infrastructure cloud de Microsoft Azure, et une facilité d'intégration grâce à des API simples et des SDK dans différents langages de programmation.\",   \n",
    "\"Le service Azure OpenAI peut être utilisé pour de nombreux cas d'utilisation, dans différents domaines et secteurs d'activité, tels que l'éducation, la santé, le divertissement et le commerce.\",   \n",
    "\"Pour mettre en œuvre le service Azure OpenAI, il faut créer un compte Microsoft Azure, souscrire à un abonnement au service, créer une clé d'API, choisir ou créer un modèle d'IA, créer un point de terminaison, envoyer des requêtes au point de terminaison, et analyser les résultats des requêtes.\",   \n",
    "\"Les composants principaux du service Azure OpenAI sont le portail Azure OpenAI et les API Azure OpenAI.\",   \n",
    "\"Les modèles d'IA accessibles via le service Azure OpenAI incluent GPT-3, Codex, DALL-E et CLIP.\",   \n",
    "\"Les modèles d'IA du service Azure OpenAI peuvent réaliser des tâches complexes et variées, telles que la génération de texte, la compréhension du langage naturel, la vision par ordinateur, la synthèse vocale ou la création de contenu multimédia.\",   \n",
    "\"L'infrastructure sur laquelle repose le service Azure OpenAI est l'infrastructure cloud de Microsoft Azure.\",   \n",
    "\"Le service Azure OpenAI fournit des outils de formation et de réglage pour créer son propre modèle d'IA.\"\n",
    "]  \n",
    "\n",
    "ground_truths = [  \n",
    "[\"Le service Azure OpenAI est un service cloud qui permet aux développeurs et aux chercheurs d'accéder à la plateforme OpenAI\", \"Le service Azure OpenAI offre la possibilité d'utiliser des modèles d'IA pré-entraînés ou personnalisés, de les déployer sur le cloud ou en périphérie, et de les intégrer à des applications et des scénarios variés.\"],  \n",
    "[\"Le service Azure OpenAI repose sur deux composants principaux : le portail Azure OpenAI et les API Azure OpenAI.\", \"Le portail Azure OpenAI est une interface web qui permet de créer, de gérer et de surveiller les ressources du service Azure OpenAI, telles que les abonnements, les clés, les modèles, les points de terminaison et les requêtes.\"],  \n",
    "[\"Le service Azure OpenAI présente plusieurs avantages, parmi lesquels :\", \"Il garantit une haute performance, en s'appuyant sur l'infrastructure cloud de Microsoft Azure, qui assure une disponibilité, une scalabilité et une sécurité optimales.\"],  \n",
    "[\"Le service Azure OpenAI peut être utilisé pour de nombreux cas d'utilisation, dans différents domaines et secteurs d'activité.\", \"Dans le domaine du commerce, le service Azure OpenAI peut être utilisé pour créer des outils d'optimisation, tels que des recommandations personnalisées, des prévisions de ventes, des analyses de marché ou des chatbots.\"],  \n",
    "[\"Pour mettre en œuvre le service Azure OpenAI, il faut suivre les étapes suivantes :\", \"Analyser les résultats des requêtes, en utilisant les outils de suivi et de statistiques fournis par le portail Azure OpenAI.\"],  \n",
    "[\"Le service Azure OpenAI repose sur deux composants principaux : le portail Azure OpenAI et les API Azure OpenAI.\", \"Les API Azure OpenAI sont des interfaces de programmation qui permettent d'interagir avec les modèles d'IA du service Azure OpenAI, tels que GPT-3, Codex, DALL-E ou CLIP.\"],  \n",
    "[\"Les API Azure OpenAI sont des interfaces de programmation qui permettent d'interagir avec les modèles d'IA du service Azure OpenAI\", \"tels que GPT-3, Codex, DALL-E ou CLIP.\"],  \n",
    "[\"Il permet d'accéder à des modèles d'IA de pointe, capables de réaliser des tâches complexes et variées\", \"telles que la génération de texte, la compréhension du langage naturel, la vision par ordinateur, la synthèse vocale ou la création de contenu multimédia.\"],  \n",
    "[\"Il garantit une haute performance, en s'appuyant sur l'infrastructure cloud de Microsoft Azure\", \"qui assure une disponibilité, une scalabilité et une sécurité optimales.\"],  \n",
    "[\"Choisir un modèle d'IA, parmi ceux proposés par le service Azure OpenAI\", \"ou créer son propre modèle, en utilisant les outils de formation et de réglage fournis par le service Azure OpenAI.\"]  \n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "contexts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for query in questions:\n",
    "    result = qa_stuff({\"query\": query})\n",
    "    results.append(result['result']) \n",
    "    sources = result[\"source_documents\"]\n",
    "    contents = [source.page_content for source in sources]  \n",
    "\n",
    "    processed_contents = []\n",
    "    for content in contents:\n",
    "        if isinstance(content, str):  \n",
    "            processed_content = content.split('.')[:2]  \n",
    "            processed_contents.extend(processed_content)  \n",
    "        else:\n",
    "            print(f\"Expected a string but got {type(content)}\") \n",
    "    contexts.append(processed_contents)\n",
    "\n",
    "print(len(questions))\n",
    "print(len(ground_truths))\n",
    "print(len(results))\n",
    "print(len(contexts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a DataFrame\n",
    "data = {\n",
    "    'question': questions,\n",
    "    'answer': results,\n",
    "    'contexts': contexts,\n",
    "    'ground_truth': ground_truths\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df.head())\n",
    "\n",
    "# Save as .csv\n",
    "df.to_csv('ragas_aoai_dataset.csv', index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://blog.cellenza.com/securite-2/utiliser-azure-openai-langchain-et-ragas-pour-la-classification-des-documents-confidentiels-et-la-protection-des-donnees-sensibles/ \n",
    "\n",
    "# from ragas.testset.generator import TestsetGenerator\n",
    "# from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "\n",
    "# testset = generator.generate_with_langchain_docs(documents, test_size=10)\n",
    "\n",
    "# print(testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ragas datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "import pyarrow as py\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as ds\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import answer_relevancy, answer_similarity, answer_correctness\n",
    "from ragas.metrics import context_recall, context_precision, context_entity_recall, context_utilization, context_relevancy\n",
    "from ragas.metrics import faithfulness\n",
    "from ragas.metrics.critique import harmfulness, maliciousness, coherence, correctness, conciseness\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from datasets import Dataset, Features, Sequence, Value, load_dataset\n",
    "\n",
    "\n",
    "# ragas.metrics.answer_relevancy : Scores the relevancy of the answer according to the given question.\n",
    "# ragas.metrics.answer_similarity : Scores the semantic similarity of ground truth with generated answer.\n",
    "# ragas.metrics.answer_correctness : Measures answer correctness compared to ground truth as a combination of factuality and semantic similarity.\n",
    "# ragas.metrics.context_precision : Average Precision is a metric that evaluates whether all of the relevant items selected by the model are ranked higher or not.\n",
    "# ragas.metrics.context_recall : Estimates context recall by estimating TP and FN using annotated answer and retrieved context.\n",
    "# ragas.metrics.context_entity_recall : Calculates recall based on entities present in ground truth and context.\n",
    "\n",
    "# Définir le dataset explicitement \n",
    "features = Features({\n",
    "    \"question\": Value(\"string\"),\n",
    "    \"answer\": Value(\"string\"),\n",
    "    \"contexts\": Sequence(Value(\"string\")),\n",
    "    \"ground_truth\": Value(\"string\")\n",
    "})\n",
    "dataset = Dataset.from_dict(df, features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.llms import LangchainLLMWrapper\n",
    "\n",
    "# wrapper around azure_model \n",
    "ragas_azure_model = LangchainLLMWrapper(llm)\n",
    "# patch the new RagasLLM instance\n",
    "faithfulness.llm = ragas_azure_model\n",
    "answer_relevancy.llm = ragas_azure_model\n",
    "context_precision.llm = ragas_azure_model\n",
    "context_recall.llm = ragas_azure_model\n",
    "# harmfulness.llm = ragas_azure_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# only for answer_relevancy\n",
    "azure_embeddings = OpenAIEmbeddings(\n",
    "    # deployment=\"text-embedding-ada-002\",\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    openai_api_base=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    openai_api_key=os.getenv(\"AZURE_OPENAI_KEY\"),\n",
    "    openai_api_type=\"azure\"\n",
    ")\n",
    "\n",
    "answer_relevancy.embeddings = azure_embeddings\n",
    "answer_similarity.embeddings = azure_embeddings\n",
    "answer_correctness.embeddings = azure_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_contexts = evaluate(\n",
    "       dataset=dataset,\n",
    "       metrics=[context_recall, context_precision],\n",
    "       llm=ragas_azure_model,\n",
    "       raise_exceptions=False,\n",
    ")\n",
    "\n",
    "evaluate_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_contexts_df = evaluate_contexts.to_pandas()\n",
    "evaluate_contexts_df.sort_values(by='context_precision', ascending=True).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_answers = evaluate(\n",
    "       dataset=dataset,\n",
    "       metrics=[faithfulness, answer_relevancy, answer_similarity, answer_correctness],\n",
    "       llm=ragas_azure_model,\n",
    "       raise_exceptions=False,\n",
    ")\n",
    "\n",
    "evaluate_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_answers_df = evaluate_answers.to_pandas()\n",
    "evaluate_answers_df.sort_values(by='faithfulness', ascending=True).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_aspect_critique = evaluate(\n",
    "       dataset=dataset,\n",
    "       metrics=[harmfulness, maliciousness, coherence, correctness, conciseness],\n",
    "       llm=ragas_azure_model,\n",
    "       raise_exceptions=False,\n",
    ")\n",
    "\n",
    "evaluate_aspect_critique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_aspect_critique_df = evaluate_aspect_critique.to_pandas()\n",
    "evaluate_aspect_critique_df.sort_values(by='correctness', ascending=True).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install evaluate bert_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, the best model is microsoft/deberta-xlarge-mnli, please consider using it instead of the default roberta-large in order to have the best correlation with human evaluation.\n",
    "\n",
    "https://docs.google.com/spreadsheets/d/1RKOVpselB98Nnh_EOC4A2BYn8_201tmPODpNWu4w7xI/edit#gid=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "bertscore = load(\"bertscore\")\n",
    "predictions = [\"hello there\", \"general kenobi\"]\n",
    "references = [\"hello there\", \"general kenobi\"]\n",
    "results = bertscore.compute(predictions=predictions, references=references, lang=\"en\")\n",
    "# default model lang=en: roberta-large\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "precision: The precision for each sentence from the predictions + references lists, which ranges from 0.0 to 1.0.\n",
    "\n",
    "recall: The recall for each sentence from the predictions + references lists, which ranges from 0.0 to 1.0.\n",
    "\n",
    "f1: The F1 score for each sentence from the predictions + references lists, which ranges from 0.0 to 1.0.\n",
    "\n",
    "hashcode: The hashcode of the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajuster les options pandas pour afficher les textes complets\n",
    "pd.set_option('display.max_colwidth', None)  # Désactive la troncation des colonnes\n",
    "pd.set_option('display.max_rows', None)      # Désactive la troncation des lignes\n",
    "pd.set_option('display.max_columns', None)   # Affiche toutes les colonnes\n",
    "pd.set_option('display.width', None)         # Ajuste automatiquement la largeur d'affichage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = answers\n",
    "references = [sublist[0] for sublist in ground_truths]\n",
    "results = bertscore.compute(predictions=predictions, references=references, lang=\"fr\", model_type=\"microsoft/deberta-xlarge-mnli\")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'predictions': predictions,\n",
    "    'references': references,\n",
    "    'precision': results['precision'],\n",
    "    'recall': results['recall'],\n",
    "    'f1': results['f1']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(df.sort_values(by='precision', ascending=True).head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.sort_values(by='precision', ascending=False).head(3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
