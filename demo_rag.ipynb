{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain.llms import AzureOpenAI\n",
    "# from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.chat_models import openai\n",
    "from openai import AzureOpenAI\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "import requests\n",
    "import os\n",
    "from datetime import datetime\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import faiss\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_KEY\"] = \"b98951878a904abb8ed21a264523758c\"\n",
    "os.environ[\"AZURE_OPENAI_KEY\"] = os.environ[\"OPENAI_KEY\"]\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = os.environ[\"OPENAI_KEY\"]\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://lab-ai-openai-eus.openai.azure.com/\"\n",
    "os.environ[\"AZURE_OPENAI_API_VERSION\"] = \"2024-02-01\"\n",
    "os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"] = \"gpt-35-turbo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-9XR6Sf5s3JRY8vLuGInVO7xM6a0vn', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Great! How can I assist you today?', role='assistant', function_call=None, tool_calls=None), content_filter_results={'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}})], created=1717756036, model='gpt-35-turbo', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=9, prompt_tokens=19, total_tokens=28), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}])\n"
     ]
    }
   ],
   "source": [
    "client = AzureOpenAI(\n",
    "  azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"), \n",
    "  api_key=os.getenv(\"AZURE_OPENAI_KEY\"),  \n",
    "  api_version=\"2024-02-01\" #2024-02-15-preview ?\n",
    ")\n",
    "\n",
    "message_text = [{\"role\":\"system\",\"content\":\"You are an AI assistant that helps people find information.\"}]\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-35-turbo\", # model = \"deployment_name\"\n",
    "  messages = message_text,\n",
    "  temperature=0.7,\n",
    "  max_tokens=800,\n",
    "  top_p=0.95,\n",
    "  frequency_penalty=0,\n",
    "  presence_penalty=0,\n",
    "  stop=None\n",
    ")\n",
    "\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"J'aime programmer.\", response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 20, 'total_tokens': 25}, 'model_name': 'gpt-35-turbo', 'system_fingerprint': None, 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run-84af2c29-14e3-42d2-8d50-08fb6c2f1c5a-0', usage_metadata={'input_tokens': 20, 'output_tokens': 5, 'total_tokens': 25})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = AzureChatOpenAI(\n",
    "    openai_api_version=\"2024-02-01\",\n",
    "    azure_deployment=\"gpt-35-turbo\",\n",
    ")\n",
    "\n",
    "llm4 = AzureChatOpenAI(\n",
    "    openai_api_version=\"2024-05-13\",\n",
    "    azure_deployment=\"gpt-4o\",\n",
    ")\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "message = HumanMessage(\n",
    "    content=\"Translate this sentence from English to French. I love programming.\"\n",
    ")\n",
    "llm.invoke([message])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install unstructured\n",
    "# !pip install \"unstructured[pdf]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"Attention !\\n\\nCe fichier contient des informations confidentielles et sensibles qui ne doivent pas être divulguées à des tiers. La lecture de ce fichier est strictement réservée aux personnes autorisées par le propriétaire ou le créateur du fichier. Toute tentative de lecture non autorisée ou de copie du fichier constitue une violation de la loi et expose le contrevenant à des sanctions pénales et civiles.\\n\\nSi vous n'êtes pas une personne autorisée, veuillez fermer immédiatement ce fichier et le supprimer de votre appareil. Si vous avez reçu ce fichier par erreur, veuillez en informer le propriétaire ou le créateur du fichier et lui renvoyer ou lui détruire le fichier. Si vous êtes une personne autorisée, veuillez vous assurer que le fichier est protégé par un mot de passe et qu'il n'est pas accessible à des personnes non autorisées.\\n\\nMerci de respecter la confidentialité et la sécurité des informations contenues dans ce fichier.\", metadata={'source': 'microsoft_learn\\\\Liticia\\\\Attention.pdf'}),\n",
       " Document(page_content=\"Les modèles de fondation de l’IA générative\\n\\nLes modèles de fondation de l’IA générative sont des modèles d’intelligence artificielle capables de générer du contenu à partir de données brutes, comme du texte, des images, des sons ou des vidéos. Ces modèles utilisent des techniques d’apprentissage profond pour apprendre les caractéristiques et les relations des données, et pour produire de nouveaux exemples qui respectent les mêmes règles. Les modèles de fondation de l’IA générative sont considérés comme une avancée majeure dans le domaine de l’IA, car ils permettent de créer des applications innovantes et créatives dans divers domaines.\\n\\nParmi les modèles de fondation de l’IA générative, les modèles de génération d’images sont ceux qui visent à créer des images réalistes et diversifiées à partir de données brutes ou conditionnées. Par exemple, un modèle de génération d’images peut prendre une description textuelle comme entrée et produire une image qui correspond au texte. Ou bien, il peut prendre une image existante et la modifier selon un style, une couleur ou un attribut donné. Les modèles de génération d’images peuvent être basés sur des réseaux de neurones convolutifs (CNN), qui sont capables de traiter et de reconnaître les caractéristiques visuelles des images, ou sur des réseaux antagonistes génératifs (GAN), qui sont des systèmes composés de deux réseaux concurrents : un générateur qui crée les images et un discriminateur qui les évalue. Les modèles de génération d’images offrent de nombreuses possibilités d’application, comme la synthèse de visages, le dessin assisté, la colorisation, la restauration ou la super- résolution d’images.\\n\\nLes LLM, ou masters of laws, sont des diplômes de droit de niveau supérieur qui permettent aux étudiants et aux professionnels d'approfondir leurs connaissances juridiques dans un domaine spécifique. Les LLM sont généralement proposés par des universités prestigieuses, et sont reconnus internationalement comme des qualifications académiques de haut niveau. Les LLM sont souvent destinés aux personnes qui souhaitent se spécialiser dans un domaine du droit, comme le droit international, le droit des affaires, le droit fiscal, le droit de la propriété intellectuelle ou le droit des droits de l'homme. Les LLM peuvent également servir à acquérir une expérience juridique dans un pays ou une région différente, ou à se préparer à un doctorat en droit. Les LLM offrent de nombreux avantages aux étudiants et aux professionnels, comme l'amélioration de leurs compétences juridiques, la mise en réseau avec des pairs et des experts, l'ouverture de nouvelles opportunités de carrière ou de recherche, ou l'augmentation de leur crédibilité et de leur réputation.\\n\\nLe service Azure OpenAI : un outil puissant pour l'intelligence artificielle\\n\\nUn document qui présente le service Azure OpenAI, ses caractéristiques, ses bénéfices et ses applications.\\n\\nQu'est-ce que le service Azure OpenAI ? Le service Azure OpenAI est un service cloud qui permet aux développeurs et aux chercheurs d'accéder à la plateforme OpenAI, une initiative qui vise à créer une intelligence artificielle générale (AGI) bénéfique pour l'humanité. Le service Azure OpenAI offre la possibilité d'utiliser des modèles d'IA pré-entraînés ou personnalisés, de les déployer sur le cloud ou en périphérie, et de les intégrer à des applications et des scénarios variés.\", metadata={'source': 'microsoft_learn\\\\Paul\\\\Le deep learning pour les images.pdf'}),\n",
       " Document(page_content=\"Comment fonctionne le service Azure OpenAI ? Le service Azure OpenAI repose sur deux composants principaux : le portail Azure OpenAI et les API Azure OpenAI. Le portail Azure OpenAI est une interface web qui permet de créer, de gérer et de surveiller les ressources du service Azure OpenAI, telles que les abonnements, les clés, les modèles, les points de terminaison et les requêtes. Les API Azure OpenAI sont des interfaces de programmation qui permettent d'interagir avec les modèles d'IA du service Azure OpenAI, tels que GPT-3, Codex, DALL-E ou CLIP. Les API Azure OpenAI sont accessibles via des requêtes HTTP ou des SDK dans différents langages de programmation, tels que Python, Java, C#, Node.js ou Ruby.\\n\\nQuels sont les avantages du service Azure OpenAI ? Le service Azure OpenAI présente plusieurs avantages, parmi lesquels :\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIl permet d'accéder à des modèles d'IA de pointe, capables de réaliser des tâches complexes et variées, telles que la génération de texte, la compréhension du langage naturel, la vision par ordinateur, la synthèse vocale ou la création de contenu multimédia. Il offre une grande flexibilité, en permettant de choisir entre des modèles pré-entraînés ou personnalisés, et de les adapter aux besoins spécifiques de chaque projet ou domaine. Il garantit une haute performance, en s'appuyant sur l'infrastructure cloud de Microsoft Azure, qui assure une disponibilité, une scalabilité et une sécurité optimales. Il facilite l'intégration, en proposant des API simples et des SDK dans différents langages de programmation, qui permettent de connecter les modèles d'IA du service Azure OpenAI à des applications et des scénarios existants ou nouveaux.\\n\\nQuels sont les cas d'utilisation du service Azure OpenAI ? Le service Azure OpenAI peut être utilisé pour de nombreux cas d'utilisation, dans différents domaines et secteurs d'activité. Voici quelques exemples :\\n\\nDans le domaine de l'éducation, le service Azure OpenAI peut être utilisé pour créer des outils\\n\\npédagogiques, tels que des assistants de rédaction, des générateurs de questions, des correcteurs automatiques ou des tuteurs virtuels.\\n\\nDans le domaine de la santé, le service Azure OpenAI peut être utilisé pour créer des outils d'aide au diagnostic, à la prescription, à la recherche ou à la prévention, en exploitant les données médicales et les connaissances scientifiques.\\n\\nDans le domaine du divertissement, le service Azure OpenAI peut être utilisé pour créer des outils de génération de contenu, tels que des scénarios, des dialogues, des personnages, des musiques ou des images.\\n\\nDans le domaine du commerce, le service Azure OpenAI peut être utilisé pour créer des outils\\n\\nd'optimisation, tels que des recommandations personnalisées, des prévisions de ventes, des analyses de marché ou des chatbots.\\n\\nComment mettre en œuvre le service Azure OpenAI ? Pour mettre en œuvre le service Azure OpenAI, il faut suivre les étapes suivantes :\\n\\nCréer un compte Microsoft Azure, si ce n'est pas déjà fait, et se connecter au portail Azure OpenAI.\\n\\nSouscrire à un abonnement au service Azure OpenAI, en choisissant le niveau de tarification\\n\\nadapté à ses besoins.\\n\\nCréer une clé d'API, qui permettra d'authentifier les requêtes au service Azure OpenAI. • Choisir un modèle d'IA, parmi ceux proposés par le service Azure OpenAI, ou créer son propre modèle, en utilisant les outils de formation et de réglage fournis par le service Azure OpenAI.\\n\\nCréer un point de terminaison, qui permettra d'exposer le modèle d'IA sur le cloud ou en\\n\\npériphérie, et de le rendre accessible via une URL.\\n\\nEnvoyer des requêtes au point de terminaison, en utilisant les API Azure OpenAI ou les SDK dans le langage de programmation de son choix.\\n\\nAnalyser les résultats des requêtes, en utilisant les outils de suivi et de statistiques fournis par le\\n\\nportail Azure OpenAI.\\n\\nLes services cognitifs Azure\", metadata={'source': 'microsoft_learn\\\\Paul\\\\Le deep learning pour les images.pdf'}),\n",
       " Document(page_content=\"Analyser les résultats des requêtes, en utilisant les outils de suivi et de statistiques fournis par le\\n\\nportail Azure OpenAI.\\n\\nLes services cognitifs Azure\\n\\nLes services cognitifs Azure sont un ensemble de services cloud qui permettent aux développeurs de créer des applications intelligentes, en utilisant les capacités d'intelligence artificielle (IA) et de machine learning (ML) fournies par Microsoft. Les services cognitifs Azure couvrent différents domaines, tels que la vision, le langage, la recherche, la décision et la synthèse vocale. Ils offrent des API, des SDK et des outils faciles à utiliser, pour intégrer les fonctionnalités d'IA et de ML dans les applications web, mobiles ou de bureau.\\n\\nLa vision Le domaine de la vision regroupe les services cognitifs Azure qui permettent aux applications de comprendre et d'analyser le contenu visuel, comme les images, les vidéos ou les flux en direct. Les services cognitifs Azure de la vision comprennent :\\n\\nLe service Vision par ordinateur, qui fournit des fonctions de détection, d'analyse, de description et de balisage des images, ainsi que de reconnaissance optique de caractères (OCR) et de génération de vignettes.\\n\\nLe service Reconnaissance faciale, qui permet d'identifier, de vérifier, de rechercher et de regrouper des visages dans des images ou des vidéos, ainsi que de détecter des attributs faciaux, comme l'âge, le genre, l'émotion ou les accessoires.\\n\\nLe service Analyseur vidéo, qui offre des capacités d'analyse avancée des vidéos, comme la détection et le suivi des visages, des objets, des logos, des scènes ou des activités, ainsi que la transcription, la traduction et la synthèse vocale des pistes audio.\\n\\nLe service Form Recognizer, qui permet d'extraire et de structurer les données clés contenues dans des formulaires papier ou numériques, comme des factures, des reçus, des chèques ou des cartes d'identité.\\n\\nLe service Custom Vision, qui permet de personnaliser les modèles de vision par ordinateur et de reconnaissance faciale, en les entraînant avec ses propres données, afin de répondre à des besoins spécifiques.\\n\\nLe langage Le domaine du langage regroupe les services cognitifs Azure qui permettent aux applications de comprendre et de manipuler le langage naturel, c'est-à-dire le langage parlé ou écrit par les humains. Les services cognitifs Azure du langage comprennent :\\n\\nLe service Analyse de texte, qui fournit des fonctions d'analyse s>yntaxique, sémantique et sentimentale des textes, ainsi que de détection des entités nommées, des mots-clés, des liens ou des langues.\\n\\nLe service Reconnaissance vocale, qui permet de transcrire le discours en texte, en reconnaissant les mots prononcés, le locuteur, l'intention ou les commandes.\\n\\nLe service Traduction vocale, qui permet de traduire le discours d'une langue à une autre, en conservant la voix et le ton du locuteur.\\n\\nLe service Génération de langage naturel, qui permet de produire du texte à partir de données structurées, comme des tableaux, des graphiques ou des images, en utilisant des modèles pré-entraînés ou personnalisés.\\n\\nLe service Linguistic Analysis, qui fournit des fonctions d'analyse morphologique, syntaxique et sémantique des textes, en utilisant des règles et des dictionnaires linguistiques.\\n\\nLe service QnA Maker, qui permet de créer des agents conversationnels capables de répondre aux questions des utilisateurs, en se basant sur des sources de connaissances existantes, comme des documents, des sites web ou des FAQ.\\n\\nLes bases de données vectorielles\", metadata={'source': 'microsoft_learn\\\\Paul\\\\Le deep learning pour les images.pdf'}),\n",
       " Document(page_content=\"Les bases de données vectorielles\\n\\nLes bases de données vectorielles sont des systèmes de gestion de données qui stockent et manipulent des vecteurs, c'est-à-dire des séquences de nombres réels qui représentent des entités ou des objets. Ces vecteurs peuvent être obtenus à partir de diverses sources, comme des images, des textes, des sons, des vidéos, ou des signaux biométriques. Les bases de données vectorielles permettent d'effectuer des opérations efficaces sur les vecteurs, comme la recherche par similarité, le calcul de distances, le regroupement, la classification, ou la recommandation.\\n\\nLes bases de données vectorielles sont utilisées dans de nombreux domaines d'application, comme la vision par ordinateur, le traitement du langage naturel, la bioinformatique, ou le commerce électronique. Elles offrent plusieurs avantages par rapport aux bases de données traditionnelles, comme la réduction de la dimensionnalité, la généralisation, la robustesse au bruit, ou la facilité d'interprétation. Elles présentent aussi des défis spécifiques, comme le choix des méthodes de vectorisation, l'indexation des vecteurs de grande dimension, ou la garantie de la sécurité et de la confidentialité des données.\\n\\nDans cet article, nous allons présenter les principes et les caractéristiques des bases de données vectorielles, ainsi que les principales techniques et outils utilisés pour les mettre en œuvre. Nous allons également illustrer les applications et les bénéfices des bases de données vectorielles à travers quelques exemples concrets.\\n\\nPrincipes et caractéristiques des bases de données vectorielles Une base de données vectorielle est un système de gestion de données qui stocke et manipule des vecteurs, c'est-à-dire des séquences ordonnées de nombres réels qui représentent des entités ou des objets. Un vecteur peut avoir une dimension fixe ou variable, selon le type de données qu'il encode. Par exemple, un vecteur peut représenter les pixels d'une image, les mots d'un texte, les fréquences d'un son, ou les caractéristiques d'un produit.\\n\\nLes vecteurs sont généralement obtenus à partir de données brutes, comme des fichiers multimédias, des documents, ou des flux de données, en utilisant des méthodes de vectorisation, qui transforment les données en vecteurs selon certains critères. Il existe différentes méthodes de vectorisation, selon le domaine et le but de l'application. Par exemple, on peut utiliser des méthodes basées sur des descripteurs, qui extraient des caractéristiques pertinentes des données, comme les couleurs, les formes, ou les textures pour les images, ou les mots, les phrases, ou les concepts> pour les textes. On peut aussi utiliser des méthodes basées sur des modèles, qui apprennent des représentations vectorielles à partir de données massives, en utilisant des techniques d'apprentissage automatique, comme les réseaux de neurones, les modèles probabilistes, ou les modèles géométriques.\\n\\nLe deep learning pour la classification d'images et la détection d'objets\\n\\nIntroduction La classification d'images et la détection d'objets sont deux tâches importantes de la vision par ordinateur, qui consistent à identifier et à localiser les objets présents dans une image. Ces tâches ont de nombreuses applications pratiques, comme la reconnaissance faciale, la surveillance, la robotique, ou la médecine. Le deep learning, qui est une branche de l'apprentissage automatique basée sur des modèles profonds, a révolutionné le domaine de la vision par ordinateur, en offrant des performances supérieures aux méthodes classiques, basées sur des caractéristiques manuelles ou des modèles simples. Dans ce document, nous allons présenter les principes et les caractéristiques du deep learning pour la classification d'images et la détection d'objets, ainsi que les principaux modèles et techniques utilisés.\", metadata={'source': 'microsoft_learn\\\\Paul\\\\Le deep learning pour les images.pdf'}),\n",
       " Document(page_content=\"Principes du deep learning Le deep learning est une méthode d'apprentissage automatique qui utilise des réseaux de neurones artificiels, qui sont des structures composées de couches de neurones interconnectés, qui reçoivent des données en entrée et produisent des sorties en fonction de paramètres appris. Un réseau de neurones peut être vu comme une fonction non linéaire complexe, qui approxime une relation entre les données d'entrée et les données de sortie. Le réseau de neurones apprend ses paramètres à partir d'un ensemble d'exemples, appelé jeu de données, en utilisant un algorithme d'optimisation, généralement basé sur la descente de gradient, qui minimise une fonction de coût, qui mesure l'erreur entre les sorties prédites et les sorties attendues. Le réseau de neurones est dit profond lorsque le nombre de couches est élevé, ce qui lui permet de capturer des niveaux de plus en plus abstraits et complexes de représentation des données. Le deep learning se distingue des autres méthodes d'apprentissage automatique par sa capacité à apprendre directement à partir des données brutes, sans avoir besoin de définir des caractéristiques spécifiques au domaine.\\n\\nClassification d'images La classification d'images est la tâche qui consiste à attribuer une étiquette à une image, parmi un ensemble de classes prédéfinies. Par exemple, on peut vouloir classer une image selon qu'elle contient un chat, un chien, ou un oiseau. La classification d'images peut être vue comme un problème de classification supervisée, où l'on dispose d'un jeu de données d'images annotées avec leurs classes respectives, et où l'on cherche à apprendre un> modèle qui prédit la classe d'une image inconnue. Le deep learning a permis de réaliser des progrès significatifs dans la classification d'images, en utilisant des réseaux de neurones convolutifs, qui sont des modèles spécialisés pour le traitement des données visuelles.\", metadata={'source': 'microsoft_learn\\\\Paul\\\\Le deep learning pour les images.pdf'}),\n",
       " Document(page_content=\"Réseaux de neurones convolutifs Un réseau de neurones convolutif (CNN) est un type de réseau de neurones qui utilise des opérations de convolution, qui consistent à appliquer des filtres linéaires sur les données d'entrée, pour extraire des caractéristiques locales et invariantes. Un CNN est composé de plusieurs couches, qui peuvent être de différents types, comme des couches de convolution, des couches d'activation, des couches de regroupement, ou des couches entièrement connectées. Les couches de convolution appliquent des filtres sur les données d'entrée, pour produire des cartes d'activation, qui représentent la réponse du filtre à une région de l'entrée. Les filtres sont appris par le réseau, et sont généralement de petite taille, comme 3x3 ou 5x5 pixels. Les couches d'activation appliquent une fonction non linéaire aux cartes d'activation, pour introduire de la non-linéarité dans le modèle. Les fonctions d'activation les plus courantes sont la fonction sigmoïde, la fonction tangente hyperbolique, ou la fonction ReLU. Les couches de regroupement réduisent la dimensionnalité des cartes d'activation, en appliquant une opération d'agrégation, comme le maximum, la moyenne, ou la norme, sur des régions voisines. Les couches de regroupement permettent de réduire le nombre de paramètres, de limiter le surapprentissage, et d'augmenter l'invariance à la translation. Les couches entièrement connectées sont des couches classiques de réseau de neurones, qui relient tous les neurones d'une couche à ceux de la couche suivante. Les couches entièrement connectées permettent de combiner les caractéristiques extraites par les couches précédentes, et de réaliser la tâche finale, comme la classification. Un CNN typique pour la classification d'images est composé de plusieurs blocs de couches de convolution, d'activation, et de regroupement, suivis de quelques couches entièrement connectées. La dernière couche entièrement connectée produit un vecteur de scores, qui représente la probabilité d'appartenance à chaque classe. La fonction de coût utilisée est généralement l'entropie croisée, qui mesure la divergence entre la distribution des scores et la distribution des classes réelles. Le réseau apprend ses paramètres en utilisant la rétroprop> agation du gradient, qui consiste à calculer le gradient de la fonction de coût par rapport aux paramètres, en utilisant la règle de la chaîne, et à mettre à jour les paramètres en suivant la direction opposée au gradient.\\n\\nModèles et techniques Les CNN ont été popularisés par le modèle AlexNet, qui a remporté le défi ImageNet en 2012, en battant les méthodes classiques basées sur des caractéristiques manuelles. ImageNet est un jeu de données d'images de grande taille, qui contient plus de 14 millions d'images, réparties en plus de 20 000 classes. Le modèle AlexNet est composé de huit couches, dont cinq couches de convolution et trois couches entièrement connectées, et utilise la fonction ReLU comme fonction d'activation, ainsi que des techniques de régularisation, comme le dropout et la normalisation des lots. Le modèle AlexNet a permis de réduire l'erreur de classification sur ImageNet de plus de 10 points.\\n\\nDepuis, de nombreux modèles de CNN ont été proposés, en augmentant la profondeur, la largeur, ou la complexité du réseau, pour améliorer les performances sur la classification d'images. Parmi ces modèles, on peut citer VGG, qui utilise des filtres de taille 3x3 et des couches de convolution successives, Inception, qui utilise des blocs modulaires avec des branches parallèles de différentes tailles, ResNet, qui utilise des connexions résiduelles qui permettent de sauter des couches, DenseNet, qui utilise des connexions denses qui relient toutes les couches entre elles, ou encore EfficientNet, qui utilise une recherche automatique de l'architecture optimale du réseau. Ces modèles ont atteint des niveaux de performance proches ou supérieurs à ceux des humains sur la classification d'images.\", metadata={'source': 'microsoft_learn\\\\Paul\\\\Le deep learning pour les images.pdf'}),\n",
       " Document(page_content=\"Outre les modèles, plusieurs techniques ont été développées pour améliorer le deep learning pour la classification d'images. Parmi ces techniques, on peut citer l'augmentation des données, qui consiste à appliquer des transformations aléatoires sur les images d'entraînement, comme le recadrage, le retournement, la rotation, le bruit, ou la variation des couleurs, pour augmenter la diversité et la robustesse du modèle. On peut aussi citer le transfert d'apprentissage, qui consiste à utiliser un modèle pré-entraîné sur un jeu de données de grande taille, comme ImageNet, et à l'adapter à un jeu de données de plus petite taille, en conservant les couches de convolution, qui captent des caractéristiques génériques, et en ré-entraînant les couches entièrement connectées, qui captent des caractéristiques spécifiques. On peut également> citer l'apprentissage par auto-étiquetage, qui consiste à utiliser un modèle pré-entraîné pour générer des étiquettes pour des images non annotées, et à les utiliser comme données d'entraînement supplémentaires.\\n\\nDétection d'objets La détection d'objets est la tâche qui consiste à identifier et à localiser les objets présents dans une image, en leur attribuant une classe et une boîte englobante. Par exemple, on peut vouloir détecter les personnes, les voitures, ou les animaux dans une image. La détection d'objets peut être vue comme un problème de régression supervisée, où l'on dispose d'un jeu de données d'images annotées avec les classes et les boîtes englobantes des objets, et où l'on cherche à apprendre un modèle qui prédit les classes et les boîtes englobantes des objets dans une image inconnue. Le deep learning a permis de réaliser des progrès significatifs dans la détection d'objets, en utilisant des modèles basés sur des CNN, qui peuvent être classés en deux catégories : les modèles à deux étapes, qui utilisent une région d'intérêt pour localiser les objets, et les modèles à une étape, qui détectent les objets directement à partir de l'image entière.\\n\\nModèles à deux étapes Les modèles à deux étapes sont des modèles qui utilisent une région d'intérêt (RoI), qui est une sous- région de l'image qui contient potentiellement un objet, pour localiser les objets. Les modèles à deux étapes sont composés de deux parties : un extracteur de caractéristiques, qui utilise un CNN pour extraire des caractéristiques de l'image entière, et un détecteur de région, qui utilise un autre CNN pour détecter les objets à l'intérieur de chaque RoI. Les modèles à deux étapes sont généralement plus précis que les modèles à une étape, mais aussi plus lents.\\n\\nLe modèle R-CNN est le premier modèle à deux étapes, qui utilise un algorithme de segmentation, comme le recherche sélective, pour générer environ 2000 RoI par image, et qui utilise un CNN pré- entraîné, comme VGG ou AlexNet, pour extraire des caractéristiques de chaque RoI. Le modèle R-CNN utilise ensuite un classifieur SVM, qui utilise les caractéristiques extraites comme entrée, pour prédire la classe de chaque RoI, et un régresseur linéaire, qui utilise également les caractéristiques extraites comme entrée, pour affiner les boîtes englobantes de chaque RoI. Le> modèle R-CNN a permis de réduire l'erreur de détection sur le jeu de données PASCAL VOC, qui est un jeu de données de référence pour la détection d'objets, qui contient environ 20 000 images, réparties en 20 classes. Cependant, le modèle R-CNN est très lent, car il nécessite d'exécuter le CNN sur chaque RoI individuellement, et utilise plusieurs composants distincts, qui doivent être entraînés séparément.\\n\\nLe modèle Fast R-CNN améliore le modèle R-CNN, en utilisant une couche de regroupement de région (RoIPool), qui permet de réduire le nombre d'exécutions du CNN, en extrayant les caractéristiques de\", metadata={'source': 'microsoft_learn\\\\Paul\\\\Le deep learning pour les images.pdf'}),\n",
       " Document(page_content=\"Le modèle Fast R-CNN améliore le modèle R-CNN, en utilisant une couche de regroupement de région (RoIPool), qui permet de réduire le nombre d'exécutions du CNN, en extrayant les caractéristiques de\\n\\nl'image entière une seule fois, et en les partageant entre les différentes RoI. Le modèle Fast R-CNN utilise également un seul réseau, qui combine le classifieur et le régresseur, et qui peut être entraîné de bout en bout. Le modèle Fast R-CNN est plus rapide et plus précis que le modèle R-CNN, mais il dépend toujours d'un algorithme externe pour générer les RoI, ce qui limite sa performance et sa flexibilité.\\n\\nLe modèle Faster R-CNN résout le problème du modèle Fast R-CNN, en utilisant un réseau de proposition de région (RPN), qui est un autre CNN, qui prend les caractéristiques extraites par l'extracteur de caractéristiques comme entrée, et qui prédit les RoI à partir de plusieurs ancres, qui sont des boîtes englobantes de tailles et de ratios variés, situées à différentes positions de l'image. Le modèle Faster R- CNN utilise ensuite la même couche RoIPool et le même détecteur de région que le modèle Fast R-CNN, pour prédire la classe et la boîte englobante de chaque RoI. Le modèle Faster R-CNN est plus rapide et plus précis que le modèle Fast R-CNN, car il utilise un seul réseau, qui peut être entraîné de bout en bout, et qui génère les RoI de manière adaptative.\\n\\nModèles à une étape Les modèles à une étape sont des modèles qui détectent les objets directement à partir de l'image entière, sans utiliser de RoI. Les modèles à une étape sont composés d'un seul réseau, qui utilise un CNN pour extraire des caractéristiques de l'image, et qui prédit la classe et la boîte englobante de chaque objet à partir de plusieurs ancres, comme le modèle Faster R-CNN. Les modèles à une étape sont généralement plus rapides que les modèles à deux étapes, mais aussi moins précis.\\n\\nLe modèle YOLO est le premier modèle à une étape, qui divise l'image> en une grille de cellules, et qui prédit la classe et la boîte englobante de chaque objet à partir de plusieurs ancres par cellule. Le modèle YOLO utilise un seul réseau, qui combine l'extracteur de caractéristiques et le détecteur d'objets, et qui peut être entraîné de bout en bout. Le modèle YOLO est très rapide, car il traite l'image entière en une seule fois, mais il a tendance à manquer les petits objets, ou les objets proches les uns des autres, car il limite le nombre d'objets par cellule.\\n\\nLe modèle SSD améliore le modèle YOLO, en utilisant plusieurs cartes de caractéristiques à différentes échelles, pour détecter les objets de différentes tailles, et en utilisant plus d'ancres par cellule, pour détecter les objets de différents ratios. Le modèle SSD utilise également un seul réseau, qui combine l'extracteur de caractéristiques et le détecteur d'objets, et qui peut être entraîné de bout en bout. Le modèle SSD est plus précis que le modèle YOLO, car il capture mieux les objets de différentes tailles et formes, mais il est aussi plus lent.\\n\\nConclusion Le deep learning pour la classification d'images et la détection d'objets est un domaine en constante évolution, qui continue de proposer des modèles et des techniques innovants, pour relever les défis liés à la complexité et à la diversité des données visuelles.\", metadata={'source': 'microsoft_learn\\\\Paul\\\\Le deep learning pour les images.pdf'}),\n",
       " Document(page_content=\"Les modèles de fondation de l’IA générative\\n\\nLes modèles de fondation de l’IA générative sont des modèles d’intelligence artificielle capables de générer du contenu à partir de données brutes, comme du texte, des images, des sons ou des vidéos. Ces modèles utilisent des techniques d’apprentissage profond pour apprendre les caractéristiques et les relations des données, et pour produire de nouveaux exemples qui respectent les mêmes règles. Les modèles de fondation de l’IA générative sont considérés comme une avancée majeure dans le domaine de l’IA, car ils permettent de créer des applications innovantes et créatives dans divers domaines.\\n\\nParmi les modèles de fondation de l’IA générative, les modèles de génération d’images sont ceux qui visent à créer des images réalistes et diversifiées à partir de données brutes ou conditionnées. Par exemple, un modèle de génération d’images peut prendre une description textuelle comme entrée et produire une image qui correspond au texte. Ou bien, il peut prendre une image existante et la modifier selon un style, une couleur ou un attribut donné. Les modèles de génération d’images peuvent être basés sur des réseaux de neurones convolutifs (CNN), qui sont capables de traiter et de reconnaître les caractéristiques visuelles des images, ou sur des réseaux antagonistes génératifs (GAN), qui sont des systèmes composés de deux réseaux concurrents : un générateur qui crée les images et un discriminateur qui les évalue. Les modèles de génération d’images offrent de nombreuses possibilités d’application, comme la synthèse de visages, le dessin assisté, la colorisation, la restauration ou la super- résolution d’images.\\n\\nLes LLM, ou masters of laws, sont des diplômes de droit de niveau supérieur qui permettent aux étudiants et aux professionnels d'approfondir leurs connaissances juridiques dans un domaine spécifique. Les LLM sont généralement proposés par des universités prestigieuses, et sont reconnus internationalement comme des qualifications académiques de haut niveau. Les LLM sont souvent destinés aux personnes qui souhaitent se spécialiser dans un domaine du droit, comme le droit international, le droit des affaires, le droit fiscal, le droit de la propriété intellectuelle ou le droit des droits de l'homme. Les LLM peuvent également servir à acquérir une expérience juridique dans un pays ou une région différente, ou à se préparer à un doctorat en droit. Les LLM offrent de nombreux avantages aux étudiants et aux professionnels, comme l'amélioration de leurs compétences juridiques, la mise en réseau avec des pairs et des experts, l'ouverture de nouvelles opportunités de carrière ou de recherche, ou l'augmentation de leur crédibilité et de leur réputation.\\n\\nLe service Azure OpenAI : un outil puissant pour l'intelligence artificielle\\n\\nUn document qui présente le service Azure OpenAI, ses caractéristiques, ses bénéfices et ses applications.\\n\\nQu'est-ce que le service Azure OpenAI ? Le service Azure OpenAI est un service cloud qui permet aux développeurs et aux chercheurs d'accéder à la plateforme OpenAI, une initiative qui vise à créer une intelligence artificielle générale (AGI) bénéfique pour l'humanité. Le service Azure OpenAI offre la possibilité d'utiliser des modèles d'IA pré-entraînés ou personnalisés, de les déployer sur le cloud ou en périphérie, et de les intégrer à des applications et des scénarios variés.\", metadata={'source': 'microsoft_learn\\\\Paul\\\\Le service Azure OpenAI.pdf'}),\n",
       " Document(page_content=\"Comment fonctionne le service Azure OpenAI ? Le service Azure OpenAI repose sur deux composants principaux : le portail Azure OpenAI et les API Azure OpenAI. Le portail Azure OpenAI est une interface web qui permet de créer, de gérer et de surveiller les ressources du service Azure OpenAI, telles que les abonnements, les clés, les modèles, les points de terminaison et les requêtes. Les API Azure OpenAI sont des interfaces de programmation qui permettent d'interagir avec les modèles d'IA du service Azure OpenAI, tels que GPT-3, Codex, DALL-E ou CLIP. Les API Azure OpenAI sont accessibles via des requêtes HTTP ou des SDK dans différents langages de programmation, tels que Python, Java, C#, Node.js ou Ruby.\\n\\nQuels sont les avantages du service Azure OpenAI ? Le service Azure OpenAI présente plusieurs avantages, parmi lesquels :\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIl permet d'accéder à des modèles d'IA de pointe, capables de réaliser des tâches complexes et variées, telles que la génération de texte, la compréhension du langage naturel, la vision par ordinateur, la synthèse vocale ou la création de contenu multimédia. Il offre une grande flexibilité, en permettant de choisir entre des modèles pré-entraînés ou personnalisés, et de les adapter aux besoins spécifiques de chaque projet ou domaine. Il garantit une haute performance, en s'appuyant sur l'infrastructure cloud de Microsoft Azure, qui assure une disponibilité, une scalabilité et une sécurité optimales. Il facilite l'intégration, en proposant des API simples et des SDK dans différents langages de programmation, qui permettent de connecter les modèles d'IA du service Azure OpenAI à des applications et des scénarios existants ou nouveaux.\\n\\nQuels sont les cas d'utilisation du service Azure OpenAI ? Le service Azure OpenAI peut être utilisé pour de nombreux cas d'utilisation, dans différents domaines et secteurs d'activité. Voici quelques exemples :\\n\\nDans le domaine de l'éducation, le service Azure OpenAI peut être utilisé pour créer des outils\\n\\npédagogiques, tels que des assistants de rédaction, des générateurs de questions, des correcteurs automatiques ou des tuteurs virtuels.\\n\\nDans le domaine de la santé, le service Azure OpenAI peut être utilisé pour créer des outils d'aide au diagnostic, à la prescription, à la recherche ou à la prévention, en exploitant les données médicales et les connaissances scientifiques.\\n\\nDans le domaine du divertissement, le service Azure OpenAI peut être utilisé pour créer des outils de génération de contenu, tels que des scénarios, des dialogues, des personnages, des musiques ou des images.\\n\\nDans le domaine du commerce, le service Azure OpenAI peut être utilisé pour créer des outils\\n\\nd'optimisation, tels que des recommandations personnalisées, des prévisions de ventes, des analyses de marché ou des chatbots.\\n\\nComment mettre en œuvre le service Azure OpenAI ? Pour mettre en œuvre le service Azure OpenAI, il faut suivre les étapes suivantes :\\n\\nCréer un compte Microsoft Azure, si ce n'est pas déjà fait, et se connecter au portail Azure OpenAI.\\n\\nSouscrire à un abonnement au service Azure OpenAI, en choisissant le niveau de tarification\\n\\nadapté à ses besoins.\\n\\nCréer une clé d'API, qui permettra d'authentifier les requêtes au service Azure OpenAI. • Choisir un modèle d'IA, parmi ceux proposés par le service Azure OpenAI, ou créer son propre modèle, en utilisant les outils de formation et de réglage fournis par le service Azure OpenAI.\\n\\nCréer un point de terminaison, qui permettra d'exposer le modèle d'IA sur le cloud ou en\\n\\npériphérie, et de le rendre accessible via une URL.\\n\\nEnvoyer des requêtes au point de terminaison, en utilisant les API Azure OpenAI ou les SDK dans le langage de programmation de son choix.\\n\\nAnalyser les résultats des requêtes, en utilisant les outils de suivi et de statistiques fournis par le\\n\\nportail Azure OpenAI.\\n\\nLes services cognitifs Azure\", metadata={'source': 'microsoft_learn\\\\Paul\\\\Le service Azure OpenAI.pdf'}),\n",
       " Document(page_content=\"Analyser les résultats des requêtes, en utilisant les outils de suivi et de statistiques fournis par le\\n\\nportail Azure OpenAI.\\n\\nLes services cognitifs Azure\\n\\nLes services cognitifs Azure sont un ensemble de services cloud qui permettent aux développeurs de créer des applications intelligentes, en utilisant les capacités d'intelligence artificielle (IA) et de machine learning (ML) fournies par Microsoft. Les services cognitifs Azure couvrent différents domaines, tels que la vision, le langage, la recherche, la décision et la synthèse vocale. Ils offrent des API, des SDK et des outils faciles à utiliser, pour intégrer les fonctionnalités d'IA et de ML dans les applications web, mobiles ou de bureau.\\n\\nLa vision Le domaine de la vision regroupe les services cognitifs Azure qui permettent aux applications de comprendre et d'analyser le contenu visuel, comme les images, les vidéos ou les flux en direct. Les services cognitifs Azure de la vision comprennent :\\n\\nLe service Vision par ordinateur, qui fournit des fonctions de détection, d'analyse, de description et de balisage des images, ainsi que de reconnaissance optique de caractères (OCR) et de génération de vignettes.\\n\\nLe service Reconnaissance faciale, qui permet d'identifier, de vérifier, de rechercher et de regrouper des visages dans des images ou des vidéos, ainsi que de détecter des attributs faciaux, comme l'âge, le genre, l'émotion ou les accessoires.\\n\\nLe service Analyseur vidéo, qui offre des capacités d'analyse avancée des vidéos, comme la détection et le suivi des visages, des objets, des logos, des scènes ou des activités, ainsi que la transcription, la traduction et la synthèse vocale des pistes audio.\\n\\nLe service Form Recognizer, qui permet d'extraire et de structurer les données clés contenues dans des formulaires papier ou numériques, comme des factures, des reçus, des chèques ou des cartes d'identité.\\n\\nLe service Custom Vision, qui permet de personnaliser les modèles de vision par ordinateur et de reconnaissance faciale, en les entraînant avec ses propres données, afin de répondre à des besoins spécifiques.\\n\\nLe langage Le domaine du langage regroupe les services cognitifs Azure qui permettent aux applications de comprendre et de manipuler le langage naturel, c'est-à-dire le langage parlé ou écrit par les humains. Les services cognitifs Azure du langage comprennent :\\n\\nLe service Analyse de texte, qui fournit des fonctions d'analyse s>yntaxique, sémantique et sentimentale des textes, ainsi que de détection des entités nommées, des mots-clés, des liens ou des langues.\\n\\nLe service Reconnaissance vocale, qui permet de transcrire le discours en texte, en reconnaissant les mots prononcés, le locuteur, l'intention ou les commandes.\\n\\nLe service Traduction vocale, qui permet de traduire le discours d'une langue à une autre, en conservant la voix et le ton du locuteur.\\n\\nLe service Génération de langage naturel, qui permet de produire du texte à partir de données structurées, comme des tableaux, des graphiques ou des images, en utilisant des modèles pré-entraînés ou personnalisés.\\n\\nLe service Linguistic Analysis, qui fournit des fonctions d'analyse morphologique, syntaxique et sémantique des textes, en utilisant des règles et des dictionnaires linguistiques.\\n\\nLe service QnA Maker, qui permet de créer des agents conversationnels capables de répondre aux questions des utilisateurs, en se basant sur des sources de connaissances existantes, comme des documents, des sites web ou des FAQ.\\n\\nLes bases de données vectorielles\", metadata={'source': 'microsoft_learn\\\\Paul\\\\Le service Azure OpenAI.pdf'}),\n",
       " Document(page_content=\"Les bases de données vectorielles\\n\\nLes bases de données vectorielles sont des systèmes de gestion de données qui stockent et manipulent des vecteurs, c'est-à-dire des séquences de nombres réels qui représentent des entités ou des objets. Ces vecteurs peuvent être obtenus à partir de diverses sources, comme des images, des textes, des sons, des vidéos, ou des signaux biométriques. Les bases de données vectorielles permettent d'effectuer des opérations efficaces sur les vecteurs, comme la recherche par similarité, le calcul de distances, le regroupement, la classification, ou la recommandation.\\n\\nLes bases de données vectorielles sont utilisées dans de nombreux domaines d'application, comme la vision par ordinateur, le traitement du langage naturel, la bioinformatique, ou le commerce électronique. Elles offrent plusieurs avantages par rapport aux bases de données traditionnelles, comme la réduction de la dimensionnalité, la généralisation, la robustesse au bruit, ou la facilité d'interprétation. Elles présentent aussi des défis spécifiques, comme le choix des méthodes de vectorisation, l'indexation des vecteurs de grande dimension, ou la garantie de la sécurité et de la confidentialité des données.\\n\\nDans cet article, nous allons présenter les principes et les caractéristiques des bases de données vectorielles, ainsi que les principales techniques et outils utilisés pour les mettre en œuvre. Nous allons également illustrer les applications et les bénéfices des bases de données vectorielles à travers quelques exemples concrets.\\n\\nPrincipes et caractéristiques des bases de données vectorielles Une base de données vectorielle est un système de gestion de données qui stocke et manipule des vecteurs, c'est-à-dire des séquences ordonnées de nombres réels qui représentent des entités ou des objets. Un vecteur peut avoir une dimension fixe ou variable, selon le type de données qu'il encode. Par exemple, un vecteur peut représenter les pixels d'une image, les mots d'un texte, les fréquences d'un son, ou les caractéristiques d'un produit.\\n\\nLes vecteurs sont généralement obtenus à partir de données brutes, comme des fichiers multimédias, des documents, ou des flux de données, en utilisant des méthodes de vectorisation, qui transforment les données en vecteurs selon certains critères. Il existe différentes méthodes de vectorisation, selon le domaine et le but de l'application. Par exemple, on peut utiliser des méthodes basées sur des descripteurs, qui extraient des caractéristiques pertinentes des données, comme les couleurs, les formes, ou les textures pour les images, ou les mots, les phrases, ou les concepts> pour les textes. On peut aussi utiliser des méthodes basées sur des modèles, qui apprennent des représentations vectorielles à partir de données massives, en utilisant des techniques d'apprentissage automatique, comme les réseaux de neurones, les modèles probabilistes, ou les modèles géométriques.\\n\\nLe deep learning pour la classification d'images et la détection d'objets\\n\\nIntroduction La classification d'images et la détection d'objets sont deux tâches importantes de la vision par ordinateur, qui consistent à identifier et à localiser les objets présents dans une image. Ces tâches ont de nombreuses applications pratiques, comme la reconnaissance faciale, la surveillance, la robotique, ou la médecine. Le deep learning, qui est une branche de l'apprentissage automatique basée sur des modèles profonds, a révolutionné le domaine de la vision par ordinateur, en offrant des performances supérieures aux méthodes classiques, basées sur des caractéristiques manuelles ou des modèles simples. Dans ce document, nous allons présenter les principes et les caractéristiques du deep learning pour la classification d'images et la détection d'objets, ainsi que les principaux modèles et techniques utilisés.\", metadata={'source': 'microsoft_learn\\\\Paul\\\\Le service Azure OpenAI.pdf'}),\n",
       " Document(page_content=\"Principes du deep learning Le deep learning est une méthode d'apprentissage automatique qui utilise des réseaux de neurones artificiels, qui sont des structures composées de couches de neurones interconnectés, qui reçoivent des données en entrée et produisent des sorties en fonction de paramètres appris. Un réseau de neurones peut être vu comme une fonction non linéaire complexe, qui approxime une relation entre les données d'entrée et les données de sortie. Le réseau de neurones apprend ses paramètres à partir d'un ensemble d'exemples, appelé jeu de données, en utilisant un algorithme d'optimisation, généralement basé sur la descente de gradient, qui minimise une fonction de coût, qui mesure l'erreur entre les sorties prédites et les sorties attendues. Le réseau de neurones est dit profond lorsque le nombre de couches est élevé, ce qui lui permet de capturer des niveaux de plus en plus abstraits et complexes de représentation des données. Le deep learning se distingue des autres méthodes d'apprentissage automatique par sa capacité à apprendre directement à partir des données brutes, sans avoir besoin de définir des caractéristiques spécifiques au domaine.\\n\\nClassification d'images La classification d'images est la tâche qui consiste à attribuer une étiquette à une image, parmi un ensemble de classes prédéfinies. Par exemple, on peut vouloir classer une image selon qu'elle contient un chat, un chien, ou un oiseau. La classification d'images peut être vue comme un problème de classification supervisée, où l'on dispose d'un jeu de données d'images annotées avec leurs classes respectives, et où l'on cherche à apprendre un> modèle qui prédit la classe d'une image inconnue. Le deep learning a permis de réaliser des progrès significatifs dans la classification d'images, en utilisant des réseaux de neurones convolutifs, qui sont des modèles spécialisés pour le traitement des données visuelles.\", metadata={'source': 'microsoft_learn\\\\Paul\\\\Le service Azure OpenAI.pdf'}),\n",
       " Document(page_content=\"Réseaux de neurones convolutifs Un réseau de neurones convolutif (CNN) est un type de réseau de neurones qui utilise des opérations de convolution, qui consistent à appliquer des filtres linéaires sur les données d'entrée, pour extraire des caractéristiques locales et invariantes. Un CNN est composé de plusieurs couches, qui peuvent être de différents types, comme des couches de convolution, des couches d'activation, des couches de regroupement, ou des couches entièrement connectées. Les couches de convolution appliquent des filtres sur les données d'entrée, pour produire des cartes d'activation, qui représentent la réponse du filtre à une région de l'entrée. Les filtres sont appris par le réseau, et sont généralement de petite taille, comme 3x3 ou 5x5 pixels. Les couches d'activation appliquent une fonction non linéaire aux cartes d'activation, pour introduire de la non-linéarité dans le modèle. Les fonctions d'activation les plus courantes sont la fonction sigmoïde, la fonction tangente hyperbolique, ou la fonction ReLU. Les couches de regroupement réduisent la dimensionnalité des cartes d'activation, en appliquant une opération d'agrégation, comme le maximum, la moyenne, ou la norme, sur des régions voisines. Les couches de regroupement permettent de réduire le nombre de paramètres, de limiter le surapprentissage, et d'augmenter l'invariance à la translation. Les couches entièrement connectées sont des couches classiques de réseau de neurones, qui relient tous les neurones d'une couche à ceux de la couche suivante. Les couches entièrement connectées permettent de combiner les caractéristiques extraites par les couches précédentes, et de réaliser la tâche finale, comme la classification. Un CNN typique pour la classification d'images est composé de plusieurs blocs de couches de convolution, d'activation, et de regroupement, suivis de quelques couches entièrement connectées. La dernière couche entièrement connectée produit un vecteur de scores, qui représente la probabilité d'appartenance à chaque classe. La fonction de coût utilisée est généralement l'entropie croisée, qui mesure la divergence entre la distribution des scores et la distribution des classes réelles. Le réseau apprend ses paramètres en utilisant la rétroprop> agation du gradient, qui consiste à calculer le gradient de la fonction de coût par rapport aux paramètres, en utilisant la règle de la chaîne, et à mettre à jour les paramètres en suivant la direction opposée au gradient.\\n\\nModèles et techniques Les CNN ont été popularisés par le modèle AlexNet, qui a remporté le défi ImageNet en 2012, en battant les méthodes classiques basées sur des caractéristiques manuelles. ImageNet est un jeu de données d'images de grande taille, qui contient plus de 14 millions d'images, réparties en plus de 20 000 classes. Le modèle AlexNet est composé de huit couches, dont cinq couches de convolution et trois couches entièrement connectées, et utilise la fonction ReLU comme fonction d'activation, ainsi que des techniques de régularisation, comme le dropout et la normalisation des lots. Le modèle AlexNet a permis de réduire l'erreur de classification sur ImageNet de plus de 10 points.\\n\\nDepuis, de nombreux modèles de CNN ont été proposés, en augmentant la profondeur, la largeur, ou la complexité du réseau, pour améliorer les performances sur la classification d'images. Parmi ces modèles, on peut citer VGG, qui utilise des filtres de taille 3x3 et des couches de convolution successives, Inception, qui utilise des blocs modulaires avec des branches parallèles de différentes tailles, ResNet, qui utilise des connexions résiduelles qui permettent de sauter des couches, DenseNet, qui utilise des connexions denses qui relient toutes les couches entre elles, ou encore EfficientNet, qui utilise une recherche automatique de l'architecture optimale du réseau. Ces modèles ont atteint des niveaux de performance proches ou supérieurs à ceux des humains sur la classification d'images.\", metadata={'source': 'microsoft_learn\\\\Paul\\\\Le service Azure OpenAI.pdf'}),\n",
       " Document(page_content=\"Outre les modèles, plusieurs techniques ont été développées pour améliorer le deep learning pour la classification d'images. Parmi ces techniques, on peut citer l'augmentation des données, qui consiste à appliquer des transformations aléatoires sur les images d'entraînement, comme le recadrage, le retournement, la rotation, le bruit, ou la variation des couleurs, pour augmenter la diversité et la robustesse du modèle. On peut aussi citer le transfert d'apprentissage, qui consiste à utiliser un modèle pré-entraîné sur un jeu de données de grande taille, comme ImageNet, et à l'adapter à un jeu de données de plus petite taille, en conservant les couches de convolution, qui captent des caractéristiques génériques, et en ré-entraînant les couches entièrement connectées, qui captent des caractéristiques spécifiques. On peut également> citer l'apprentissage par auto-étiquetage, qui consiste à utiliser un modèle pré-entraîné pour générer des étiquettes pour des images non annotées, et à les utiliser comme données d'entraînement supplémentaires.\\n\\nDétection d'objets La détection d'objets est la tâche qui consiste à identifier et à localiser les objets présents dans une image, en leur attribuant une classe et une boîte englobante. Par exemple, on peut vouloir détecter les personnes, les voitures, ou les animaux dans une image. La détection d'objets peut être vue comme un problème de régression supervisée, où l'on dispose d'un jeu de données d'images annotées avec les classes et les boîtes englobantes des objets, et où l'on cherche à apprendre un modèle qui prédit les classes et les boîtes englobantes des objets dans une image inconnue. Le deep learning a permis de réaliser des progrès significatifs dans la détection d'objets, en utilisant des modèles basés sur des CNN, qui peuvent être classés en deux catégories : les modèles à deux étapes, qui utilisent une région d'intérêt pour localiser les objets, et les modèles à une étape, qui détectent les objets directement à partir de l'image entière.\\n\\nModèles à deux étapes Les modèles à deux étapes sont des modèles qui utilisent une région d'intérêt (RoI), qui est une sous- région de l'image qui contient potentiellement un objet, pour localiser les objets. Les modèles à deux étapes sont composés de deux parties : un extracteur de caractéristiques, qui utilise un CNN pour extraire des caractéristiques de l'image entière, et un détecteur de région, qui utilise un autre CNN pour détecter les objets à l'intérieur de chaque RoI. Les modèles à deux étapes sont généralement plus précis que les modèles à une étape, mais aussi plus lents.\\n\\nLe modèle R-CNN est le premier modèle à deux étapes, qui utilise un algorithme de segmentation, comme le recherche sélective, pour générer environ 2000 RoI par image, et qui utilise un CNN pré- entraîné, comme VGG ou AlexNet, pour extraire des caractéristiques de chaque RoI. Le modèle R-CNN utilise ensuite un classifieur SVM, qui utilise les caractéristiques extraites comme entrée, pour prédire la classe de chaque RoI, et un régresseur linéaire, qui utilise également les caractéristiques extraites comme entrée, pour affiner les boîtes englobantes de chaque RoI. Le> modèle R-CNN a permis de réduire l'erreur de détection sur le jeu de données PASCAL VOC, qui est un jeu de données de référence pour la détection d'objets, qui contient environ 20 000 images, réparties en 20 classes. Cependant, le modèle R-CNN est très lent, car il nécessite d'exécuter le CNN sur chaque RoI individuellement, et utilise plusieurs composants distincts, qui doivent être entraînés séparément.\\n\\nLe modèle Fast R-CNN améliore le modèle R-CNN, en utilisant une couche de regroupement de région (RoIPool), qui permet de réduire le nombre d'exécutions du CNN, en extrayant les caractéristiques de\", metadata={'source': 'microsoft_learn\\\\Paul\\\\Le service Azure OpenAI.pdf'}),\n",
       " Document(page_content=\"Le modèle Fast R-CNN améliore le modèle R-CNN, en utilisant une couche de regroupement de région (RoIPool), qui permet de réduire le nombre d'exécutions du CNN, en extrayant les caractéristiques de\\n\\nl'image entière une seule fois, et en les partageant entre les différentes RoI. Le modèle Fast R-CNN utilise également un seul réseau, qui combine le classifieur et le régresseur, et qui peut être entraîné de bout en bout. Le modèle Fast R-CNN est plus rapide et plus précis que le modèle R-CNN, mais il dépend toujours d'un algorithme externe pour générer les RoI, ce qui limite sa performance et sa flexibilité.\\n\\nLe modèle Faster R-CNN résout le problème du modèle Fast R-CNN, en utilisant un réseau de proposition de région (RPN), qui est un autre CNN, qui prend les caractéristiques extraites par l'extracteur de caractéristiques comme entrée, et qui prédit les RoI à partir de plusieurs ancres, qui sont des boîtes englobantes de tailles et de ratios variés, situées à différentes positions de l'image. Le modèle Faster R- CNN utilise ensuite la même couche RoIPool et le même détecteur de région que le modèle Fast R-CNN, pour prédire la classe et la boîte englobante de chaque RoI. Le modèle Faster R-CNN est plus rapide et plus précis que le modèle Fast R-CNN, car il utilise un seul réseau, qui peut être entraîné de bout en bout, et qui génère les RoI de manière adaptative.\\n\\nModèles à une étape Les modèles à une étape sont des modèles qui détectent les objets directement à partir de l'image entière, sans utiliser de RoI. Les modèles à une étape sont composés d'un seul réseau, qui utilise un CNN pour extraire des caractéristiques de l'image, et qui prédit la classe et la boîte englobante de chaque objet à partir de plusieurs ancres, comme le modèle Faster R-CNN. Les modèles à une étape sont généralement plus rapides que les modèles à deux étapes, mais aussi moins précis.\\n\\nLe modèle YOLO est le premier modèle à une étape, qui divise l'image> en une grille de cellules, et qui prédit la classe et la boîte englobante de chaque objet à partir de plusieurs ancres par cellule. Le modèle YOLO utilise un seul réseau, qui combine l'extracteur de caractéristiques et le détecteur d'objets, et qui peut être entraîné de bout en bout. Le modèle YOLO est très rapide, car il traite l'image entière en une seule fois, mais il a tendance à manquer les petits objets, ou les objets proches les uns des autres, car il limite le nombre d'objets par cellule.\\n\\nLe modèle SSD améliore le modèle YOLO, en utilisant plusieurs cartes de caractéristiques à différentes échelles, pour détecter les objets de différentes tailles, et en utilisant plus d'ancres par cellule, pour détecter les objets de différents ratios. Le modèle SSD utilise également un seul réseau, qui combine l'extracteur de caractéristiques et le détecteur d'objets, et qui peut être entraîné de bout en bout. Le modèle SSD est plus précis que le modèle YOLO, car il capture mieux les objets de différentes tailles et formes, mais il est aussi plus lent.\\n\\nConclusion Le deep learning pour la classification d'images et la détection d'objets est un domaine en constante évolution, qui continue de proposer des modèles et des techniques innovants, pour relever les défis liés à la complexité et à la diversité des données visuelles.\", metadata={'source': 'microsoft_learn\\\\Paul\\\\Le service Azure OpenAI.pdf'}),\n",
       " Document(page_content=\"Les modèles de fondation de l’IA générative\\n\\nLes modèles de fondation de l’IA générative sont des modèles d’intelligence artificielle capables de générer du contenu à partir de données brutes, comme du texte, des images, des sons ou des vidéos. Ces modèles utilisent des techniques d’apprentissage profond pour apprendre les caractéristiques et les relations des données, et pour produire de nouveaux exemples qui respectent les mêmes règles. Les modèles de fondation de l’IA générative sont considérés comme une avancée majeure dans le domaine de l’IA, car ils permettent de créer des applications innovantes et créatives dans divers domaines.\\n\\nParmi les modèles de fondation de l’IA générative, les modèles de génération d’images sont ceux qui visent à créer des images réalistes et diversifiées à partir de données brutes ou conditionnées. Par exemple, un modèle de génération d’images peut prendre une description textuelle comme entrée et produire une image qui correspond au texte. Ou bien, il peut prendre une image existante et la modifier selon un style, une couleur ou un attribut donné. Les modèles de génération d’images peuvent être basés sur des réseaux de neurones convolutifs (CNN), qui sont capables de traiter et de reconnaître les caractéristiques visuelles des images, ou sur des réseaux antagonistes génératifs (GAN), qui sont des systèmes composés de deux réseaux concurrents : un générateur qui crée les images et un discriminateur qui les évalue. Les modèles de génération d’images offrent de nombreuses possibilités d’application, comme la synthèse de visages, le dessin assisté, la colorisation, la restauration ou la super- résolution d’images.\\n\\nLes LLM, ou masters of laws, sont des diplômes de droit de niveau supérieur qui permettent aux étudiants et aux professionnels d'approfondir leurs connaissances juridiques dans un domaine spécifique. Les LLM sont généralement proposés par des universités prestigieuses, et sont reconnus internationalement comme des qualifications académiques de haut niveau. Les LLM sont souvent destinés aux personnes qui souhaitent se spécialiser dans un domaine du droit, comme le droit international, le droit des affaires, le droit fiscal, le droit de la propriété intellectuelle ou le droit des droits de l'homme. Les LLM peuvent également servir à acquérir une expérience juridique dans un pays ou une région différente, ou à se préparer à un doctorat en droit. Les LLM offrent de nombreux avantages aux étudiants et aux professionnels, comme l'amélioration de leurs compétences juridiques, la mise en réseau avec des pairs et des experts, l'ouverture de nouvelles opportunités de carrière ou de recherche, ou l'augmentation de leur crédibilité et de leur réputation.\\n\\nLe service Azure OpenAI : un outil puissant pour l'intelligence artificielle\\n\\nUn document qui présente le service Azure OpenAI, ses caractéristiques, ses bénéfices et ses applications.\\n\\nQu'est-ce que le service Azure OpenAI ? Le service Azure OpenAI est un service cloud qui permet aux développeurs et aux chercheurs d'accéder à la plateforme OpenAI, une initiative qui vise à créer une intelligence artificielle générale (AGI) bénéfique pour l'humanité. Le service Azure OpenAI offre la possibilité d'utiliser des modèles d'IA pré-entraînés ou personnalisés, de les déployer sur le cloud ou en périphérie, et de les intégrer à des applications et des scénarios variés.\", metadata={'source': 'microsoft_learn\\\\Paul\\\\Les bases de données vectorielles.pdf'}),\n",
       " Document(page_content=\"Comment fonctionne le service Azure OpenAI ? Le service Azure OpenAI repose sur deux composants principaux : le portail Azure OpenAI et les API Azure OpenAI. Le portail Azure OpenAI est une interface web qui permet de créer, de gérer et de surveiller les ressources du service Azure OpenAI, telles que les abonnements, les clés, les modèles, les points de terminaison et les requêtes. Les API Azure OpenAI sont des interfaces de programmation qui permettent d'interagir avec les modèles d'IA du service Azure OpenAI, tels que GPT-3, Codex, DALL-E ou CLIP. Les API Azure OpenAI sont accessibles via des requêtes HTTP ou des SDK dans différents langages de programmation, tels que Python, Java, C#, Node.js ou Ruby.\\n\\nQuels sont les avantages du service Azure OpenAI ? Le service Azure OpenAI présente plusieurs avantages, parmi lesquels :\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIl permet d'accéder à des modèles d'IA de pointe, capables de réaliser des tâches complexes et variées, telles que la génération de texte, la compréhension du langage naturel, la vision par ordinateur, la synthèse vocale ou la création de contenu multimédia. Il offre une grande flexibilité, en permettant de choisir entre des modèles pré-entraînés ou personnalisés, et de les adapter aux besoins spécifiques de chaque projet ou domaine. Il garantit une haute performance, en s'appuyant sur l'infrastructure cloud de Microsoft Azure, qui assure une disponibilité, une scalabilité et une sécurité optimales. Il facilite l'intégration, en proposant des API simples et des SDK dans différents langages de programmation, qui permettent de connecter les modèles d'IA du service Azure OpenAI à des applications et des scénarios existants ou nouveaux.\\n\\nQuels sont les cas d'utilisation du service Azure OpenAI ? Le service Azure OpenAI peut être utilisé pour de nombreux cas d'utilisation, dans différents domaines et secteurs d'activité. Voici quelques exemples :\\n\\nDans le domaine de l'éducation, le service Azure OpenAI peut être utilisé pour créer des outils\\n\\npédagogiques, tels que des assistants de rédaction, des générateurs de questions, des correcteurs automatiques ou des tuteurs virtuels.\\n\\nDans le domaine de la santé, le service Azure OpenAI peut être utilisé pour créer des outils d'aide au diagnostic, à la prescription, à la recherche ou à la prévention, en exploitant les données médicales et les connaissances scientifiques.\\n\\nDans le domaine du divertissement, le service Azure OpenAI peut être utilisé pour créer des outils de génération de contenu, tels que des scénarios, des dialogues, des personnages, des musiques ou des images.\\n\\nDans le domaine du commerce, le service Azure OpenAI peut être utilisé pour créer des outils\\n\\nd'optimisation, tels que des recommandations personnalisées, des prévisions de ventes, des analyses de marché ou des chatbots.\\n\\nComment mettre en œuvre le service Azure OpenAI ? Pour mettre en œuvre le service Azure OpenAI, il faut suivre les étapes suivantes :\\n\\nCréer un compte Microsoft Azure, si ce n'est pas déjà fait, et se connecter au portail Azure OpenAI.\\n\\nSouscrire à un abonnement au service Azure OpenAI, en choisissant le niveau de tarification\\n\\nadapté à ses besoins.\\n\\nCréer une clé d'API, qui permettra d'authentifier les requêtes au service Azure OpenAI. • Choisir un modèle d'IA, parmi ceux proposés par le service Azure OpenAI, ou créer son propre modèle, en utilisant les outils de formation et de réglage fournis par le service Azure OpenAI.\\n\\nCréer un point de terminaison, qui permettra d'exposer le modèle d'IA sur le cloud ou en\\n\\npériphérie, et de le rendre accessible via une URL.\\n\\nEnvoyer des requêtes au point de terminaison, en utilisant les API Azure OpenAI ou les SDK dans le langage de programmation de son choix.\\n\\nAnalyser les résultats des requêtes, en utilisant les outils de suivi et de statistiques fournis par le\\n\\nportail Azure OpenAI.\\n\\nLes services cognitifs Azure\", metadata={'source': 'microsoft_learn\\\\Paul\\\\Les bases de données vectorielles.pdf'}),\n",
       " Document(page_content=\"Analyser les résultats des requêtes, en utilisant les outils de suivi et de statistiques fournis par le\\n\\nportail Azure OpenAI.\\n\\nLes services cognitifs Azure\\n\\nLes services cognitifs Azure sont un ensemble de services cloud qui permettent aux développeurs de créer des applications intelligentes, en utilisant les capacités d'intelligence artificielle (IA) et de machine learning (ML) fournies par Microsoft. Les services cognitifs Azure couvrent différents domaines, tels que la vision, le langage, la recherche, la décision et la synthèse vocale. Ils offrent des API, des SDK et des outils faciles à utiliser, pour intégrer les fonctionnalités d'IA et de ML dans les applications web, mobiles ou de bureau.\\n\\nLa vision Le domaine de la vision regroupe les services cognitifs Azure qui permettent aux applications de comprendre et d'analyser le contenu visuel, comme les images, les vidéos ou les flux en direct. Les services cognitifs Azure de la vision comprennent :\\n\\nLe service Vision par ordinateur, qui fournit des fonctions de détection, d'analyse, de description et de balisage des images, ainsi que de reconnaissance optique de caractères (OCR) et de génération de vignettes.\\n\\nLe service Reconnaissance faciale, qui permet d'identifier, de vérifier, de rechercher et de regrouper des visages dans des images ou des vidéos, ainsi que de détecter des attributs faciaux, comme l'âge, le genre, l'émotion ou les accessoires.\\n\\nLe service Analyseur vidéo, qui offre des capacités d'analyse avancée des vidéos, comme la détection et le suivi des visages, des objets, des logos, des scènes ou des activités, ainsi que la transcription, la traduction et la synthèse vocale des pistes audio.\\n\\nLe service Form Recognizer, qui permet d'extraire et de structurer les données clés contenues dans des formulaires papier ou numériques, comme des factures, des reçus, des chèques ou des cartes d'identité.\\n\\nLe service Custom Vision, qui permet de personnaliser les modèles de vision par ordinateur et de reconnaissance faciale, en les entraînant avec ses propres données, afin de répondre à des besoins spécifiques.\\n\\nLe langage Le domaine du langage regroupe les services cognitifs Azure qui permettent aux applications de comprendre et de manipuler le langage naturel, c'est-à-dire le langage parlé ou écrit par les humains. Les services cognitifs Azure du langage comprennent :\\n\\nLe service Analyse de texte, qui fournit des fonctions d'analyse s>yntaxique, sémantique et sentimentale des textes, ainsi que de détection des entités nommées, des mots-clés, des liens ou des langues.\\n\\nLe service Reconnaissance vocale, qui permet de transcrire le discours en texte, en reconnaissant les mots prononcés, le locuteur, l'intention ou les commandes.\\n\\nLe service Traduction vocale, qui permet de traduire le discours d'une langue à une autre, en conservant la voix et le ton du locuteur.\\n\\nLe service Génération de langage naturel, qui permet de produire du texte à partir de données structurées, comme des tableaux, des graphiques ou des images, en utilisant des modèles pré-entraînés ou personnalisés.\\n\\nLe service Linguistic Analysis, qui fournit des fonctions d'analyse morphologique, syntaxique et sémantique des textes, en utilisant des règles et des dictionnaires linguistiques.\\n\\nLe service QnA Maker, qui permet de créer des agents conversationnels capables de répondre aux questions des utilisateurs, en se basant sur des sources de connaissances existantes, comme des documents, des sites web ou des FAQ.\\n\\nLes bases de données vectorielles\", metadata={'source': 'microsoft_learn\\\\Paul\\\\Les bases de données vectorielles.pdf'}),\n",
       " Document(page_content=\"Les bases de données vectorielles\\n\\nLes bases de données vectorielles sont des systèmes de gestion de données qui stockent et manipulent des vecteurs, c'est-à-dire des séquences de nombres réels qui représentent des entités ou des objets. Ces vecteurs peuvent être obtenus à partir de diverses sources, comme des images, des textes, des sons, des vidéos, ou des signaux biométriques. Les bases de données vectorielles permettent d'effectuer des opérations efficaces sur les vecteurs, comme la recherche par similarité, le calcul de distances, le regroupement, la classification, ou la recommandation.\\n\\nLes bases de données vectorielles sont utilisées dans de nombreux domaines d'application, comme la vision par ordinateur, le traitement du langage naturel, la bioinformatique, ou le commerce électronique. Elles offrent plusieurs avantages par rapport aux bases de données traditionnelles, comme la réduction de la dimensionnalité, la généralisation, la robustesse au bruit, ou la facilité d'interprétation. Elles présentent aussi des défis spécifiques, comme le choix des méthodes de vectorisation, l'indexation des vecteurs de grande dimension, ou la garantie de la sécurité et de la confidentialité des données.\\n\\nDans cet article, nous allons présenter les principes et les caractéristiques des bases de données vectorielles, ainsi que les principales techniques et outils utilisés pour les mettre en œuvre. Nous allons également illustrer les applications et les bénéfices des bases de données vectorielles à travers quelques exemples concrets.\\n\\nPrincipes et caractéristiques des bases de données vectorielles Une base de données vectorielle est un système de gestion de données qui stocke et manipule des vecteurs, c'est-à-dire des séquences ordonnées de nombres réels qui représentent des entités ou des objets. Un vecteur peut avoir une dimension fixe ou variable, selon le type de données qu'il encode. Par exemple, un vecteur peut représenter les pixels d'une image, les mots d'un texte, les fréquences d'un son, ou les caractéristiques d'un produit.\\n\\nLes vecteurs sont généralement obtenus à partir de données brutes, comme des fichiers multimédias, des documents, ou des flux de données, en utilisant des méthodes de vectorisation, qui transforment les données en vecteurs selon certains critères. Il existe différentes méthodes de vectorisation, selon le domaine et le but de l'application. Par exemple, on peut utiliser des méthodes basées sur des descripteurs, qui extraient des caractéristiques pertinentes des données, comme les couleurs, les formes, ou les textures pour les images, ou les mots, les phrases, ou les concepts> pour les textes. On peut aussi utiliser des méthodes basées sur des modèles, qui apprennent des représentations vectorielles à partir de données massives, en utilisant des techniques d'apprentissage automatique, comme les réseaux de neurones, les modèles probabilistes, ou les modèles géométriques.\\n\\nLe deep learning pour la classification d'images et la détection d'objets\\n\\nIntroduction La classification d'images et la détection d'objets sont deux tâches importantes de la vision par ordinateur, qui consistent à identifier et à localiser les objets présents dans une image. Ces tâches ont de nombreuses applications pratiques, comme la reconnaissance faciale, la surveillance, la robotique, ou la médecine. Le deep learning, qui est une branche de l'apprentissage automatique basée sur des modèles profonds, a révolutionné le domaine de la vision par ordinateur, en offrant des performances supérieures aux méthodes classiques, basées sur des caractéristiques manuelles ou des modèles simples. Dans ce document, nous allons présenter les principes et les caractéristiques du deep learning pour la classification d'images et la détection d'objets, ainsi que les principaux modèles et techniques utilisés.\", metadata={'source': 'microsoft_learn\\\\Paul\\\\Les bases de données vectorielles.pdf'}),\n",
       " Document(page_content=\"Principes du deep learning Le deep learning est une méthode d'apprentissage automatique qui utilise des réseaux de neurones artificiels, qui sont des structures composées de couches de neurones interconnectés, qui reçoivent des données en entrée et produisent des sorties en fonction de paramètres appris. Un réseau de neurones peut être vu comme une fonction non linéaire complexe, qui approxime une relation entre les données d'entrée et les données de sortie. Le réseau de neurones apprend ses paramètres à partir d'un ensemble d'exemples, appelé jeu de données, en utilisant un algorithme d'optimisation, généralement basé sur la descente de gradient, qui minimise une fonction de coût, qui mesure l'erreur entre les sorties prédites et les sorties attendues. Le réseau de neurones est dit profond lorsque le nombre de couches est élevé, ce qui lui permet de capturer des niveaux de plus en plus abstraits et complexes de représentation des données. Le deep learning se distingue des autres méthodes d'apprentissage automatique par sa capacité à apprendre directement à partir des données brutes, sans avoir besoin de définir des caractéristiques spécifiques au domaine.\\n\\nClassification d'images La classification d'images est la tâche qui consiste à attribuer une étiquette à une image, parmi un ensemble de classes prédéfinies. Par exemple, on peut vouloir classer une image selon qu'elle contient un chat, un chien, ou un oiseau. La classification d'images peut être vue comme un problème de classification supervisée, où l'on dispose d'un jeu de données d'images annotées avec leurs classes respectives, et où l'on cherche à apprendre un> modèle qui prédit la classe d'une image inconnue. Le deep learning a permis de réaliser des progrès significatifs dans la classification d'images, en utilisant des réseaux de neurones convolutifs, qui sont des modèles spécialisés pour le traitement des données visuelles.\", metadata={'source': 'microsoft_learn\\\\Paul\\\\Les bases de données vectorielles.pdf'}),\n",
       " Document(page_content=\"Réseaux de neurones convolutifs Un réseau de neurones convolutif (CNN) est un type de réseau de neurones qui utilise des opérations de convolution, qui consistent à appliquer des filtres linéaires sur les données d'entrée, pour extraire des caractéristiques locales et invariantes. Un CNN est composé de plusieurs couches, qui peuvent être de différents types, comme des couches de convolution, des couches d'activation, des couches de regroupement, ou des couches entièrement connectées. Les couches de convolution appliquent des filtres sur les données d'entrée, pour produire des cartes d'activation, qui représentent la réponse du filtre à une région de l'entrée. Les filtres sont appris par le réseau, et sont généralement de petite taille, comme 3x3 ou 5x5 pixels. Les couches d'activation appliquent une fonction non linéaire aux cartes d'activation, pour introduire de la non-linéarité dans le modèle. Les fonctions d'activation les plus courantes sont la fonction sigmoïde, la fonction tangente hyperbolique, ou la fonction ReLU. Les couches de regroupement réduisent la dimensionnalité des cartes d'activation, en appliquant une opération d'agrégation, comme le maximum, la moyenne, ou la norme, sur des régions voisines. Les couches de regroupement permettent de réduire le nombre de paramètres, de limiter le surapprentissage, et d'augmenter l'invariance à la translation. Les couches entièrement connectées sont des couches classiques de réseau de neurones, qui relient tous les neurones d'une couche à ceux de la couche suivante. Les couches entièrement connectées permettent de combiner les caractéristiques extraites par les couches précédentes, et de réaliser la tâche finale, comme la classification. Un CNN typique pour la classification d'images est composé de plusieurs blocs de couches de convolution, d'activation, et de regroupement, suivis de quelques couches entièrement connectées. La dernière couche entièrement connectée produit un vecteur de scores, qui représente la probabilité d'appartenance à chaque classe. La fonction de coût utilisée est généralement l'entropie croisée, qui mesure la divergence entre la distribution des scores et la distribution des classes réelles. Le réseau apprend ses paramètres en utilisant la rétroprop> agation du gradient, qui consiste à calculer le gradient de la fonction de coût par rapport aux paramètres, en utilisant la règle de la chaîne, et à mettre à jour les paramètres en suivant la direction opposée au gradient.\\n\\nModèles et techniques Les CNN ont été popularisés par le modèle AlexNet, qui a remporté le défi ImageNet en 2012, en battant les méthodes classiques basées sur des caractéristiques manuelles. ImageNet est un jeu de données d'images de grande taille, qui contient plus de 14 millions d'images, réparties en plus de 20 000 classes. Le modèle AlexNet est composé de huit couches, dont cinq couches de convolution et trois couches entièrement connectées, et utilise la fonction ReLU comme fonction d'activation, ainsi que des techniques de régularisation, comme le dropout et la normalisation des lots. Le modèle AlexNet a permis de réduire l'erreur de classification sur ImageNet de plus de 10 points.\\n\\nDepuis, de nombreux modèles de CNN ont été proposés, en augmentant la profondeur, la largeur, ou la complexité du réseau, pour améliorer les performances sur la classification d'images. Parmi ces modèles, on peut citer VGG, qui utilise des filtres de taille 3x3 et des couches de convolution successives, Inception, qui utilise des blocs modulaires avec des branches parallèles de différentes tailles, ResNet, qui utilise des connexions résiduelles qui permettent de sauter des couches, DenseNet, qui utilise des connexions denses qui relient toutes les couches entre elles, ou encore EfficientNet, qui utilise une recherche automatique de l'architecture optimale du réseau. Ces modèles ont atteint des niveaux de performance proches ou supérieurs à ceux des humains sur la classification d'images.\", metadata={'source': 'microsoft_learn\\\\Paul\\\\Les bases de données vectorielles.pdf'}),\n",
       " Document(page_content=\"Outre les modèles, plusieurs techniques ont été développées pour améliorer le deep learning pour la classification d'images. Parmi ces techniques, on peut citer l'augmentation des données, qui consiste à appliquer des transformations aléatoires sur les images d'entraînement, comme le recadrage, le retournement, la rotation, le bruit, ou la variation des couleurs, pour augmenter la diversité et la robustesse du modèle. On peut aussi citer le transfert d'apprentissage, qui consiste à utiliser un modèle pré-entraîné sur un jeu de données de grande taille, comme ImageNet, et à l'adapter à un jeu de données de plus petite taille, en conservant les couches de convolution, qui captent des caractéristiques génériques, et en ré-entraînant les couches entièrement connectées, qui captent des caractéristiques spécifiques. On peut également> citer l'apprentissage par auto-étiquetage, qui consiste à utiliser un modèle pré-entraîné pour générer des étiquettes pour des images non annotées, et à les utiliser comme données d'entraînement supplémentaires.\\n\\nDétection d'objets La détection d'objets est la tâche qui consiste à identifier et à localiser les objets présents dans une image, en leur attribuant une classe et une boîte englobante. Par exemple, on peut vouloir détecter les personnes, les voitures, ou les animaux dans une image. La détection d'objets peut être vue comme un problème de régression supervisée, où l'on dispose d'un jeu de données d'images annotées avec les classes et les boîtes englobantes des objets, et où l'on cherche à apprendre un modèle qui prédit les classes et les boîtes englobantes des objets dans une image inconnue. Le deep learning a permis de réaliser des progrès significatifs dans la détection d'objets, en utilisant des modèles basés sur des CNN, qui peuvent être classés en deux catégories : les modèles à deux étapes, qui utilisent une région d'intérêt pour localiser les objets, et les modèles à une étape, qui détectent les objets directement à partir de l'image entière.\\n\\nModèles à deux étapes Les modèles à deux étapes sont des modèles qui utilisent une région d'intérêt (RoI), qui est une sous- région de l'image qui contient potentiellement un objet, pour localiser les objets. Les modèles à deux étapes sont composés de deux parties : un extracteur de caractéristiques, qui utilise un CNN pour extraire des caractéristiques de l'image entière, et un détecteur de région, qui utilise un autre CNN pour détecter les objets à l'intérieur de chaque RoI. Les modèles à deux étapes sont généralement plus précis que les modèles à une étape, mais aussi plus lents.\\n\\nLe modèle R-CNN est le premier modèle à deux étapes, qui utilise un algorithme de segmentation, comme le recherche sélective, pour générer environ 2000 RoI par image, et qui utilise un CNN pré- entraîné, comme VGG ou AlexNet, pour extraire des caractéristiques de chaque RoI. Le modèle R-CNN utilise ensuite un classifieur SVM, qui utilise les caractéristiques extraites comme entrée, pour prédire la classe de chaque RoI, et un régresseur linéaire, qui utilise également les caractéristiques extraites comme entrée, pour affiner les boîtes englobantes de chaque RoI. Le> modèle R-CNN a permis de réduire l'erreur de détection sur le jeu de données PASCAL VOC, qui est un jeu de données de référence pour la détection d'objets, qui contient environ 20 000 images, réparties en 20 classes. Cependant, le modèle R-CNN est très lent, car il nécessite d'exécuter le CNN sur chaque RoI individuellement, et utilise plusieurs composants distincts, qui doivent être entraînés séparément.\\n\\nLe modèle Fast R-CNN améliore le modèle R-CNN, en utilisant une couche de regroupement de région (RoIPool), qui permet de réduire le nombre d'exécutions du CNN, en extrayant les caractéristiques de\", metadata={'source': 'microsoft_learn\\\\Paul\\\\Les bases de données vectorielles.pdf'}),\n",
       " Document(page_content=\"Le modèle Fast R-CNN améliore le modèle R-CNN, en utilisant une couche de regroupement de région (RoIPool), qui permet de réduire le nombre d'exécutions du CNN, en extrayant les caractéristiques de\\n\\nl'image entière une seule fois, et en les partageant entre les différentes RoI. Le modèle Fast R-CNN utilise également un seul réseau, qui combine le classifieur et le régresseur, et qui peut être entraîné de bout en bout. Le modèle Fast R-CNN est plus rapide et plus précis que le modèle R-CNN, mais il dépend toujours d'un algorithme externe pour générer les RoI, ce qui limite sa performance et sa flexibilité.\\n\\nLe modèle Faster R-CNN résout le problème du modèle Fast R-CNN, en utilisant un réseau de proposition de région (RPN), qui est un autre CNN, qui prend les caractéristiques extraites par l'extracteur de caractéristiques comme entrée, et qui prédit les RoI à partir de plusieurs ancres, qui sont des boîtes englobantes de tailles et de ratios variés, situées à différentes positions de l'image. Le modèle Faster R- CNN utilise ensuite la même couche RoIPool et le même détecteur de région que le modèle Fast R-CNN, pour prédire la classe et la boîte englobante de chaque RoI. Le modèle Faster R-CNN est plus rapide et plus précis que le modèle Fast R-CNN, car il utilise un seul réseau, qui peut être entraîné de bout en bout, et qui génère les RoI de manière adaptative.\\n\\nModèles à une étape Les modèles à une étape sont des modèles qui détectent les objets directement à partir de l'image entière, sans utiliser de RoI. Les modèles à une étape sont composés d'un seul réseau, qui utilise un CNN pour extraire des caractéristiques de l'image, et qui prédit la classe et la boîte englobante de chaque objet à partir de plusieurs ancres, comme le modèle Faster R-CNN. Les modèles à une étape sont généralement plus rapides que les modèles à deux étapes, mais aussi moins précis.\\n\\nLe modèle YOLO est le premier modèle à une étape, qui divise l'image> en une grille de cellules, et qui prédit la classe et la boîte englobante de chaque objet à partir de plusieurs ancres par cellule. Le modèle YOLO utilise un seul réseau, qui combine l'extracteur de caractéristiques et le détecteur d'objets, et qui peut être entraîné de bout en bout. Le modèle YOLO est très rapide, car il traite l'image entière en une seule fois, mais il a tendance à manquer les petits objets, ou les objets proches les uns des autres, car il limite le nombre d'objets par cellule.\\n\\nLe modèle SSD améliore le modèle YOLO, en utilisant plusieurs cartes de caractéristiques à différentes échelles, pour détecter les objets de différentes tailles, et en utilisant plus d'ancres par cellule, pour détecter les objets de différents ratios. Le modèle SSD utilise également un seul réseau, qui combine l'extracteur de caractéristiques et le détecteur d'objets, et qui peut être entraîné de bout en bout. Le modèle SSD est plus précis que le modèle YOLO, car il capture mieux les objets de différentes tailles et formes, mais il est aussi plus lent.\\n\\nConclusion Le deep learning pour la classification d'images et la détection d'objets est un domaine en constante évolution, qui continue de proposer des modèles et des techniques innovants, pour relever les défis liés à la complexité et à la diversité des données visuelles.\", metadata={'source': 'microsoft_learn\\\\Paul\\\\Les bases de données vectorielles.pdf'}),\n",
       " Document(page_content=\"Les modèles de fondation de l’IA générative\\n\\nLes modèles de fondation de l’IA générative sont des modèles d’intelligence artificielle capables de générer du contenu à partir de données brutes, comme du texte, des images, des sons ou des vidéos. Ces modèles utilisent des techniques d’apprentissage profond pour apprendre les caractéristiques et les relations des données, et pour produire de nouveaux exemples qui respectent les mêmes règles. Les modèles de fondation de l’IA générative sont considérés comme une avancée majeure dans le domaine de l’IA, car ils permettent de créer des applications innovantes et créatives dans divers domaines.\\n\\nParmi les modèles de fondation de l’IA générative, les modèles de génération d’images sont ceux qui visent à créer des images réalistes et diversifiées à partir de données brutes ou conditionnées. Par exemple, un modèle de génération d’images peut prendre une description textuelle comme entrée et produire une image qui correspond au texte. Ou bien, il peut prendre une image existante et la modifier selon un style, une couleur ou un attribut donné. Les modèles de génération d’images peuvent être basés sur des réseaux de neurones convolutifs (CNN), qui sont capables de traiter et de reconnaître les caractéristiques visuelles des images, ou sur des réseaux antagonistes génératifs (GAN), qui sont des systèmes composés de deux réseaux concurrents : un générateur qui crée les images et un discriminateur qui les évalue. Les modèles de génération d’images offrent de nombreuses possibilités d’application, comme la synthèse de visages, le dessin assisté, la colorisation, la restauration ou la super- résolution d’images.\\n\\nLes LLM, ou masters of laws, sont des diplômes de droit de niveau supérieur qui permettent aux étudiants et aux professionnels d'approfondir leurs connaissances juridiques dans un domaine spécifique. Les LLM sont généralement proposés par des universités prestigieuses, et sont reconnus internationalement comme des qualifications académiques de haut niveau. Les LLM sont souvent destinés aux personnes qui souhaitent se spécialiser dans un domaine du droit, comme le droit international, le droit des affaires, le droit fiscal, le droit de la propriété intellectuelle ou le droit des droits de l'homme. Les LLM peuvent également servir à acquérir une expérience juridique dans un pays ou une région différente, ou à se préparer à un doctorat en droit. Les LLM offrent de nombreux avantages aux étudiants et aux professionnels, comme l'amélioration de leurs compétences juridiques, la mise en réseau avec des pairs et des experts, l'ouverture de nouvelles opportunités de carrière ou de recherche, ou l'augmentation de leur crédibilité et de leur réputation.\\n\\nLe service Azure OpenAI : un outil puissant pour l'intelligence artificielle\\n\\nUn document qui présente le service Azure OpenAI, ses caractéristiques, ses bénéfices et ses applications.\\n\\nQu'est-ce que le service Azure OpenAI ? Le service Azure OpenAI est un service cloud qui permet aux développeurs et aux chercheurs d'accéder à la plateforme OpenAI, une initiative qui vise à créer une intelligence artificielle générale (AGI) bénéfique pour l'humanité. Le service Azure OpenAI offre la possibilité d'utiliser des modèles d'IA pré-entraînés ou personnalisés, de les déployer sur le cloud ou en périphérie, et de les intégrer à des applications et des scénarios variés.\", metadata={'source': 'microsoft_learn\\\\Paul\\\\Les modèles de fondation de l IA générative.pdf'}),\n",
       " Document(page_content=\"Comment fonctionne le service Azure OpenAI ? Le service Azure OpenAI repose sur deux composants principaux : le portail Azure OpenAI et les API Azure OpenAI. Le portail Azure OpenAI est une interface web qui permet de créer, de gérer et de surveiller les ressources du service Azure OpenAI, telles que les abonnements, les clés, les modèles, les points de terminaison et les requêtes. Les API Azure OpenAI sont des interfaces de programmation qui permettent d'interagir avec les modèles d'IA du service Azure OpenAI, tels que GPT-3, Codex, DALL-E ou CLIP. Les API Azure OpenAI sont accessibles via des requêtes HTTP ou des SDK dans différents langages de programmation, tels que Python, Java, C#, Node.js ou Ruby.\\n\\nQuels sont les avantages du service Azure OpenAI ? Le service Azure OpenAI présente plusieurs avantages, parmi lesquels :\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIl permet d'accéder à des modèles d'IA de pointe, capables de réaliser des tâches complexes et variées, telles que la génération de texte, la compréhension du langage naturel, la vision par ordinateur, la synthèse vocale ou la création de contenu multimédia. Il offre une grande flexibilité, en permettant de choisir entre des modèles pré-entraînés ou personnalisés, et de les adapter aux besoins spécifiques de chaque projet ou domaine. Il garantit une haute performance, en s'appuyant sur l'infrastructure cloud de Microsoft Azure, qui assure une disponibilité, une scalabilité et une sécurité optimales. Il facilite l'intégration, en proposant des API simples et des SDK dans différents langages de programmation, qui permettent de connecter les modèles d'IA du service Azure OpenAI à des applications et des scénarios existants ou nouveaux.\\n\\nQuels sont les cas d'utilisation du service Azure OpenAI ? Le service Azure OpenAI peut être utilisé pour de nombreux cas d'utilisation, dans différents domaines et secteurs d'activité. Voici quelques exemples :\\n\\nDans le domaine de l'éducation, le service Azure OpenAI peut être utilisé pour créer des outils\\n\\npédagogiques, tels que des assistants de rédaction, des générateurs de questions, des correcteurs automatiques ou des tuteurs virtuels.\\n\\nDans le domaine de la santé, le service Azure OpenAI peut être utilisé pour créer des outils d'aide au diagnostic, à la prescription, à la recherche ou à la prévention, en exploitant les données médicales et les connaissances scientifiques.\\n\\nDans le domaine du divertissement, le service Azure OpenAI peut être utilisé pour créer des outils de génération de contenu, tels que des scénarios, des dialogues, des personnages, des musiques ou des images.\\n\\nDans le domaine du commerce, le service Azure OpenAI peut être utilisé pour créer des outils\\n\\nd'optimisation, tels que des recommandations personnalisées, des prévisions de ventes, des analyses de marché ou des chatbots.\\n\\nComment mettre en œuvre le service Azure OpenAI ? Pour mettre en œuvre le service Azure OpenAI, il faut suivre les étapes suivantes :\\n\\nCréer un compte Microsoft Azure, si ce n'est pas déjà fait, et se connecter au portail Azure OpenAI.\\n\\nSouscrire à un abonnement au service Azure OpenAI, en choisissant le niveau de tarification\\n\\nadapté à ses besoins.\\n\\nCréer une clé d'API, qui permettra d'authentifier les requêtes au service Azure OpenAI. • Choisir un modèle d'IA, parmi ceux proposés par le service Azure OpenAI, ou créer son propre modèle, en utilisant les outils de formation et de réglage fournis par le service Azure OpenAI.\\n\\nCréer un point de terminaison, qui permettra d'exposer le modèle d'IA sur le cloud ou en\\n\\npériphérie, et de le rendre accessible via une URL.\\n\\nEnvoyer des requêtes au point de terminaison, en utilisant les API Azure OpenAI ou les SDK dans le langage de programmation de son choix.\\n\\nAnalyser les résultats des requêtes, en utilisant les outils de suivi et de statistiques fournis par le\\n\\nportail Azure OpenAI.\\n\\nLes services cognitifs Azure\", metadata={'source': 'microsoft_learn\\\\Paul\\\\Les modèles de fondation de l IA générative.pdf'}),\n",
       " Document(page_content=\"Analyser les résultats des requêtes, en utilisant les outils de suivi et de statistiques fournis par le\\n\\nportail Azure OpenAI.\\n\\nLes services cognitifs Azure\\n\\nLes services cognitifs Azure sont un ensemble de services cloud qui permettent aux développeurs de créer des applications intelligentes, en utilisant les capacités d'intelligence artificielle (IA) et de machine learning (ML) fournies par Microsoft. Les services cognitifs Azure couvrent différents domaines, tels que la vision, le langage, la recherche, la décision et la synthèse vocale. Ils offrent des API, des SDK et des outils faciles à utiliser, pour intégrer les fonctionnalités d'IA et de ML dans les applications web, mobiles ou de bureau.\\n\\nLa vision Le domaine de la vision regroupe les services cognitifs Azure qui permettent aux applications de comprendre et d'analyser le contenu visuel, comme les images, les vidéos ou les flux en direct. Les services cognitifs Azure de la vision comprennent :\\n\\nLe service Vision par ordinateur, qui fournit des fonctions de détection, d'analyse, de description et de balisage des images, ainsi que de reconnaissance optique de caractères (OCR) et de génération de vignettes.\\n\\nLe service Reconnaissance faciale, qui permet d'identifier, de vérifier, de rechercher et de regrouper des visages dans des images ou des vidéos, ainsi que de détecter des attributs faciaux, comme l'âge, le genre, l'émotion ou les accessoires.\\n\\nLe service Analyseur vidéo, qui offre des capacités d'analyse avancée des vidéos, comme la détection et le suivi des visages, des objets, des logos, des scènes ou des activités, ainsi que la transcription, la traduction et la synthèse vocale des pistes audio.\\n\\nLe service Form Recognizer, qui permet d'extraire et de structurer les données clés contenues dans des formulaires papier ou numériques, comme des factures, des reçus, des chèques ou des cartes d'identité.\\n\\nLe service Custom Vision, qui permet de personnaliser les modèles de vision par ordinateur et de reconnaissance faciale, en les entraînant avec ses propres données, afin de répondre à des besoins spécifiques.\\n\\nLe langage Le domaine du langage regroupe les services cognitifs Azure qui permettent aux applications de comprendre et de manipuler le langage naturel, c'est-à-dire le langage parlé ou écrit par les humains. Les services cognitifs Azure du langage comprennent :\\n\\nLe service Analyse de texte, qui fournit des fonctions d'analyse s>yntaxique, sémantique et sentimentale des textes, ainsi que de détection des entités nommées, des mots-clés, des liens ou des langues.\\n\\nLe service Reconnaissance vocale, qui permet de transcrire le discours en texte, en reconnaissant les mots prononcés, le locuteur, l'intention ou les commandes.\\n\\nLe service Traduction vocale, qui permet de traduire le discours d'une langue à une autre, en conservant la voix et le ton du locuteur.\\n\\nLe service Génération de langage naturel, qui permet de produire du texte à partir de données structurées, comme des tableaux, des graphiques ou des images, en utilisant des modèles pré-entraînés ou personnalisés.\\n\\nLe service Linguistic Analysis, qui fournit des fonctions d'analyse morphologique, syntaxique et sémantique des textes, en utilisant des règles et des dictionnaires linguistiques.\\n\\nLe service QnA Maker, qui permet de créer des agents conversationnels capables de répondre aux questions des utilisateurs, en se basant sur des sources de connaissances existantes, comme des documents, des sites web ou des FAQ.\\n\\nLes bases de données vectorielles\", metadata={'source': 'microsoft_learn\\\\Paul\\\\Les modèles de fondation de l IA générative.pdf'}),\n",
       " Document(page_content=\"Les bases de données vectorielles\\n\\nLes bases de données vectorielles sont des systèmes de gestion de données qui stockent et manipulent des vecteurs, c'est-à-dire des séquences de nombres réels qui représentent des entités ou des objets. Ces vecteurs peuvent être obtenus à partir de diverses sources, comme des images, des textes, des sons, des vidéos, ou des signaux biométriques. Les bases de données vectorielles permettent d'effectuer des opérations efficaces sur les vecteurs, comme la recherche par similarité, le calcul de distances, le regroupement, la classification, ou la recommandation.\\n\\nLes bases de données vectorielles sont utilisées dans de nombreux domaines d'application, comme la vision par ordinateur, le traitement du langage naturel, la bioinformatique, ou le commerce électronique. Elles offrent plusieurs avantages par rapport aux bases de données traditionnelles, comme la réduction de la dimensionnalité, la généralisation, la robustesse au bruit, ou la facilité d'interprétation. Elles présentent aussi des défis spécifiques, comme le choix des méthodes de vectorisation, l'indexation des vecteurs de grande dimension, ou la garantie de la sécurité et de la confidentialité des données.\\n\\nDans cet article, nous allons présenter les principes et les caractéristiques des bases de données vectorielles, ainsi que les principales techniques et outils utilisés pour les mettre en œuvre. Nous allons également illustrer les applications et les bénéfices des bases de données vectorielles à travers quelques exemples concrets.\\n\\nPrincipes et caractéristiques des bases de données vectorielles Une base de données vectorielle est un système de gestion de données qui stocke et manipule des vecteurs, c'est-à-dire des séquences ordonnées de nombres réels qui représentent des entités ou des objets. Un vecteur peut avoir une dimension fixe ou variable, selon le type de données qu'il encode. Par exemple, un vecteur peut représenter les pixels d'une image, les mots d'un texte, les fréquences d'un son, ou les caractéristiques d'un produit.\\n\\nLes vecteurs sont généralement obtenus à partir de données brutes, comme des fichiers multimédias, des documents, ou des flux de données, en utilisant des méthodes de vectorisation, qui transforment les données en vecteurs selon certains critères. Il existe différentes méthodes de vectorisation, selon le domaine et le but de l'application. Par exemple, on peut utiliser des méthodes basées sur des descripteurs, qui extraient des caractéristiques pertinentes des données, comme les couleurs, les formes, ou les textures pour les images, ou les mots, les phrases, ou les concepts> pour les textes. On peut aussi utiliser des méthodes basées sur des modèles, qui apprennent des représentations vectorielles à partir de données massives, en utilisant des techniques d'apprentissage automatique, comme les réseaux de neurones, les modèles probabilistes, ou les modèles géométriques.\\n\\nLe deep learning pour la classification d'images et la détection d'objets\\n\\nIntroduction La classification d'images et la détection d'objets sont deux tâches importantes de la vision par ordinateur, qui consistent à identifier et à localiser les objets présents dans une image. Ces tâches ont de nombreuses applications pratiques, comme la reconnaissance faciale, la surveillance, la robotique, ou la médecine. Le deep learning, qui est une branche de l'apprentissage automatique basée sur des modèles profonds, a révolutionné le domaine de la vision par ordinateur, en offrant des performances supérieures aux méthodes classiques, basées sur des caractéristiques manuelles ou des modèles simples. Dans ce document, nous allons présenter les principes et les caractéristiques du deep learning pour la classification d'images et la détection d'objets, ainsi que les principaux modèles et techniques utilisés.\", metadata={'source': 'microsoft_learn\\\\Paul\\\\Les modèles de fondation de l IA générative.pdf'}),\n",
       " Document(page_content=\"Principes du deep learning Le deep learning est une méthode d'apprentissage automatique qui utilise des réseaux de neurones artificiels, qui sont des structures composées de couches de neurones interconnectés, qui reçoivent des données en entrée et produisent des sorties en fonction de paramètres appris. Un réseau de neurones peut être vu comme une fonction non linéaire complexe, qui approxime une relation entre les données d'entrée et les données de sortie. Le réseau de neurones apprend ses paramètres à partir d'un ensemble d'exemples, appelé jeu de données, en utilisant un algorithme d'optimisation, généralement basé sur la descente de gradient, qui minimise une fonction de coût, qui mesure l'erreur entre les sorties prédites et les sorties attendues. Le réseau de neurones est dit profond lorsque le nombre de couches est élevé, ce qui lui permet de capturer des niveaux de plus en plus abstraits et complexes de représentation des données. Le deep learning se distingue des autres méthodes d'apprentissage automatique par sa capacité à apprendre directement à partir des données brutes, sans avoir besoin de définir des caractéristiques spécifiques au domaine.\\n\\nClassification d'images La classification d'images est la tâche qui consiste à attribuer une étiquette à une image, parmi un ensemble de classes prédéfinies. Par exemple, on peut vouloir classer une image selon qu'elle contient un chat, un chien, ou un oiseau. La classification d'images peut être vue comme un problème de classification supervisée, où l'on dispose d'un jeu de données d'images annotées avec leurs classes respectives, et où l'on cherche à apprendre un> modèle qui prédit la classe d'une image inconnue. Le deep learning a permis de réaliser des progrès significatifs dans la classification d'images, en utilisant des réseaux de neurones convolutifs, qui sont des modèles spécialisés pour le traitement des données visuelles.\", metadata={'source': 'microsoft_learn\\\\Paul\\\\Les modèles de fondation de l IA générative.pdf'}),\n",
       " Document(page_content=\"Réseaux de neurones convolutifs Un réseau de neurones convolutif (CNN) est un type de réseau de neurones qui utilise des opérations de convolution, qui consistent à appliquer des filtres linéaires sur les données d'entrée, pour extraire des caractéristiques locales et invariantes. Un CNN est composé de plusieurs couches, qui peuvent être de différents types, comme des couches de convolution, des couches d'activation, des couches de regroupement, ou des couches entièrement connectées. Les couches de convolution appliquent des filtres sur les données d'entrée, pour produire des cartes d'activation, qui représentent la réponse du filtre à une région de l'entrée. Les filtres sont appris par le réseau, et sont généralement de petite taille, comme 3x3 ou 5x5 pixels. Les couches d'activation appliquent une fonction non linéaire aux cartes d'activation, pour introduire de la non-linéarité dans le modèle. Les fonctions d'activation les plus courantes sont la fonction sigmoïde, la fonction tangente hyperbolique, ou la fonction ReLU. Les couches de regroupement réduisent la dimensionnalité des cartes d'activation, en appliquant une opération d'agrégation, comme le maximum, la moyenne, ou la norme, sur des régions voisines. Les couches de regroupement permettent de réduire le nombre de paramètres, de limiter le surapprentissage, et d'augmenter l'invariance à la translation. Les couches entièrement connectées sont des couches classiques de réseau de neurones, qui relient tous les neurones d'une couche à ceux de la couche suivante. Les couches entièrement connectées permettent de combiner les caractéristiques extraites par les couches précédentes, et de réaliser la tâche finale, comme la classification. Un CNN typique pour la classification d'images est composé de plusieurs blocs de couches de convolution, d'activation, et de regroupement, suivis de quelques couches entièrement connectées. La dernière couche entièrement connectée produit un vecteur de scores, qui représente la probabilité d'appartenance à chaque classe. La fonction de coût utilisée est généralement l'entropie croisée, qui mesure la divergence entre la distribution des scores et la distribution des classes réelles. Le réseau apprend ses paramètres en utilisant la rétroprop> agation du gradient, qui consiste à calculer le gradient de la fonction de coût par rapport aux paramètres, en utilisant la règle de la chaîne, et à mettre à jour les paramètres en suivant la direction opposée au gradient.\\n\\nModèles et techniques Les CNN ont été popularisés par le modèle AlexNet, qui a remporté le défi ImageNet en 2012, en battant les méthodes classiques basées sur des caractéristiques manuelles. ImageNet est un jeu de données d'images de grande taille, qui contient plus de 14 millions d'images, réparties en plus de 20 000 classes. Le modèle AlexNet est composé de huit couches, dont cinq couches de convolution et trois couches entièrement connectées, et utilise la fonction ReLU comme fonction d'activation, ainsi que des techniques de régularisation, comme le dropout et la normalisation des lots. Le modèle AlexNet a permis de réduire l'erreur de classification sur ImageNet de plus de 10 points.\\n\\nDepuis, de nombreux modèles de CNN ont été proposés, en augmentant la profondeur, la largeur, ou la complexité du réseau, pour améliorer les performances sur la classification d'images. Parmi ces modèles, on peut citer VGG, qui utilise des filtres de taille 3x3 et des couches de convolution successives, Inception, qui utilise des blocs modulaires avec des branches parallèles de différentes tailles, ResNet, qui utilise des connexions résiduelles qui permettent de sauter des couches, DenseNet, qui utilise des connexions denses qui relient toutes les couches entre elles, ou encore EfficientNet, qui utilise une recherche automatique de l'architecture optimale du réseau. Ces modèles ont atteint des niveaux de performance proches ou supérieurs à ceux des humains sur la classification d'images.\", metadata={'source': 'microsoft_learn\\\\Paul\\\\Les modèles de fondation de l IA générative.pdf'}),\n",
       " Document(page_content=\"Outre les modèles, plusieurs techniques ont été développées pour améliorer le deep learning pour la classification d'images. Parmi ces techniques, on peut citer l'augmentation des données, qui consiste à appliquer des transformations aléatoires sur les images d'entraînement, comme le recadrage, le retournement, la rotation, le bruit, ou la variation des couleurs, pour augmenter la diversité et la robustesse du modèle. On peut aussi citer le transfert d'apprentissage, qui consiste à utiliser un modèle pré-entraîné sur un jeu de données de grande taille, comme ImageNet, et à l'adapter à un jeu de données de plus petite taille, en conservant les couches de convolution, qui captent des caractéristiques génériques, et en ré-entraînant les couches entièrement connectées, qui captent des caractéristiques spécifiques. On peut également> citer l'apprentissage par auto-étiquetage, qui consiste à utiliser un modèle pré-entraîné pour générer des étiquettes pour des images non annotées, et à les utiliser comme données d'entraînement supplémentaires.\\n\\nDétection d'objets La détection d'objets est la tâche qui consiste à identifier et à localiser les objets présents dans une image, en leur attribuant une classe et une boîte englobante. Par exemple, on peut vouloir détecter les personnes, les voitures, ou les animaux dans une image. La détection d'objets peut être vue comme un problème de régression supervisée, où l'on dispose d'un jeu de données d'images annotées avec les classes et les boîtes englobantes des objets, et où l'on cherche à apprendre un modèle qui prédit les classes et les boîtes englobantes des objets dans une image inconnue. Le deep learning a permis de réaliser des progrès significatifs dans la détection d'objets, en utilisant des modèles basés sur des CNN, qui peuvent être classés en deux catégories : les modèles à deux étapes, qui utilisent une région d'intérêt pour localiser les objets, et les modèles à une étape, qui détectent les objets directement à partir de l'image entière.\\n\\nModèles à deux étapes Les modèles à deux étapes sont des modèles qui utilisent une région d'intérêt (RoI), qui est une sous- région de l'image qui contient potentiellement un objet, pour localiser les objets. Les modèles à deux étapes sont composés de deux parties : un extracteur de caractéristiques, qui utilise un CNN pour extraire des caractéristiques de l'image entière, et un détecteur de région, qui utilise un autre CNN pour détecter les objets à l'intérieur de chaque RoI. Les modèles à deux étapes sont généralement plus précis que les modèles à une étape, mais aussi plus lents.\\n\\nLe modèle R-CNN est le premier modèle à deux étapes, qui utilise un algorithme de segmentation, comme le recherche sélective, pour générer environ 2000 RoI par image, et qui utilise un CNN pré- entraîné, comme VGG ou AlexNet, pour extraire des caractéristiques de chaque RoI. Le modèle R-CNN utilise ensuite un classifieur SVM, qui utilise les caractéristiques extraites comme entrée, pour prédire la classe de chaque RoI, et un régresseur linéaire, qui utilise également les caractéristiques extraites comme entrée, pour affiner les boîtes englobantes de chaque RoI. Le> modèle R-CNN a permis de réduire l'erreur de détection sur le jeu de données PASCAL VOC, qui est un jeu de données de référence pour la détection d'objets, qui contient environ 20 000 images, réparties en 20 classes. Cependant, le modèle R-CNN est très lent, car il nécessite d'exécuter le CNN sur chaque RoI individuellement, et utilise plusieurs composants distincts, qui doivent être entraînés séparément.\\n\\nLe modèle Fast R-CNN améliore le modèle R-CNN, en utilisant une couche de regroupement de région (RoIPool), qui permet de réduire le nombre d'exécutions du CNN, en extrayant les caractéristiques de\", metadata={'source': 'microsoft_learn\\\\Paul\\\\Les modèles de fondation de l IA générative.pdf'}),\n",
       " Document(page_content=\"Le modèle Fast R-CNN améliore le modèle R-CNN, en utilisant une couche de regroupement de région (RoIPool), qui permet de réduire le nombre d'exécutions du CNN, en extrayant les caractéristiques de\\n\\nl'image entière une seule fois, et en les partageant entre les différentes RoI. Le modèle Fast R-CNN utilise également un seul réseau, qui combine le classifieur et le régresseur, et qui peut être entraîné de bout en bout. Le modèle Fast R-CNN est plus rapide et plus précis que le modèle R-CNN, mais il dépend toujours d'un algorithme externe pour générer les RoI, ce qui limite sa performance et sa flexibilité.\\n\\nLe modèle Faster R-CNN résout le problème du modèle Fast R-CNN, en utilisant un réseau de proposition de région (RPN), qui est un autre CNN, qui prend les caractéristiques extraites par l'extracteur de caractéristiques comme entrée, et qui prédit les RoI à partir de plusieurs ancres, qui sont des boîtes englobantes de tailles et de ratios variés, situées à différentes positions de l'image. Le modèle Faster R- CNN utilise ensuite la même couche RoIPool et le même détecteur de région que le modèle Fast R-CNN, pour prédire la classe et la boîte englobante de chaque RoI. Le modèle Faster R-CNN est plus rapide et plus précis que le modèle Fast R-CNN, car il utilise un seul réseau, qui peut être entraîné de bout en bout, et qui génère les RoI de manière adaptative.\\n\\nModèles à une étape Les modèles à une étape sont des modèles qui détectent les objets directement à partir de l'image entière, sans utiliser de RoI. Les modèles à une étape sont composés d'un seul réseau, qui utilise un CNN pour extraire des caractéristiques de l'image, et qui prédit la classe et la boîte englobante de chaque objet à partir de plusieurs ancres, comme le modèle Faster R-CNN. Les modèles à une étape sont généralement plus rapides que les modèles à deux étapes, mais aussi moins précis.\\n\\nLe modèle YOLO est le premier modèle à une étape, qui divise l'image> en une grille de cellules, et qui prédit la classe et la boîte englobante de chaque objet à partir de plusieurs ancres par cellule. Le modèle YOLO utilise un seul réseau, qui combine l'extracteur de caractéristiques et le détecteur d'objets, et qui peut être entraîné de bout en bout. Le modèle YOLO est très rapide, car il traite l'image entière en une seule fois, mais il a tendance à manquer les petits objets, ou les objets proches les uns des autres, car il limite le nombre d'objets par cellule.\\n\\nLe modèle SSD améliore le modèle YOLO, en utilisant plusieurs cartes de caractéristiques à différentes échelles, pour détecter les objets de différentes tailles, et en utilisant plus d'ancres par cellule, pour détecter les objets de différents ratios. Le modèle SSD utilise également un seul réseau, qui combine l'extracteur de caractéristiques et le détecteur d'objets, et qui peut être entraîné de bout en bout. Le modèle SSD est plus précis que le modèle YOLO, car il capture mieux les objets de différentes tailles et formes, mais il est aussi plus lent.\\n\\nConclusion Le deep learning pour la classification d'images et la détection d'objets est un domaine en constante évolution, qui continue de proposer des modèles et des techniques innovants, pour relever les défis liés à la complexité et à la diversité des données visuelles.\", metadata={'source': 'microsoft_learn\\\\Paul\\\\Les modèles de fondation de l IA générative.pdf'}),\n",
       " Document(page_content=\"Les modèles de fondation de l’IA générative\\n\\nLes modèles de fondation de l’IA générative sont des modèles d’intelligence artificielle capables de générer du contenu à partir de données brutes, comme du texte, des images, des sons ou des vidéos. Ces modèles utilisent des techniques d’apprentissage profond pour apprendre les caractéristiques et les relations des données, et pour produire de nouveaux exemples qui respectent les mêmes règles. Les modèles de fondation de l’IA générative sont considérés comme une avancée majeure dans le domaine de l’IA, car ils permettent de créer des applications innovantes et créatives dans divers domaines.\\n\\nParmi les modèles de fondation de l’IA générative, les modèles de génération d’images sont ceux qui visent à créer des images réalistes et diversifiées à partir de données brutes ou conditionnées. Par exemple, un modèle de génération d’images peut prendre une description textuelle comme entrée et produire une image qui correspond au texte. Ou bien, il peut prendre une image existante et la modifier selon un style, une couleur ou un attribut donné. Les modèles de génération d’images peuvent être basés sur des réseaux de neurones convolutifs (CNN), qui sont capables de traiter et de reconnaître les caractéristiques visuelles des images, ou sur des réseaux antagonistes génératifs (GAN), qui sont des systèmes composés de deux réseaux concurrents : un générateur qui crée les images et un discriminateur qui les évalue. Les modèles de génération d’images offrent de nombreuses possibilités d’application, comme la synthèse de visages, le dessin assisté, la colorisation, la restauration ou la super- résolution d’images.\\n\\nLes LLM, ou masters of laws, sont des diplômes de droit de niveau supérieur qui permettent aux étudiants et aux professionnels d'approfondir leurs connaissances juridiques dans un domaine spécifique. Les LLM sont généralement proposés par des universités prestigieuses, et sont reconnus internationalement comme des qualifications académiques de haut niveau. Les LLM sont souvent destinés aux personnes qui souhaitent se spécialiser dans un domaine du droit, comme le droit international, le droit des affaires, le droit fiscal, le droit de la propriété intellectuelle ou le droit des droits de l'homme. Les LLM peuvent également servir à acquérir une expérience juridique dans un pays ou une région différente, ou à se préparer à un doctorat en droit. Les LLM offrent de nombreux avantages aux étudiants et aux professionnels, comme l'amélioration de leurs compétences juridiques, la mise en réseau avec des pairs et des experts, l'ouverture de nouvelles opportunités de carrière ou de recherche, ou l'augmentation de leur crédibilité et de leur réputation.\\n\\nLe service Azure OpenAI : un outil puissant pour l'intelligence artificielle\\n\\nUn document qui présente le service Azure OpenAI, ses caractéristiques, ses bénéfices et ses applications.\\n\\nQu'est-ce que le service Azure OpenAI ? Le service Azure OpenAI est un service cloud qui permet aux développeurs et aux chercheurs d'accéder à la plateforme OpenAI, une initiative qui vise à créer une intelligence artificielle générale (AGI) bénéfique pour l'humanité. Le service Azure OpenAI offre la possibilité d'utiliser des modèles d'IA pré-entraînés ou personnalisés, de les déployer sur le cloud ou en périphérie, et de les intégrer à des applications et des scénarios variés.\", metadata={'source': 'microsoft_learn\\\\Paul\\\\Les services cognitifs Azure.pdf'}),\n",
       " Document(page_content=\"Comment fonctionne le service Azure OpenAI ? Le service Azure OpenAI repose sur deux composants principaux : le portail Azure OpenAI et les API Azure OpenAI. Le portail Azure OpenAI est une interface web qui permet de créer, de gérer et de surveiller les ressources du service Azure OpenAI, telles que les abonnements, les clés, les modèles, les points de terminaison et les requêtes. Les API Azure OpenAI sont des interfaces de programmation qui permettent d'interagir avec les modèles d'IA du service Azure OpenAI, tels que GPT-3, Codex, DALL-E ou CLIP. Les API Azure OpenAI sont accessibles via des requêtes HTTP ou des SDK dans différents langages de programmation, tels que Python, Java, C#, Node.js ou Ruby.\\n\\nQuels sont les avantages du service Azure OpenAI ? Le service Azure OpenAI présente plusieurs avantages, parmi lesquels :\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIl permet d'accéder à des modèles d'IA de pointe, capables de réaliser des tâches complexes et variées, telles que la génération de texte, la compréhension du langage naturel, la vision par ordinateur, la synthèse vocale ou la création de contenu multimédia. Il offre une grande flexibilité, en permettant de choisir entre des modèles pré-entraînés ou personnalisés, et de les adapter aux besoins spécifiques de chaque projet ou domaine. Il garantit une haute performance, en s'appuyant sur l'infrastructure cloud de Microsoft Azure, qui assure une disponibilité, une scalabilité et une sécurité optimales. Il facilite l'intégration, en proposant des API simples et des SDK dans différents langages de programmation, qui permettent de connecter les modèles d'IA du service Azure OpenAI à des applications et des scénarios existants ou nouveaux.\\n\\nQuels sont les cas d'utilisation du service Azure OpenAI ? Le service Azure OpenAI peut être utilisé pour de nombreux cas d'utilisation, dans différents domaines et secteurs d'activité. Voici quelques exemples :\\n\\nDans le domaine de l'éducation, le service Azure OpenAI peut être utilisé pour créer des outils\\n\\npédagogiques, tels que des assistants de rédaction, des générateurs de questions, des correcteurs automatiques ou des tuteurs virtuels.\\n\\nDans le domaine de la santé, le service Azure OpenAI peut être utilisé pour créer des outils d'aide au diagnostic, à la prescription, à la recherche ou à la prévention, en exploitant les données médicales et les connaissances scientifiques.\\n\\nDans le domaine du divertissement, le service Azure OpenAI peut être utilisé pour créer des outils de génération de contenu, tels que des scénarios, des dialogues, des personnages, des musiques ou des images.\\n\\nDans le domaine du commerce, le service Azure OpenAI peut être utilisé pour créer des outils\\n\\nd'optimisation, tels que des recommandations personnalisées, des prévisions de ventes, des analyses de marché ou des chatbots.\\n\\nComment mettre en œuvre le service Azure OpenAI ? Pour mettre en œuvre le service Azure OpenAI, il faut suivre les étapes suivantes :\\n\\nCréer un compte Microsoft Azure, si ce n'est pas déjà fait, et se connecter au portail Azure OpenAI.\\n\\nSouscrire à un abonnement au service Azure OpenAI, en choisissant le niveau de tarification\\n\\nadapté à ses besoins.\\n\\nCréer une clé d'API, qui permettra d'authentifier les requêtes au service Azure OpenAI. • Choisir un modèle d'IA, parmi ceux proposés par le service Azure OpenAI, ou créer son propre modèle, en utilisant les outils de formation et de réglage fournis par le service Azure OpenAI.\\n\\nCréer un point de terminaison, qui permettra d'exposer le modèle d'IA sur le cloud ou en\\n\\npériphérie, et de le rendre accessible via une URL.\\n\\nEnvoyer des requêtes au point de terminaison, en utilisant les API Azure OpenAI ou les SDK dans le langage de programmation de son choix.\\n\\nAnalyser les résultats des requêtes, en utilisant les outils de suivi et de statistiques fournis par le\\n\\nportail Azure OpenAI.\\n\\nLes services cognitifs Azure\", metadata={'source': 'microsoft_learn\\\\Paul\\\\Les services cognitifs Azure.pdf'}),\n",
       " Document(page_content=\"Analyser les résultats des requêtes, en utilisant les outils de suivi et de statistiques fournis par le\\n\\nportail Azure OpenAI.\\n\\nLes services cognitifs Azure\\n\\nLes services cognitifs Azure sont un ensemble de services cloud qui permettent aux développeurs de créer des applications intelligentes, en utilisant les capacités d'intelligence artificielle (IA) et de machine learning (ML) fournies par Microsoft. Les services cognitifs Azure couvrent différents domaines, tels que la vision, le langage, la recherche, la décision et la synthèse vocale. Ils offrent des API, des SDK et des outils faciles à utiliser, pour intégrer les fonctionnalités d'IA et de ML dans les applications web, mobiles ou de bureau.\\n\\nLa vision Le domaine de la vision regroupe les services cognitifs Azure qui permettent aux applications de comprendre et d'analyser le contenu visuel, comme les images, les vidéos ou les flux en direct. Les services cognitifs Azure de la vision comprennent :\\n\\nLe service Vision par ordinateur, qui fournit des fonctions de détection, d'analyse, de description et de balisage des images, ainsi que de reconnaissance optique de caractères (OCR) et de génération de vignettes.\\n\\nLe service Reconnaissance faciale, qui permet d'identifier, de vérifier, de rechercher et de regrouper des visages dans des images ou des vidéos, ainsi que de détecter des attributs faciaux, comme l'âge, le genre, l'émotion ou les accessoires.\\n\\nLe service Analyseur vidéo, qui offre des capacités d'analyse avancée des vidéos, comme la détection et le suivi des visages, des objets, des logos, des scènes ou des activités, ainsi que la transcription, la traduction et la synthèse vocale des pistes audio.\\n\\nLe service Form Recognizer, qui permet d'extraire et de structurer les données clés contenues dans des formulaires papier ou numériques, comme des factures, des reçus, des chèques ou des cartes d'identité.\\n\\nLe service Custom Vision, qui permet de personnaliser les modèles de vision par ordinateur et de reconnaissance faciale, en les entraînant avec ses propres données, afin de répondre à des besoins spécifiques.\\n\\nLe langage Le domaine du langage regroupe les services cognitifs Azure qui permettent aux applications de comprendre et de manipuler le langage naturel, c'est-à-dire le langage parlé ou écrit par les humains. Les services cognitifs Azure du langage comprennent :\\n\\nLe service Analyse de texte, qui fournit des fonctions d'analyse s>yntaxique, sémantique et sentimentale des textes, ainsi que de détection des entités nommées, des mots-clés, des liens ou des langues.\\n\\nLe service Reconnaissance vocale, qui permet de transcrire le discours en texte, en reconnaissant les mots prononcés, le locuteur, l'intention ou les commandes.\\n\\nLe service Traduction vocale, qui permet de traduire le discours d'une langue à une autre, en conservant la voix et le ton du locuteur.\\n\\nLe service Génération de langage naturel, qui permet de produire du texte à partir de données structurées, comme des tableaux, des graphiques ou des images, en utilisant des modèles pré-entraînés ou personnalisés.\\n\\nLe service Linguistic Analysis, qui fournit des fonctions d'analyse morphologique, syntaxique et sémantique des textes, en utilisant des règles et des dictionnaires linguistiques.\\n\\nLe service QnA Maker, qui permet de créer des agents conversationnels capables de répondre aux questions des utilisateurs, en se basant sur des sources de connaissances existantes, comme des documents, des sites web ou des FAQ.\\n\\nLes bases de données vectorielles\", metadata={'source': 'microsoft_learn\\\\Paul\\\\Les services cognitifs Azure.pdf'}),\n",
       " Document(page_content=\"Les bases de données vectorielles\\n\\nLes bases de données vectorielles sont des systèmes de gestion de données qui stockent et manipulent des vecteurs, c'est-à-dire des séquences de nombres réels qui représentent des entités ou des objets. Ces vecteurs peuvent être obtenus à partir de diverses sources, comme des images, des textes, des sons, des vidéos, ou des signaux biométriques. Les bases de données vectorielles permettent d'effectuer des opérations efficaces sur les vecteurs, comme la recherche par similarité, le calcul de distances, le regroupement, la classification, ou la recommandation.\\n\\nLes bases de données vectorielles sont utilisées dans de nombreux domaines d'application, comme la vision par ordinateur, le traitement du langage naturel, la bioinformatique, ou le commerce électronique. Elles offrent plusieurs avantages par rapport aux bases de données traditionnelles, comme la réduction de la dimensionnalité, la généralisation, la robustesse au bruit, ou la facilité d'interprétation. Elles présentent aussi des défis spécifiques, comme le choix des méthodes de vectorisation, l'indexation des vecteurs de grande dimension, ou la garantie de la sécurité et de la confidentialité des données.\\n\\nDans cet article, nous allons présenter les principes et les caractéristiques des bases de données vectorielles, ainsi que les principales techniques et outils utilisés pour les mettre en œuvre. Nous allons également illustrer les applications et les bénéfices des bases de données vectorielles à travers quelques exemples concrets.\\n\\nPrincipes et caractéristiques des bases de données vectorielles Une base de données vectorielle est un système de gestion de données qui stocke et manipule des vecteurs, c'est-à-dire des séquences ordonnées de nombres réels qui représentent des entités ou des objets. Un vecteur peut avoir une dimension fixe ou variable, selon le type de données qu'il encode. Par exemple, un vecteur peut représenter les pixels d'une image, les mots d'un texte, les fréquences d'un son, ou les caractéristiques d'un produit.\\n\\nLes vecteurs sont généralement obtenus à partir de données brutes, comme des fichiers multimédias, des documents, ou des flux de données, en utilisant des méthodes de vectorisation, qui transforment les données en vecteurs selon certains critères. Il existe différentes méthodes de vectorisation, selon le domaine et le but de l'application. Par exemple, on peut utiliser des méthodes basées sur des descripteurs, qui extraient des caractéristiques pertinentes des données, comme les couleurs, les formes, ou les textures pour les images, ou les mots, les phrases, ou les concepts> pour les textes. On peut aussi utiliser des méthodes basées sur des modèles, qui apprennent des représentations vectorielles à partir de données massives, en utilisant des techniques d'apprentissage automatique, comme les réseaux de neurones, les modèles probabilistes, ou les modèles géométriques.\\n\\nLe deep learning pour la classification d'images et la détection d'objets\\n\\nIntroduction La classification d'images et la détection d'objets sont deux tâches importantes de la vision par ordinateur, qui consistent à identifier et à localiser les objets présents dans une image. Ces tâches ont de nombreuses applications pratiques, comme la reconnaissance faciale, la surveillance, la robotique, ou la médecine. Le deep learning, qui est une branche de l'apprentissage automatique basée sur des modèles profonds, a révolutionné le domaine de la vision par ordinateur, en offrant des performances supérieures aux méthodes classiques, basées sur des caractéristiques manuelles ou des modèles simples. Dans ce document, nous allons présenter les principes et les caractéristiques du deep learning pour la classification d'images et la détection d'objets, ainsi que les principaux modèles et techniques utilisés.\", metadata={'source': 'microsoft_learn\\\\Paul\\\\Les services cognitifs Azure.pdf'}),\n",
       " Document(page_content=\"Principes du deep learning Le deep learning est une méthode d'apprentissage automatique qui utilise des réseaux de neurones artificiels, qui sont des structures composées de couches de neurones interconnectés, qui reçoivent des données en entrée et produisent des sorties en fonction de paramètres appris. Un réseau de neurones peut être vu comme une fonction non linéaire complexe, qui approxime une relation entre les données d'entrée et les données de sortie. Le réseau de neurones apprend ses paramètres à partir d'un ensemble d'exemples, appelé jeu de données, en utilisant un algorithme d'optimisation, généralement basé sur la descente de gradient, qui minimise une fonction de coût, qui mesure l'erreur entre les sorties prédites et les sorties attendues. Le réseau de neurones est dit profond lorsque le nombre de couches est élevé, ce qui lui permet de capturer des niveaux de plus en plus abstraits et complexes de représentation des données. Le deep learning se distingue des autres méthodes d'apprentissage automatique par sa capacité à apprendre directement à partir des données brutes, sans avoir besoin de définir des caractéristiques spécifiques au domaine.\\n\\nClassification d'images La classification d'images est la tâche qui consiste à attribuer une étiquette à une image, parmi un ensemble de classes prédéfinies. Par exemple, on peut vouloir classer une image selon qu'elle contient un chat, un chien, ou un oiseau. La classification d'images peut être vue comme un problème de classification supervisée, où l'on dispose d'un jeu de données d'images annotées avec leurs classes respectives, et où l'on cherche à apprendre un> modèle qui prédit la classe d'une image inconnue. Le deep learning a permis de réaliser des progrès significatifs dans la classification d'images, en utilisant des réseaux de neurones convolutifs, qui sont des modèles spécialisés pour le traitement des données visuelles.\", metadata={'source': 'microsoft_learn\\\\Paul\\\\Les services cognitifs Azure.pdf'}),\n",
       " Document(page_content=\"Réseaux de neurones convolutifs Un réseau de neurones convolutif (CNN) est un type de réseau de neurones qui utilise des opérations de convolution, qui consistent à appliquer des filtres linéaires sur les données d'entrée, pour extraire des caractéristiques locales et invariantes. Un CNN est composé de plusieurs couches, qui peuvent être de différents types, comme des couches de convolution, des couches d'activation, des couches de regroupement, ou des couches entièrement connectées. Les couches de convolution appliquent des filtres sur les données d'entrée, pour produire des cartes d'activation, qui représentent la réponse du filtre à une région de l'entrée. Les filtres sont appris par le réseau, et sont généralement de petite taille, comme 3x3 ou 5x5 pixels. Les couches d'activation appliquent une fonction non linéaire aux cartes d'activation, pour introduire de la non-linéarité dans le modèle. Les fonctions d'activation les plus courantes sont la fonction sigmoïde, la fonction tangente hyperbolique, ou la fonction ReLU. Les couches de regroupement réduisent la dimensionnalité des cartes d'activation, en appliquant une opération d'agrégation, comme le maximum, la moyenne, ou la norme, sur des régions voisines. Les couches de regroupement permettent de réduire le nombre de paramètres, de limiter le surapprentissage, et d'augmenter l'invariance à la translation. Les couches entièrement connectées sont des couches classiques de réseau de neurones, qui relient tous les neurones d'une couche à ceux de la couche suivante. Les couches entièrement connectées permettent de combiner les caractéristiques extraites par les couches précédentes, et de réaliser la tâche finale, comme la classification. Un CNN typique pour la classification d'images est composé de plusieurs blocs de couches de convolution, d'activation, et de regroupement, suivis de quelques couches entièrement connectées. La dernière couche entièrement connectée produit un vecteur de scores, qui représente la probabilité d'appartenance à chaque classe. La fonction de coût utilisée est généralement l'entropie croisée, qui mesure la divergence entre la distribution des scores et la distribution des classes réelles. Le réseau apprend ses paramètres en utilisant la rétroprop> agation du gradient, qui consiste à calculer le gradient de la fonction de coût par rapport aux paramètres, en utilisant la règle de la chaîne, et à mettre à jour les paramètres en suivant la direction opposée au gradient.\\n\\nModèles et techniques Les CNN ont été popularisés par le modèle AlexNet, qui a remporté le défi ImageNet en 2012, en battant les méthodes classiques basées sur des caractéristiques manuelles. ImageNet est un jeu de données d'images de grande taille, qui contient plus de 14 millions d'images, réparties en plus de 20 000 classes. Le modèle AlexNet est composé de huit couches, dont cinq couches de convolution et trois couches entièrement connectées, et utilise la fonction ReLU comme fonction d'activation, ainsi que des techniques de régularisation, comme le dropout et la normalisation des lots. Le modèle AlexNet a permis de réduire l'erreur de classification sur ImageNet de plus de 10 points.\\n\\nDepuis, de nombreux modèles de CNN ont été proposés, en augmentant la profondeur, la largeur, ou la complexité du réseau, pour améliorer les performances sur la classification d'images. Parmi ces modèles, on peut citer VGG, qui utilise des filtres de taille 3x3 et des couches de convolution successives, Inception, qui utilise des blocs modulaires avec des branches parallèles de différentes tailles, ResNet, qui utilise des connexions résiduelles qui permettent de sauter des couches, DenseNet, qui utilise des connexions denses qui relient toutes les couches entre elles, ou encore EfficientNet, qui utilise une recherche automatique de l'architecture optimale du réseau. Ces modèles ont atteint des niveaux de performance proches ou supérieurs à ceux des humains sur la classification d'images.\", metadata={'source': 'microsoft_learn\\\\Paul\\\\Les services cognitifs Azure.pdf'}),\n",
       " Document(page_content=\"Outre les modèles, plusieurs techniques ont été développées pour améliorer le deep learning pour la classification d'images. Parmi ces techniques, on peut citer l'augmentation des données, qui consiste à appliquer des transformations aléatoires sur les images d'entraînement, comme le recadrage, le retournement, la rotation, le bruit, ou la variation des couleurs, pour augmenter la diversité et la robustesse du modèle. On peut aussi citer le transfert d'apprentissage, qui consiste à utiliser un modèle pré-entraîné sur un jeu de données de grande taille, comme ImageNet, et à l'adapter à un jeu de données de plus petite taille, en conservant les couches de convolution, qui captent des caractéristiques génériques, et en ré-entraînant les couches entièrement connectées, qui captent des caractéristiques spécifiques. On peut également> citer l'apprentissage par auto-étiquetage, qui consiste à utiliser un modèle pré-entraîné pour générer des étiquettes pour des images non annotées, et à les utiliser comme données d'entraînement supplémentaires.\\n\\nDétection d'objets La détection d'objets est la tâche qui consiste à identifier et à localiser les objets présents dans une image, en leur attribuant une classe et une boîte englobante. Par exemple, on peut vouloir détecter les personnes, les voitures, ou les animaux dans une image. La détection d'objets peut être vue comme un problème de régression supervisée, où l'on dispose d'un jeu de données d'images annotées avec les classes et les boîtes englobantes des objets, et où l'on cherche à apprendre un modèle qui prédit les classes et les boîtes englobantes des objets dans une image inconnue. Le deep learning a permis de réaliser des progrès significatifs dans la détection d'objets, en utilisant des modèles basés sur des CNN, qui peuvent être classés en deux catégories : les modèles à deux étapes, qui utilisent une région d'intérêt pour localiser les objets, et les modèles à une étape, qui détectent les objets directement à partir de l'image entière.\\n\\nModèles à deux étapes Les modèles à deux étapes sont des modèles qui utilisent une région d'intérêt (RoI), qui est une sous- région de l'image qui contient potentiellement un objet, pour localiser les objets. Les modèles à deux étapes sont composés de deux parties : un extracteur de caractéristiques, qui utilise un CNN pour extraire des caractéristiques de l'image entière, et un détecteur de région, qui utilise un autre CNN pour détecter les objets à l'intérieur de chaque RoI. Les modèles à deux étapes sont généralement plus précis que les modèles à une étape, mais aussi plus lents.\\n\\nLe modèle R-CNN est le premier modèle à deux étapes, qui utilise un algorithme de segmentation, comme le recherche sélective, pour générer environ 2000 RoI par image, et qui utilise un CNN pré- entraîné, comme VGG ou AlexNet, pour extraire des caractéristiques de chaque RoI. Le modèle R-CNN utilise ensuite un classifieur SVM, qui utilise les caractéristiques extraites comme entrée, pour prédire la classe de chaque RoI, et un régresseur linéaire, qui utilise également les caractéristiques extraites comme entrée, pour affiner les boîtes englobantes de chaque RoI. Le> modèle R-CNN a permis de réduire l'erreur de détection sur le jeu de données PASCAL VOC, qui est un jeu de données de référence pour la détection d'objets, qui contient environ 20 000 images, réparties en 20 classes. Cependant, le modèle R-CNN est très lent, car il nécessite d'exécuter le CNN sur chaque RoI individuellement, et utilise plusieurs composants distincts, qui doivent être entraînés séparément.\\n\\nLe modèle Fast R-CNN améliore le modèle R-CNN, en utilisant une couche de regroupement de région (RoIPool), qui permet de réduire le nombre d'exécutions du CNN, en extrayant les caractéristiques de\", metadata={'source': 'microsoft_learn\\\\Paul\\\\Les services cognitifs Azure.pdf'}),\n",
       " Document(page_content=\"Le modèle Fast R-CNN améliore le modèle R-CNN, en utilisant une couche de regroupement de région (RoIPool), qui permet de réduire le nombre d'exécutions du CNN, en extrayant les caractéristiques de\\n\\nl'image entière une seule fois, et en les partageant entre les différentes RoI. Le modèle Fast R-CNN utilise également un seul réseau, qui combine le classifieur et le régresseur, et qui peut être entraîné de bout en bout. Le modèle Fast R-CNN est plus rapide et plus précis que le modèle R-CNN, mais il dépend toujours d'un algorithme externe pour générer les RoI, ce qui limite sa performance et sa flexibilité.\\n\\nLe modèle Faster R-CNN résout le problème du modèle Fast R-CNN, en utilisant un réseau de proposition de région (RPN), qui est un autre CNN, qui prend les caractéristiques extraites par l'extracteur de caractéristiques comme entrée, et qui prédit les RoI à partir de plusieurs ancres, qui sont des boîtes englobantes de tailles et de ratios variés, situées à différentes positions de l'image. Le modèle Faster R- CNN utilise ensuite la même couche RoIPool et le même détecteur de région que le modèle Fast R-CNN, pour prédire la classe et la boîte englobante de chaque RoI. Le modèle Faster R-CNN est plus rapide et plus précis que le modèle Fast R-CNN, car il utilise un seul réseau, qui peut être entraîné de bout en bout, et qui génère les RoI de manière adaptative.\\n\\nModèles à une étape Les modèles à une étape sont des modèles qui détectent les objets directement à partir de l'image entière, sans utiliser de RoI. Les modèles à une étape sont composés d'un seul réseau, qui utilise un CNN pour extraire des caractéristiques de l'image, et qui prédit la classe et la boîte englobante de chaque objet à partir de plusieurs ancres, comme le modèle Faster R-CNN. Les modèles à une étape sont généralement plus rapides que les modèles à deux étapes, mais aussi moins précis.\\n\\nLe modèle YOLO est le premier modèle à une étape, qui divise l'image> en une grille de cellules, et qui prédit la classe et la boîte englobante de chaque objet à partir de plusieurs ancres par cellule. Le modèle YOLO utilise un seul réseau, qui combine l'extracteur de caractéristiques et le détecteur d'objets, et qui peut être entraîné de bout en bout. Le modèle YOLO est très rapide, car il traite l'image entière en une seule fois, mais il a tendance à manquer les petits objets, ou les objets proches les uns des autres, car il limite le nombre d'objets par cellule.\\n\\nLe modèle SSD améliore le modèle YOLO, en utilisant plusieurs cartes de caractéristiques à différentes échelles, pour détecter les objets de différentes tailles, et en utilisant plus d'ancres par cellule, pour détecter les objets de différents ratios. Le modèle SSD utilise également un seul réseau, qui combine l'extracteur de caractéristiques et le détecteur d'objets, et qui peut être entraîné de bout en bout. Le modèle SSD est plus précis que le modèle YOLO, car il capture mieux les objets de différentes tailles et formes, mais il est aussi plus lent.\\n\\nConclusion Le deep learning pour la classification d'images et la détection d'objets est un domaine en constante évolution, qui continue de proposer des modèles et des techniques innovants, pour relever les défis liés à la complexité et à la diversité des données visuelles.\", metadata={'source': 'microsoft_learn\\\\Paul\\\\Les services cognitifs Azure.pdf'})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "\n",
    "directory_path = \"microsoft_learn\"\n",
    "\n",
    "loader = DirectoryLoader(directory_path)\n",
    "pages = loader.load_and_split()\n",
    "pages\n",
    "\n",
    "# partition_pdf is not available. Install the pdf dependencies with pip install \"unstructured[pdf]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAG : recherche par similarité (en utilisant les embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract text from pdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pdfminer.six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pdfminer.high_level import extract_text\n",
    "\n",
    "\n",
    "# dirs = os.listdir(directory_path)\n",
    "# output = \"pdftotext.txt\"\n",
    "# # supprimer le fichier s'il existe déjà\n",
    "# if os.path.exists(output):  \n",
    "#     os.remove(output) \n",
    "# for file in dirs:\n",
    "#     path_file = os.path.join(directory_path, file)\n",
    "#     print(f\"PDF path file: {path_file}\")\n",
    "#     text = extract_text(path_file)\n",
    "#     with open(output, 'a', encoding='utf-8') as fichier:\n",
    "#         fichier.write(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pypdf in c:\\users\\paul.peton\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.2.0)\n",
      "Requirement already satisfied: typing_extensions>=4.0 in c:\\users\\paul.peton\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pypdf) (4.11.0)\n"
     ]
    }
   ],
   "source": [
    "# !pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader, DirectoryLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "\n",
    "# loader = TextLoader(\"pdftotext.txt\", encoding='utf-8')\n",
    "# loader = PyPDFLoader(\"microsoft_learn/intelligence_artificielle.pdf\")\n",
    "loader = DirectoryLoader(\"microsoft_learn/\", glob=\"**/*.pdf\", loader_cls=PyPDFLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader multi files types\n",
    "# https://github.com/langchain-ai/langchain/discussions/18559"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents= loader.load()\n",
    "for document in documents:\n",
    "    document.metadata[\"filename\"] = Path(document.metadata[\"source\"]).name\n",
    "    document.metadata[\"loading_date\"] = datetime.today().strftime('%Y%m%d')\n",
    "    document.metadata[\"user\"] = Path(document.metadata[\"source\"]).parent.name\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'microsoft_learn\\\\Liticia\\\\Attention.pdf',\n",
       " 'page': 0,\n",
       " 'filename': 'Attention.pdf',\n",
       " 'loading_date': '20240607',\n",
       " 'user': 'Liticia'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'microsoft_learn\\\\Paul\\\\Le deep learning pour les images.pdf',\n",
       " 'page': 0,\n",
       " 'filename': 'Le deep learning pour les images.pdf',\n",
       " 'loading_date': '20240607',\n",
       " 'user': 'Paul'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[1].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Attention !  \\nCe fichier contient des informations confidentielles et sensibles qui ne doivent pas être divulguées \\nà des tiers. La lecture de ce fichier est strictement réservée aux personnes autorisées par le \\npropriétaire ou le créateur du fichier. Toute tentative de l ecture non autorisée ou de copie du fichier \\nconstitue une violation de la loi et expose le contrevenant à des sanctions pénales et civiles.  \\nSi vous n'êtes pas une personne autorisée, veuillez fermer immédiatement ce fichier et le \\nsupprimer de votre appareil. Si vous avez reçu ce fichier par erreur, veuillez en informer le \\npropriétaire ou le créateur du fichier et lui renvoyer ou lui détruire l e fichier. Si vous êtes une \\npersonne autorisée, veuillez vous assurer que le fichier est protégé par un mot de passe et qu'il \\nn'est pas accessible à des personnes non autorisées.  \\nMerci de respecter la confidentialité et la sécurité des informations contenues dans ce fichier.  \\n \""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk settings  \n",
    "chunk_size = 200\n",
    "chunk_overlap = 30 # duplication des informations entre les vecteurs coupés "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb of chunks: 1067\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "# text_splitter = CharacterTextSplitter(\n",
    "#                     chunk_size=chunk_size,\n",
    "#                     chunk_overlap=chunk_overlap,\n",
    "#                     separator=\"\\n\",\n",
    "#                 )\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "                    chunk_size=chunk_size,\n",
    "                    chunk_overlap=chunk_overlap,\n",
    "                    separators=[\"\\n\"],\n",
    "                )\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "nb_chunks = len(chunks)\n",
    "print(f\"Nb of chunks: {nb_chunks}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Attention !  \\nCe fichier contient des informations confidentielles et sensibles qui ne doivent pas être divulguées', metadata={'source': 'microsoft_learn\\\\Liticia\\\\Attention.pdf', 'page': 0, 'filename': 'Attention.pdf', 'loading_date': '20240607', 'user': 'Liticia'})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paul\n",
      "évolution, qui continue de proposer des modèles et des techniques innovants, pour relever les défis liés \n",
      "à la complexité et à la diversité des données visuelles.\n"
     ]
    }
   ],
   "source": [
    "print(chunks[nb_chunks-1].metadata[\"user\"])\n",
    "print(chunks[nb_chunks-1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://stackoverflow.com/questions/76886954/multiple-file-loading-and-embeddings-with-openai\n",
    "\n",
    "# print(\"Loading data...\")\n",
    "# pdf_folder_path = \"content/\"\n",
    "# print(os.listdir(pdf_folder_path))\n",
    "\n",
    "# # Load multiple files\n",
    "# loaders = [UnstructuredPDFLoader(os.path.join(pdf_folder_path, fn)) for fn in os.listdir(pdf_folder_path)]\n",
    "\n",
    "# print(loaders)\n",
    "\n",
    "# all_documents = []\n",
    "\n",
    "# for loader in loaders:\n",
    "#     print(\"Loading raw document...\" + loader.file_path)\n",
    "#     raw_documents = loader.load()\n",
    "\n",
    "#     print(\"Splitting text...\")\n",
    "#     text_splitter = CharacterTextSplitter(\n",
    "#         separator=\"\\n\\n\",\n",
    "#         chunk_size=800,\n",
    "#         chunk_overlap=100,\n",
    "#         length_function=len,\n",
    "#     )\n",
    "#     documents = text_splitter.split_documents(raw_documents)\n",
    "#     all_documents.extend(documents)\n",
    "\n",
    "# print(\"Creating vectorstore...\")\n",
    "# embeddings = OpenAIEmbeddings()\n",
    "# vectorstore = FAISS.from_documents(all_documents, embeddings)\n",
    "\n",
    "# with open(\"vectorstore.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(vectorstore, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enregistrer les chunks en tant que vecteur en utilisant faiss  \n",
    "pour résoudre le problème des tokens appels nombreux API, on découpe les chunks\n",
    " en blocs , sleep \n",
    "save le tout en local pour ne pas relancer à chaque fois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U langchain-community faiss-cpu langchain-openai tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `AzureOpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import AzureOpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings import AzureOpenAIEmbeddings\n",
    "# https://github.com/facebookresearch/faiss/wiki/Special-operations-on-indexes\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=\"text-embedding-ada-002\",\n",
    "    api_key=os.environ[\"AZURE_OPENAI_KEY\"],\n",
    ")\n",
    "\n",
    "CALC_EMBEDDINGS = False\n",
    "BATCH_EMBEDDINGS = False\n",
    "\n",
    "if CALC_EMBEDDINGS:\n",
    "    if BATCH_EMBEDDINGS:\n",
    "\n",
    "        # Iterate over each chunk of documents\n",
    "        # https://github.com/langchain-ai/langchain/issues/7343\n",
    "        # https://stackoverflow.com/questions/76644262/adding-large-chunks-of-text-after-embedding-into-pinecone-without-openai-ratelim\n",
    "        batch_size = 50\n",
    "\n",
    "        for i in range(0, len(chunks), batch_size):\n",
    "            print(f\"batch {i} to {i+batch_size}\")\n",
    "            if 'faiss_vectorstore' not in locals() or 'faiss_vectorstore' not in globals():\n",
    "                faiss_vectorstore = FAISS.from_documents(chunks[i:i+batch_size], embeddings)\n",
    "            else:\n",
    "                faiss_vectorstore.add_documents(chunks[i:i+batch_size])\n",
    "                print(f\"...sleep 6s...\")\n",
    "                sleep(6)  # Add delay here\n",
    "    else:\n",
    "        faiss_vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "    faiss_retriever = faiss_vectorstore.as_retriever()\n",
    "    faiss_vectorstore.save_local(\"faiss_index\")\n",
    "else:\n",
    "    FAISS.load_local(\"faiss_index\", embeddings)\n",
    "\n",
    "# TODO : optimiser la taille des chunks plus tard\n",
    "# LangChainDeprecationWarning: The class `AzureOpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import AzureOpenAIEmbeddings`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# #with open(\"index.pkl\", \"ab\") as f:\n",
    "# with open(\"index.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(faiss_vectorstore, f)\n",
    "#     f.close()\n",
    "\n",
    "# # DEBUG : cannot pickle '_thread.RLock' object\n",
    "# # https://github.com/langchain-ai/langchain/issues/14581"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Qu'est ce que le service Azure OpenAI ?\"\n",
    "question = \"Citer trois services cognitifs Azure.\"\n",
    "# question = \"Ce document est-il autorisé ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieve docs: 4\n",
      "- 0 from Les modèles de fondation de l IA générative.pdf : \n",
      "'''comprendre et d'analyser le contenu visuel, comme les images, les vidéos ou les flux en direct. Les \n",
      "services cognitifs Azure de la vision comprennent :'''\n",
      "- 1 from Le deep learning pour les images.pdf : \n",
      "'''comprendre et d'analyser le contenu visuel, comme les images, les vidéos ou les flux en direct. Les \n",
      "services cognitifs Azure de la vision comprennent :'''\n",
      "- 2 from Les services cognitifs Azure.pdf : \n",
      "'''comprendre et d'analyser le contenu visuel, comme les images, les vidéos ou les flux en direct. Les \n",
      "services cognitifs Azure de la vision comprennent :'''\n",
      "- 3 from Le service Azure OpenAI.pdf : \n",
      "'''comprendre et d'analyser le contenu visuel, comme les images, les vidéos ou les flux en direct. Les \n",
      "services cognitifs Azure de la vision comprennent :'''\n"
     ]
    }
   ],
   "source": [
    "searchDocs = faiss_vectorstore.similarity_search(question, filter={\"user\":\"Paul\"} )\n",
    "\n",
    "print(f\"Retrieve docs: {len(searchDocs)}\")\n",
    "for i in range(len(searchDocs)):\n",
    "    print(f\"- {i} from {searchDocs[i].metadata['filename']} : \\n'''{searchDocs[i].page_content}'''\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrievers are used to find relevant documents or passages that contain the answer to a given query.\n",
    "The system \"retrieves\" any documents that could be relevant in answering the question, and then passes those documents (along with the original question) to the language model for a \"generation\" step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La question est posée en utilisant le chat,\n",
    "toujours en utilisant la similarité sématique \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test utilisant LCEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Utilisez les éléments de contexte suivants pour répondre à la question contenue dans les 3 crochets à la fin. Si vous ne connaissez pas la réponse, dites simplement que vous ne savez pas, n'essayez pas d'inventer une réponse.\n",
    "Veuillez fournir une réponse correcte sur le plan factuel et basée sur les informations extraites du magasin de vecteurs.\n",
    "Veuillez également mentionner toute citation étayant la réponse, si elle est présente dans le contexte fourni, entre deux doubles guillemets \"\" .\n",
    "\n",
    "{context}\n",
    "\n",
    "QUESTION:```{question}```\n",
    "ANSWER:\n",
    "\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\",\"question\"]\n",
    "  )\n",
    "\n",
    "chain_type_kwargs = {\"prompt\": PROMPT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "# prompt_template =  \"\"\"\n",
    "#                     Answer the question only according to the context: {context}\n",
    "#                     Question: {question}\n",
    "#                     \"\"\"\n",
    "\n",
    "# PROMPT = ChatPromptTemplate.from_template(prompt_template)\n",
    "\n",
    "retrieval_chain = (\n",
    "    {\"context\": faiss_retriever, \"question\": RunnablePassthrough()}\n",
    "    | PROMPT\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Qu'est ce que le service Azure OpenAI ?\"\n",
    "question = \"Citer trois services cognitifs Azure.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Le texte donne un seul service cognitif Azure, qui est celui de la vision. Aucun autre service n\\'est mentionné. Donc, la réponse à la question est: \"Le texte ne donne qu\\'un seul service cognitif Azure, qui est celui de la vision.\"'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "\n",
    "filtered_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"filter\": {\"user\":'Paul'}})\n",
    "\n",
    "qa_stuff = RetrievalQA.from_chain_type( #creer une chain pour répondre à la question\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", # tous les documents d'un coup et les passe au LLM\n",
    "    # chain_type_kwargs=chain_type_kwargs,\n",
    "    retriever=filtered_retriever, \n",
    "    return_source_documents= True,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'Est-ce que le contenu du contexte est autorisé ?',\n",
       " 'result': \"Oui, le contenu du contexte est autorisé. Il s'agit d'une répétition de la même phrase qui décrit la capacité à générer du contenu à partir de différents types de données brutes.\",\n",
       " 'source_documents': [Document(page_content='générer du contenu à partir de données brutes, comme du texte, des images, des sons ou des vidéos.', metadata={'source': 'microsoft_learn\\\\Paul\\\\Le deep learning pour les images.pdf', 'page': 0, 'filename': 'Le deep learning pour les images.pdf', 'loading_date': '20240607', 'user': 'Paul'}),\n",
       "  Document(page_content='générer du contenu à partir de données brutes, comme du texte, des images, des sons ou des vidéos.', metadata={'source': 'microsoft_learn\\\\Paul\\\\Le service Azure OpenAI.pdf', 'page': 0, 'filename': 'Le service Azure OpenAI.pdf', 'loading_date': '20240607', 'user': 'Paul'}),\n",
       "  Document(page_content='générer du contenu à partir de données brutes, comme du texte, des images, des sons ou des vidéos.', metadata={'source': 'microsoft_learn\\\\Paul\\\\Les modèles de fondation de l IA générative.pdf', 'page': 0, 'filename': 'Les modèles de fondation de l IA générative.pdf', 'loading_date': '20240607', 'user': 'Paul'}),\n",
       "  Document(page_content='générer du contenu à partir de données brutes, comme du texte, des images, des sons ou des vidéos.', metadata={'source': 'microsoft_learn\\\\Paul\\\\Les services cognitifs Azure.pdf', 'page': 0, 'filename': 'Les services cognitifs Azure.pdf', 'loading_date': '20240607', 'user': 'Paul'})]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# question = \"Pourquoi utiliser Azure OpenAI ?\"\n",
    "# question = \"Quel service utiliser pour de la traduction ?\"\n",
    "question = \"Est-ce que le contenu du contexte est autorisé ?\"\n",
    "\n",
    "result = qa_stuff.invoke(question)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Préparer un golden dataset \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Generating:   0%|          | 0/10 [00:00<?, ?it/s]                "
     ]
    }
   ],
   "source": [
    "# TODO : generate benchmark dataset with RAGAS\n",
    "# https://docs.ragas.io/en/stable/getstarted/testset_generation.html\n",
    "\n",
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "# generator with openai models\n",
    "generator_llm = llm\n",
    "critic_llm = llm4\n",
    "# embeddings = OpenAIEmbeddings()\n",
    "\n",
    "generator = TestsetGenerator.from_langchain(\n",
    "    generator_llm,\n",
    "    critic_llm,\n",
    "    embeddings\n",
    ")\n",
    "\n",
    "# generate testset\n",
    "testset = generator.generate_with_langchain_docs(documents, test_size=10, distributions={simple: 0.5, reasoning: 0.25, multi_context: 0.25})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>evolution_type</th>\n",
       "      <th>metadata</th>\n",
       "      <th>episode_done</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are the API Azure OpenAI and how can they...</td>\n",
       "      <td>[ce que le service Azure OpenAI ? \\nLe service...</td>\n",
       "      <td>nan</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'pdftotext.txt'}]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the purpose of the service for extract...</td>\n",
       "      <td>[ qui permet d'extraire et de structurer les d...</td>\n",
       "      <td>nan</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'pdftotext.txt'}]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the Model R-CNN and how does it work i...</td>\n",
       "      <td>[u de \\ndonnées de plus petite taille, en cons...</td>\n",
       "      <td>nan</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'pdftotext.txt'}, {'source': 'pdft...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are one-stage models and how do they diff...</td>\n",
       "      <td>[ le \\nmodèle R-CNN est très lent, car il néce...</td>\n",
       "      <td>nan</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'pdftotext.txt'}]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are the API Azure OpenAI and how can they...</td>\n",
       "      <td>[ce que le service Azure OpenAI ? \\nLe service...</td>\n",
       "      <td>nan</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'pdftotext.txt'}]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What are the two categories of CNN-based model...</td>\n",
       "      <td>[u de \\ndonnées de plus petite taille, en cons...</td>\n",
       "      <td>nan</td>\n",
       "      <td>reasoning</td>\n",
       "      <td>[{'source': 'pdftotext.txt'}]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How does Azure OpenAI service utilize AI and w...</td>\n",
       "      <td>[ce que le service Azure OpenAI ? \\nLe service...</td>\n",
       "      <td>The Azure OpenAI service utilizes AI by provid...</td>\n",
       "      <td>reasoning</td>\n",
       "      <td>[{'source': 'pdftotext.txt'}]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How do CNNs aid in image generation for Azure ...</td>\n",
       "      <td>[Les modèles de fondation de l’IA générative \\...</td>\n",
       "      <td>nan</td>\n",
       "      <td>multi_context</td>\n",
       "      <td>[{'source': 'pdftotext.txt'}]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>How has deep learning enhanced object detectio...</td>\n",
       "      <td>[u de \\ndonnées de plus petite taille, en cons...</td>\n",
       "      <td>nan</td>\n",
       "      <td>multi_context</td>\n",
       "      <td>[{'source': 'pdftotext.txt'}]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What is the task of object detection and how h...</td>\n",
       "      <td>[u de \\ndonnées de plus petite taille, en cons...</td>\n",
       "      <td>The task of object detection is to identify an...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'pdftotext.txt'}]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What are the API Azure OpenAI and how can they...   \n",
       "1  What is the purpose of the service for extract...   \n",
       "2  What is the Model R-CNN and how does it work i...   \n",
       "3  What are one-stage models and how do they diff...   \n",
       "4  What are the API Azure OpenAI and how can they...   \n",
       "5  What are the two categories of CNN-based model...   \n",
       "6  How does Azure OpenAI service utilize AI and w...   \n",
       "7  How do CNNs aid in image generation for Azure ...   \n",
       "8  How has deep learning enhanced object detectio...   \n",
       "9  What is the task of object detection and how h...   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [ce que le service Azure OpenAI ? \\nLe service...   \n",
       "1  [ qui permet d'extraire et de structurer les d...   \n",
       "2  [u de \\ndonnées de plus petite taille, en cons...   \n",
       "3  [ le \\nmodèle R-CNN est très lent, car il néce...   \n",
       "4  [ce que le service Azure OpenAI ? \\nLe service...   \n",
       "5  [u de \\ndonnées de plus petite taille, en cons...   \n",
       "6  [ce que le service Azure OpenAI ? \\nLe service...   \n",
       "7  [Les modèles de fondation de l’IA générative \\...   \n",
       "8  [u de \\ndonnées de plus petite taille, en cons...   \n",
       "9  [u de \\ndonnées de plus petite taille, en cons...   \n",
       "\n",
       "                                        ground_truth evolution_type  \\\n",
       "0                                                nan         simple   \n",
       "1                                                nan         simple   \n",
       "2                                                nan         simple   \n",
       "3                                                nan         simple   \n",
       "4                                                nan         simple   \n",
       "5                                                nan      reasoning   \n",
       "6  The Azure OpenAI service utilizes AI by provid...      reasoning   \n",
       "7                                                nan  multi_context   \n",
       "8                                                nan  multi_context   \n",
       "9  The task of object detection is to identify an...         simple   \n",
       "\n",
       "                                            metadata  episode_done  \n",
       "0                      [{'source': 'pdftotext.txt'}]          True  \n",
       "1                      [{'source': 'pdftotext.txt'}]          True  \n",
       "2  [{'source': 'pdftotext.txt'}, {'source': 'pdft...          True  \n",
       "3                      [{'source': 'pdftotext.txt'}]          True  \n",
       "4                      [{'source': 'pdftotext.txt'}]          True  \n",
       "5                      [{'source': 'pdftotext.txt'}]          True  \n",
       "6                      [{'source': 'pdftotext.txt'}]          True  \n",
       "7                      [{'source': 'pdftotext.txt'}]          True  \n",
       "8                      [{'source': 'pdftotext.txt'}]          True  \n",
       "9                      [{'source': 'pdftotext.txt'}]          True  "
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test size: 60\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test size: {testset.to_pandas().size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "\"Qu'est-ce que le service Azure OpenAI ?\",\n",
    "\"Comment fonctionne le service Azure OpenAI ?\",\n",
    "\"Quels sont les avantages du service Azure OpenAI ?\",\n",
    "\"Quels sont les cas d'utilisation du service Azure OpenAI ?\",\n",
    "\"Comment mettre en œuvre le service Azure OpenAI ?\",\n",
    "\"Quels sont les composants principaux du service Azure OpenAI ?\",\n",
    "\"Quels sont les modèles d'IA accessibles via le service Azure OpenAI ?\",\n",
    "\"Quelles tâches les modèles d'IA du service Azure OpenAI peuvent-ils accomplir ?\",\n",
    "\"Quelle est l'infrastructure sur laquelle repose le service Azure OpenAI ?\",\n",
    "\"Quels outils le service Azure OpenAI fournit-il pour la formation et le réglage des modèles d'IA ?\"\n",
    "]\n",
    "\n",
    "answers = [  \n",
    "\"Le service Azure OpenAI est un service cloud qui permet aux développeurs et aux chercheurs d'accéder à la plateforme OpenAI, une initiative qui vise à créer une intelligence artificielle générale (AGI) bénéfique pour l'humanité. Il offre la possibilité d'utiliser des modèles d'IA pré-entraînés ou personnalisés, de les déployer sur le cloud ou en périphérie, et de les intégrer à des applications et des scénarios variés.\",   \n",
    "\"Le service Azure OpenAI repose sur deux composants principaux : le portail Azure OpenAI et les API Azure OpenAI. Le portail est une interface web qui permet de créer, de gérer et de surveiller les ressources du service, tandis que les API sont des interfaces de programmation qui permettent d'interagir avec les modèles d'IA du service.\",   \n",
    "\"Le service Azure OpenAI présente plusieurs avantages, parmi lesquels l'accès à des modèles d'IA de pointe, une grande flexibilité, une haute performance grâce à l'infrastructure cloud de Microsoft Azure, et une facilité d'intégration grâce à des API simples et des SDK dans différents langages de programmation.\",   \n",
    "\"Le service Azure OpenAI peut être utilisé pour de nombreux cas d'utilisation, dans différents domaines et secteurs d'activité, tels que l'éducation, la santé, le divertissement et le commerce.\",   \n",
    "\"Pour mettre en œuvre le service Azure OpenAI, il faut créer un compte Microsoft Azure, souscrire à un abonnement au service, créer une clé d'API, choisir ou créer un modèle d'IA, créer un point de terminaison, envoyer des requêtes au point de terminaison, et analyser les résultats des requêtes.\",   \n",
    "\"Les composants principaux du service Azure OpenAI sont le portail Azure OpenAI et les API Azure OpenAI.\",   \n",
    "\"Les modèles d'IA accessibles via le service Azure OpenAI incluent GPT-3, Codex, DALL-E et CLIP.\",   \n",
    "\"Les modèles d'IA du service Azure OpenAI peuvent réaliser des tâches complexes et variées, telles que la génération de texte, la compréhension du langage naturel, la vision par ordinateur, la synthèse vocale ou la création de contenu multimédia.\",   \n",
    "\"L'infrastructure sur laquelle repose le service Azure OpenAI est l'infrastructure cloud de Microsoft Azure.\",   \n",
    "\"Le service Azure OpenAI fournit des outils de formation et de réglage pour créer son propre modèle d'IA.\"\n",
    "]  \n",
    "\n",
    "ground_truths = [  \n",
    "[\"Le service Azure OpenAI est un service cloud qui permet aux développeurs et aux chercheurs d'accéder à la plateforme OpenAI\", \"Le service Azure OpenAI offre la possibilité d'utiliser des modèles d'IA pré-entraînés ou personnalisés, de les déployer sur le cloud ou en périphérie, et de les intégrer à des applications et des scénarios variés.\"],  \n",
    "[\"Le service Azure OpenAI repose sur deux composants principaux : le portail Azure OpenAI et les API Azure OpenAI.\", \"Le portail Azure OpenAI est une interface web qui permet de créer, de gérer et de surveiller les ressources du service Azure OpenAI, telles que les abonnements, les clés, les modèles, les points de terminaison et les requêtes.\"],  \n",
    "[\"Le service Azure OpenAI présente plusieurs avantages, parmi lesquels :\", \"Il garantit une haute performance, en s'appuyant sur l'infrastructure cloud de Microsoft Azure, qui assure une disponibilité, une scalabilité et une sécurité optimales.\"],  \n",
    "[\"Le service Azure OpenAI peut être utilisé pour de nombreux cas d'utilisation, dans différents domaines et secteurs d'activité.\", \"Dans le domaine du commerce, le service Azure OpenAI peut être utilisé pour créer des outils d'optimisation, tels que des recommandations personnalisées, des prévisions de ventes, des analyses de marché ou des chatbots.\"],  \n",
    "[\"Pour mettre en œuvre le service Azure OpenAI, il faut suivre les étapes suivantes :\", \"Analyser les résultats des requêtes, en utilisant les outils de suivi et de statistiques fournis par le portail Azure OpenAI.\"],  \n",
    "[\"Le service Azure OpenAI repose sur deux composants principaux : le portail Azure OpenAI et les API Azure OpenAI.\", \"Les API Azure OpenAI sont des interfaces de programmation qui permettent d'interagir avec les modèles d'IA du service Azure OpenAI, tels que GPT-3, Codex, DALL-E ou CLIP.\"],  \n",
    "[\"Les API Azure OpenAI sont des interfaces de programmation qui permettent d'interagir avec les modèles d'IA du service Azure OpenAI\", \"tels que GPT-3, Codex, DALL-E ou CLIP.\"],  \n",
    "[\"Il permet d'accéder à des modèles d'IA de pointe, capables de réaliser des tâches complexes et variées\", \"telles que la génération de texte, la compréhension du langage naturel, la vision par ordinateur, la synthèse vocale ou la création de contenu multimédia.\"],  \n",
    "[\"Il garantit une haute performance, en s'appuyant sur l'infrastructure cloud de Microsoft Azure\", \"qui assure une disponibilité, une scalabilité et une sécurité optimales.\"],  \n",
    "[\"Choisir un modèle d'IA, parmi ceux proposés par le service Azure OpenAI\", \"ou créer son propre modèle, en utilisant les outils de formation et de réglage fournis par le service Azure OpenAI.\"]  \n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "contexts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "for query in questions:\n",
    "    result = qa_stuff({\"query\": query})\n",
    "    results.append(result['result']) \n",
    "    sources = result[\"source_documents\"]\n",
    "    contents = [source.page_content for source in sources]  \n",
    "\n",
    "    processed_contents = []\n",
    "    for content in contents:\n",
    "        if isinstance(content, str):  \n",
    "            processed_content = content.split('.')[:2]  \n",
    "            processed_contents.extend(processed_content)  \n",
    "        else:\n",
    "            print(f\"Expected a string but got {type(content)}\") \n",
    "    contexts.append(processed_contents)\n",
    "\n",
    "print(len(questions))\n",
    "print(len(ground_truths))\n",
    "print(len(results))\n",
    "print(len(contexts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            question  \\\n",
      "0            Qu'est-ce que le service Azure OpenAI ?   \n",
      "1       Comment fonctionne le service Azure OpenAI ?   \n",
      "2  Quels sont les avantages du service Azure Open...   \n",
      "3  Quels sont les cas d'utilisation du service Az...   \n",
      "4  Comment mettre en œuvre le service Azure OpenAI ?   \n",
      "\n",
      "                                              answer  \\\n",
      "0  Le service Azure OpenAI est un outil puissant ...   \n",
      "1  Le service Azure OpenAI repose sur deux compos...   \n",
      "2  Le texte mentionne qu'il y a plusieurs avantag...   \n",
      "3  Le document ne fournit pas d'informations spéc...   \n",
      "4  Pour mettre en œuvre le service Azure OpenAI, ...   \n",
      "\n",
      "                                            contexts  \\\n",
      "0  [Le service Azure OpenAI : un outil puissant \\...   \n",
      "1  [Comment fonctionne le service Azure OpenAI ? ...   \n",
      "2  [Quels sont les avantages du service Azure Ope...   \n",
      "3  [Quels sont les avantages du service Azure Ope...   \n",
      "4  [Comment mettre en œuvre le service Azure Open...   \n",
      "\n",
      "                                        ground_truth  \n",
      "0  [Le service Azure OpenAI est un service cloud ...  \n",
      "1  [Le service Azure OpenAI repose sur deux compo...  \n",
      "2  [Le service Azure OpenAI présente plusieurs av...  \n",
      "3  [Le service Azure OpenAI peut être utilisé pou...  \n",
      "4  [Pour mettre en œuvre le service Azure OpenAI,...  \n"
     ]
    }
   ],
   "source": [
    "# Creating a DataFrame\n",
    "data = {\n",
    "    'question': questions,\n",
    "    'answer': results,\n",
    "    'contexts': contexts,\n",
    "    'ground_truth': ground_truths\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df.head())\n",
    "\n",
    "# Save as .csv\n",
    "df.to_csv('ragas_aoai_dataset.csv', index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://blog.cellenza.com/securite-2/utiliser-azure-openai-langchain-et-ragas-pour-la-classification-des-documents-confidentiels-et-la-protection-des-donnees-sensibles/ \n",
    "\n",
    "# from ragas.testset.generator import TestsetGenerator\n",
    "# from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "\n",
    "# testset = generator.generate_with_langchain_docs(documents, test_size=10)\n",
    "\n",
    "# print(testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ragas datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "import pyarrow as py\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as ds\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import answer_relevancy, answer_similarity, answer_correctness\n",
    "from ragas.metrics import context_recall, context_precision, context_entity_recall, context_utilization, context_relevancy\n",
    "from ragas.metrics import faithfulness\n",
    "from ragas.metrics.critique import harmfulness, maliciousness, coherence, correctness, conciseness\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from datasets import Dataset, Features, Sequence, Value, load_dataset\n",
    "\n",
    "\n",
    "# ragas.metrics.answer_relevancy : Scores the relevancy of the answer according to the given question.\n",
    "# ragas.metrics.answer_similarity : Scores the semantic similarity of ground truth with generated answer.\n",
    "# ragas.metrics.answer_correctness : Measures answer correctness compared to ground truth as a combination of factuality and semantic similarity.\n",
    "# ragas.metrics.context_precision : Average Precision is a metric that evaluates whether all of the relevant items selected by the model are ranked higher or not.\n",
    "# ragas.metrics.context_recall : Estimates context recall by estimating TP and FN using annotated answer and retrieved context.\n",
    "# ragas.metrics.context_entity_recall : Calculates recall based on entities present in ground truth and context.\n",
    "\n",
    "# Définir le dataset explicitement \n",
    "features = Features({\n",
    "    \"question\": Value(\"string\"),\n",
    "    \"answer\": Value(\"string\"),\n",
    "    \"contexts\": Sequence(Value(\"string\")),\n",
    "    \"ground_truth\": Value(\"string\")\n",
    "})\n",
    "dataset = Dataset.from_dict(df, features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.llms import LangchainLLMWrapper\n",
    "\n",
    "# wrapper around azure_model \n",
    "ragas_azure_model = LangchainLLMWrapper(llm)\n",
    "# patch the new RagasLLM instance\n",
    "faithfulness.llm = ragas_azure_model\n",
    "answer_relevancy.llm = ragas_azure_model\n",
    "context_precision.llm = ragas_azure_model\n",
    "context_recall.llm = ragas_azure_model\n",
    "# harmfulness.llm = ragas_azure_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py:336: UserWarning: If you have openai>=1.0.0 installed and are using Azure, please use the `AzureOpenAIEmbeddings` class.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# only for answer_relevancy\n",
    "azure_embeddings = OpenAIEmbeddings(\n",
    "    # deployment=\"text-embedding-ada-002\",\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    openai_api_base=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    openai_api_key=os.getenv(\"AZURE_OPENAI_KEY\"),\n",
    "    openai_api_type=\"azure\"\n",
    ")\n",
    "\n",
    "answer_relevancy.embeddings = azure_embeddings\n",
    "answer_similarity.embeddings = azure_embeddings\n",
    "answer_correctness.embeddings = azure_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 20/20 [00:07<00:00,  2.58it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'context_recall': 0.9000, 'context_precision': 0.8920}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_contexts = evaluate(\n",
    "       dataset=dataset,\n",
    "       metrics=[context_recall, context_precision],\n",
    "       llm=ragas_azure_model,\n",
    "       raise_exceptions=False,\n",
    ")\n",
    "\n",
    "evaluate_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>context_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Qu'est-ce que le service Azure OpenAI ?</td>\n",
       "      <td>Le service Azure OpenAI est un outil puissant ...</td>\n",
       "      <td>[Le service Azure OpenAI : un outil puissant \\...</td>\n",
       "      <td>[\"Le service Azure OpenAI est un service cloud...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Comment fonctionne le service Azure OpenAI ?</td>\n",
       "      <td>Le service Azure OpenAI repose sur deux compos...</td>\n",
       "      <td>[Comment fonctionne le service Azure OpenAI ? ...</td>\n",
       "      <td>['Le service Azure OpenAI repose sur deux comp...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Quels sont les avantages du service Azure Open...</td>\n",
       "      <td>Le texte mentionne qu'il y a plusieurs avantag...</td>\n",
       "      <td>[Quels sont les avantages du service Azure Ope...</td>\n",
       "      <td>['Le service Azure OpenAI présente plusieurs a...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.876667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Quels sont les cas d'utilisation du service Az...</td>\n",
       "      <td>Le document ne fournit pas d'informations spéc...</td>\n",
       "      <td>[Quels sont les avantages du service Azure Ope...</td>\n",
       "      <td>[\"Le service Azure OpenAI peut être utilisé po...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.710000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Comment mettre en œuvre le service Azure OpenAI ?</td>\n",
       "      <td>Pour mettre en œuvre le service Azure OpenAI, ...</td>\n",
       "      <td>[Comment mettre en œuvre le service Azure Open...</td>\n",
       "      <td>['Pour mettre en œuvre le service Azure OpenAI...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Quels sont les composants principaux du servic...</td>\n",
       "      <td>Le service Azure OpenAI repose sur deux compos...</td>\n",
       "      <td>[Comment fonctionne le service Azure OpenAI ? ...</td>\n",
       "      <td>['Le service Azure OpenAI repose sur deux comp...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Quels sont les modèles d'IA accessibles via le...</td>\n",
       "      <td>Unfortunately, I do not have enough informatio...</td>\n",
       "      <td>[modèle, en utilisant les outils de formation ...</td>\n",
       "      <td>[\"Les API Azure OpenAI sont des interfaces de ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Quelles tâches les modèles d'IA du service Azu...</td>\n",
       "      <td>Je suis désolé, mais les informations fournies...</td>\n",
       "      <td>[modèle, en utilisant les outils de formation ...</td>\n",
       "      <td>[\"Il permet d'accéder à des modèles d'IA de po...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Quelle est l'infrastructure sur laquelle repos...</td>\n",
       "      <td>Le texte ne donne pas d'informations sur l'inf...</td>\n",
       "      <td>[Comment fonctionne le service Azure OpenAI ? ...</td>\n",
       "      <td>[\"Il garantit une haute performance, en s'appu...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Quels outils le service Azure OpenAI fournit-i...</td>\n",
       "      <td>Le service Azure OpenAI fournit des outils de ...</td>\n",
       "      <td>[modèle, en utilisant les outils de formation ...</td>\n",
       "      <td>[\"Choisir un modèle d'IA, parmi ceux proposés ...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0            Qu'est-ce que le service Azure OpenAI ?   \n",
       "1       Comment fonctionne le service Azure OpenAI ?   \n",
       "2  Quels sont les avantages du service Azure Open...   \n",
       "3  Quels sont les cas d'utilisation du service Az...   \n",
       "4  Comment mettre en œuvre le service Azure OpenAI ?   \n",
       "5  Quels sont les composants principaux du servic...   \n",
       "6  Quels sont les modèles d'IA accessibles via le...   \n",
       "7  Quelles tâches les modèles d'IA du service Azu...   \n",
       "8  Quelle est l'infrastructure sur laquelle repos...   \n",
       "9  Quels outils le service Azure OpenAI fournit-i...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  Le service Azure OpenAI est un outil puissant ...   \n",
       "1  Le service Azure OpenAI repose sur deux compos...   \n",
       "2  Le texte mentionne qu'il y a plusieurs avantag...   \n",
       "3  Le document ne fournit pas d'informations spéc...   \n",
       "4  Pour mettre en œuvre le service Azure OpenAI, ...   \n",
       "5  Le service Azure OpenAI repose sur deux compos...   \n",
       "6  Unfortunately, I do not have enough informatio...   \n",
       "7  Je suis désolé, mais les informations fournies...   \n",
       "8  Le texte ne donne pas d'informations sur l'inf...   \n",
       "9  Le service Azure OpenAI fournit des outils de ...   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [Le service Azure OpenAI : un outil puissant \\...   \n",
       "1  [Comment fonctionne le service Azure OpenAI ? ...   \n",
       "2  [Quels sont les avantages du service Azure Ope...   \n",
       "3  [Quels sont les avantages du service Azure Ope...   \n",
       "4  [Comment mettre en œuvre le service Azure Open...   \n",
       "5  [Comment fonctionne le service Azure OpenAI ? ...   \n",
       "6  [modèle, en utilisant les outils de formation ...   \n",
       "7  [modèle, en utilisant les outils de formation ...   \n",
       "8  [Comment fonctionne le service Azure OpenAI ? ...   \n",
       "9  [modèle, en utilisant les outils de formation ...   \n",
       "\n",
       "                                        ground_truth  context_recall  \\\n",
       "0  [\"Le service Azure OpenAI est un service cloud...             1.0   \n",
       "1  ['Le service Azure OpenAI repose sur deux comp...             1.0   \n",
       "2  ['Le service Azure OpenAI présente plusieurs a...             1.0   \n",
       "3  [\"Le service Azure OpenAI peut être utilisé po...             1.0   \n",
       "4  ['Pour mettre en œuvre le service Azure OpenAI...             0.5   \n",
       "5  ['Le service Azure OpenAI repose sur deux comp...             1.0   \n",
       "6  [\"Les API Azure OpenAI sont des interfaces de ...             1.0   \n",
       "7  [\"Il permet d'accéder à des modèles d'IA de po...             1.0   \n",
       "8  [\"Il garantit une haute performance, en s'appu...             1.0   \n",
       "9  [\"Choisir un modèle d'IA, parmi ceux proposés ...             0.5   \n",
       "\n",
       "   context_precision  \n",
       "0           1.000000  \n",
       "1           0.950000  \n",
       "2           0.876667  \n",
       "3           0.710000  \n",
       "4           0.450000  \n",
       "5           1.000000  \n",
       "6           0.966667  \n",
       "7           0.966667  \n",
       "8           1.000000  \n",
       "9           1.000000  "
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_contexts_df = evaluate_contexts.to_pandas()\n",
    "evaluate_contexts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 78, in _aresults\n",
      "    r = await future\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 37, in sema_coro\n",
      "    return await coro\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 111, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 125, in ascore\n",
      "    raise e\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 121, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_similarity.py\", line 65, in _ascore\n",
      "    embedding_1 = np.array(await self.embeddings.embed_text(ground_truth))\n",
      "AttributeError: 'OpenAIEmbeddings' object has no attribute 'embed_text'. Did you mean: 'embed_query'?\n",
      "Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 78, in _aresults\n",
      "    r = await future\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 37, in sema_coro\n",
      "    return await coro\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 111, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 125, in ascore\n",
      "    raise e\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 121, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_similarity.py\", line 65, in _ascore\n",
      "    embedding_1 = np.array(await self.embeddings.embed_text(ground_truth))\n",
      "AttributeError: 'OpenAIEmbeddings' object has no attribute 'embed_text'. Did you mean: 'embed_query'?\n",
      "Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 78, in _aresults\n",
      "    r = await future\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 37, in sema_coro\n",
      "    return await coro\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 111, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 125, in ascore\n",
      "    raise e\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 121, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_similarity.py\", line 65, in _ascore\n",
      "    embedding_1 = np.array(await self.embeddings.embed_text(ground_truth))\n",
      "AttributeError: 'OpenAIEmbeddings' object has no attribute 'embed_text'. Did you mean: 'embed_query'?\n",
      "Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 78, in _aresults\n",
      "    r = await future\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 37, in sema_coro\n",
      "    return await coro\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 111, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 125, in ascore\n",
      "    raise e\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 121, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_similarity.py\", line 65, in _ascore\n",
      "    embedding_1 = np.array(await self.embeddings.embed_text(ground_truth))\n",
      "AttributeError: 'OpenAIEmbeddings' object has no attribute 'embed_text'. Did you mean: 'embed_query'?\n",
      "Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 78, in _aresults\n",
      "    r = await future\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 37, in sema_coro\n",
      "    return await coro\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 111, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 125, in ascore\n",
      "    raise e\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 121, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_similarity.py\", line 65, in _ascore\n",
      "    embedding_1 = np.array(await self.embeddings.embed_text(ground_truth))\n",
      "AttributeError: 'OpenAIEmbeddings' object has no attribute 'embed_text'. Did you mean: 'embed_query'?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 78, in _aresults\n",
      "    r = await future\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 37, in sema_coro\n",
      "    return await coro\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 111, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 125, in ascore\n",
      "    raise e\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 121, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 167, in _ascore\n",
      "    return self._calculate_score(answers, row)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 139, in _calculate_score\n",
      "    cosine_sim = self.calculate_similarity(question, gen_questions)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 113, in calculate_similarity\n",
      "    question_vec = np.asarray(self.embeddings.embed_query(question)).reshape(1, -1)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 697, in embed_query\n",
      "    return self.embed_documents([text])[0]\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 668, in embed_documents\n",
      "    return self._get_len_safe_embeddings(texts, engine=engine)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 494, in _get_len_safe_embeddings\n",
      "    response = embed_with_retry(\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 116, in embed_with_retry\n",
      "    return embeddings.client.create(**kwargs)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\resources\\embeddings.py\", line 114, in create\n",
      "    return self._post(\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\_base_client.py\", line 1240, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\_base_client.py\", line 921, in request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\_base_client.py\", line 1020, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}\n",
      "Evaluating:  15%|█▌        | 6/40 [00:01<00:08,  4.10it/s]Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 78, in _aresults\n",
      "    r = await future\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 37, in sema_coro\n",
      "    return await coro\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 111, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 125, in ascore\n",
      "    raise e\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 121, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 167, in _ascore\n",
      "    return self._calculate_score(answers, row)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 139, in _calculate_score\n",
      "    cosine_sim = self.calculate_similarity(question, gen_questions)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 113, in calculate_similarity\n",
      "    question_vec = np.asarray(self.embeddings.embed_query(question)).reshape(1, -1)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 697, in embed_query\n",
      "    return self.embed_documents([text])[0]\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 668, in embed_documents\n",
      "    return self._get_len_safe_embeddings(texts, engine=engine)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 494, in _get_len_safe_embeddings\n",
      "    response = embed_with_retry(\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 116, in embed_with_retry\n",
      "    return embeddings.client.create(**kwargs)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\resources\\embeddings.py\", line 114, in create\n",
      "    return self._post(\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\_base_client.py\", line 1240, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\_base_client.py\", line 921, in request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\_base_client.py\", line 1020, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}\n",
      "Evaluating:  18%|█▊        | 7/40 [00:02<00:10,  3.07it/s]Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 78, in _aresults\n",
      "    r = await future\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 37, in sema_coro\n",
      "    return await coro\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 111, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 125, in ascore\n",
      "    raise e\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 121, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 167, in _ascore\n",
      "    return self._calculate_score(answers, row)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 139, in _calculate_score\n",
      "    cosine_sim = self.calculate_similarity(question, gen_questions)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 113, in calculate_similarity\n",
      "    question_vec = np.asarray(self.embeddings.embed_query(question)).reshape(1, -1)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 697, in embed_query\n",
      "    return self.embed_documents([text])[0]\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 668, in embed_documents\n",
      "    return self._get_len_safe_embeddings(texts, engine=engine)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 494, in _get_len_safe_embeddings\n",
      "    response = embed_with_retry(\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 116, in embed_with_retry\n",
      "    return embeddings.client.create(**kwargs)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\resources\\embeddings.py\", line 114, in create\n",
      "    return self._post(\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\_base_client.py\", line 1240, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\_base_client.py\", line 921, in request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\_base_client.py\", line 1020, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}\n",
      "Evaluating:  20%|██        | 8/40 [00:02<00:09,  3.50it/s]Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 78, in _aresults\n",
      "    r = await future\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 37, in sema_coro\n",
      "    return await coro\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 111, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 125, in ascore\n",
      "    raise e\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 121, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_similarity.py\", line 65, in _ascore\n",
      "    embedding_1 = np.array(await self.embeddings.embed_text(ground_truth))\n",
      "AttributeError: 'OpenAIEmbeddings' object has no attribute 'embed_text'. Did you mean: 'embed_query'?\n",
      "Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 78, in _aresults\n",
      "    r = await future\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 37, in sema_coro\n",
      "    return await coro\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 111, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 125, in ascore\n",
      "    raise e\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 121, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 167, in _ascore\n",
      "    return self._calculate_score(answers, row)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 139, in _calculate_score\n",
      "    cosine_sim = self.calculate_similarity(question, gen_questions)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 113, in calculate_similarity\n",
      "    question_vec = np.asarray(self.embeddings.embed_query(question)).reshape(1, -1)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 697, in embed_query\n",
      "    return self.embed_documents([text])[0]\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 668, in embed_documents\n",
      "    return self._get_len_safe_embeddings(texts, engine=engine)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 494, in _get_len_safe_embeddings\n",
      "    response = embed_with_retry(\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 116, in embed_with_retry\n",
      "    return embeddings.client.create(**kwargs)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\resources\\embeddings.py\", line 114, in create\n",
      "    return self._post(\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\_base_client.py\", line 1240, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\_base_client.py\", line 921, in request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\_base_client.py\", line 1020, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}\n",
      "Evaluating:  25%|██▌       | 10/40 [00:02<00:06,  4.44it/s]Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 78, in _aresults\n",
      "    r = await future\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 37, in sema_coro\n",
      "    return await coro\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 111, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 125, in ascore\n",
      "    raise e\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 121, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_similarity.py\", line 65, in _ascore\n",
      "    embedding_1 = np.array(await self.embeddings.embed_text(ground_truth))\n",
      "AttributeError: 'OpenAIEmbeddings' object has no attribute 'embed_text'. Did you mean: 'embed_query'?\n",
      "Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 78, in _aresults\n",
      "    r = await future\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 37, in sema_coro\n",
      "    return await coro\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 111, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 125, in ascore\n",
      "    raise e\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 121, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 167, in _ascore\n",
      "    return self._calculate_score(answers, row)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 139, in _calculate_score\n",
      "    cosine_sim = self.calculate_similarity(question, gen_questions)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 113, in calculate_similarity\n",
      "    question_vec = np.asarray(self.embeddings.embed_query(question)).reshape(1, -1)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 697, in embed_query\n",
      "    return self.embed_documents([text])[0]\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 668, in embed_documents\n",
      "    return self._get_len_safe_embeddings(texts, engine=engine)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 494, in _get_len_safe_embeddings\n",
      "    response = embed_with_retry(\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 116, in embed_with_retry\n",
      "    return embeddings.client.create(**kwargs)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\resources\\embeddings.py\", line 114, in create\n",
      "    return self._post(\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\_base_client.py\", line 1240, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\_base_client.py\", line 921, in request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\_base_client.py\", line 1020, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}\n",
      "Evaluating:  32%|███▎      | 13/40 [00:03<00:08,  3.19it/s]Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 78, in _aresults\n",
      "    r = await future\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 37, in sema_coro\n",
      "    return await coro\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 111, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 125, in ascore\n",
      "    raise e\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 121, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 167, in _ascore\n",
      "    return self._calculate_score(answers, row)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 139, in _calculate_score\n",
      "    cosine_sim = self.calculate_similarity(question, gen_questions)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 113, in calculate_similarity\n",
      "    question_vec = np.asarray(self.embeddings.embed_query(question)).reshape(1, -1)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 697, in embed_query\n",
      "    return self.embed_documents([text])[0]\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 668, in embed_documents\n",
      "    return self._get_len_safe_embeddings(texts, engine=engine)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 494, in _get_len_safe_embeddings\n",
      "    response = embed_with_retry(\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 116, in embed_with_retry\n",
      "    return embeddings.client.create(**kwargs)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\resources\\embeddings.py\", line 114, in create\n",
      "    return self._post(\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\_base_client.py\", line 1240, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\_base_client.py\", line 921, in request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\_base_client.py\", line 1020, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}\n",
      "Evaluating:  38%|███▊      | 15/40 [00:04<00:11,  2.23it/s]Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 78, in _aresults\n",
      "    r = await future\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 37, in sema_coro\n",
      "    return await coro\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 111, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 125, in ascore\n",
      "    raise e\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 121, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 167, in _ascore\n",
      "    return self._calculate_score(answers, row)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 139, in _calculate_score\n",
      "    cosine_sim = self.calculate_similarity(question, gen_questions)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 113, in calculate_similarity\n",
      "    question_vec = np.asarray(self.embeddings.embed_query(question)).reshape(1, -1)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 697, in embed_query\n",
      "    return self.embed_documents([text])[0]\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 668, in embed_documents\n",
      "    return self._get_len_safe_embeddings(texts, engine=engine)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 494, in _get_len_safe_embeddings\n",
      "    response = embed_with_retry(\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 116, in embed_with_retry\n",
      "    return embeddings.client.create(**kwargs)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\resources\\embeddings.py\", line 114, in create\n",
      "    return self._post(\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\_base_client.py\", line 1240, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\_base_client.py\", line 921, in request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\_base_client.py\", line 1020, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}\n",
      "Evaluating:  40%|████      | 16/40 [00:05<00:09,  2.48it/s]Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 78, in _aresults\n",
      "    r = await future\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 37, in sema_coro\n",
      "    return await coro\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 111, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 125, in ascore\n",
      "    raise e\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 121, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 167, in _ascore\n",
      "    return self._calculate_score(answers, row)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 139, in _calculate_score\n",
      "    cosine_sim = self.calculate_similarity(question, gen_questions)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 113, in calculate_similarity\n",
      "    question_vec = np.asarray(self.embeddings.embed_query(question)).reshape(1, -1)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 697, in embed_query\n",
      "    return self.embed_documents([text])[0]\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 668, in embed_documents\n",
      "    return self._get_len_safe_embeddings(texts, engine=engine)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 494, in _get_len_safe_embeddings\n",
      "    response = embed_with_retry(\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 116, in embed_with_retry\n",
      "    return embeddings.client.create(**kwargs)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\resources\\embeddings.py\", line 114, in create\n",
      "    return self._post(\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\_base_client.py\", line 1240, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\_base_client.py\", line 921, in request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\_base_client.py\", line 1020, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}\n",
      "Evaluating:  42%|████▎     | 17/40 [00:05<00:09,  2.47it/s]Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 78, in _aresults\n",
      "    r = await future\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 37, in sema_coro\n",
      "    return await coro\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 111, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 125, in ascore\n",
      "    raise e\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 121, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_similarity.py\", line 65, in _ascore\n",
      "    embedding_1 = np.array(await self.embeddings.embed_text(ground_truth))\n",
      "AttributeError: 'OpenAIEmbeddings' object has no attribute 'embed_text'. Did you mean: 'embed_query'?\n",
      "Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 78, in _aresults\n",
      "    r = await future\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 37, in sema_coro\n",
      "    return await coro\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 111, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 125, in ascore\n",
      "    raise e\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 121, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 167, in _ascore\n",
      "    return self._calculate_score(answers, row)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 139, in _calculate_score\n",
      "    cosine_sim = self.calculate_similarity(question, gen_questions)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 113, in calculate_similarity\n",
      "    question_vec = np.asarray(self.embeddings.embed_query(question)).reshape(1, -1)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 697, in embed_query\n",
      "    return self.embed_documents([text])[0]\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 668, in embed_documents\n",
      "    return self._get_len_safe_embeddings(texts, engine=engine)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 494, in _get_len_safe_embeddings\n",
      "    response = embed_with_retry(\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 116, in embed_with_retry\n",
      "    return embeddings.client.create(**kwargs)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\resources\\embeddings.py\", line 114, in create\n",
      "    return self._post(\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\_base_client.py\", line 1240, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\_base_client.py\", line 921, in request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\_base_client.py\", line 1020, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}\n",
      "Evaluating:  48%|████▊     | 19/40 [00:06<00:07,  2.93it/s]Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 78, in _aresults\n",
      "    r = await future\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 37, in sema_coro\n",
      "    return await coro\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 111, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 125, in ascore\n",
      "    raise e\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 121, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_similarity.py\", line 65, in _ascore\n",
      "    embedding_1 = np.array(await self.embeddings.embed_text(ground_truth))\n",
      "AttributeError: 'OpenAIEmbeddings' object has no attribute 'embed_text'. Did you mean: 'embed_query'?\n",
      "Evaluating:  52%|█████▎    | 21/40 [00:06<00:04,  4.21it/s]Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 78, in _aresults\n",
      "    r = await future\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 37, in sema_coro\n",
      "    return await coro\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 111, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 125, in ascore\n",
      "    raise e\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 121, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_correctness.py\", line 270, in _ascore\n",
      "    similarity_score = await self.answer_similarity.ascore(\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 125, in ascore\n",
      "    raise e\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 121, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_similarity.py\", line 65, in _ascore\n",
      "    embedding_1 = np.array(await self.embeddings.embed_text(ground_truth))\n",
      "AttributeError: 'OpenAIEmbeddings' object has no attribute 'embed_text'. Did you mean: 'embed_query'?\n",
      "Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 78, in _aresults\n",
      "    r = await future\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 37, in sema_coro\n",
      "    return await coro\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 111, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 125, in ascore\n",
      "    raise e\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 121, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_similarity.py\", line 65, in _ascore\n",
      "    embedding_1 = np.array(await self.embeddings.embed_text(ground_truth))\n",
      "AttributeError: 'OpenAIEmbeddings' object has no attribute 'embed_text'. Did you mean: 'embed_query'?\n",
      "Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 78, in _aresults\n",
      "    r = await future\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 37, in sema_coro\n",
      "    return await coro\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 111, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 125, in ascore\n",
      "    raise e\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 121, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 167, in _ascore\n",
      "    return self._calculate_score(answers, row)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 139, in _calculate_score\n",
      "    cosine_sim = self.calculate_similarity(question, gen_questions)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 113, in calculate_similarity\n",
      "    question_vec = np.asarray(self.embeddings.embed_query(question)).reshape(1, -1)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 697, in embed_query\n",
      "    return self.embed_documents([text])[0]\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 668, in embed_documents\n",
      "    return self._get_len_safe_embeddings(texts, engine=engine)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 494, in _get_len_safe_embeddings\n",
      "    response = embed_with_retry(\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 116, in embed_with_retry\n",
      "    return embeddings.client.create(**kwargs)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\resources\\embeddings.py\", line 114, in create\n",
      "    return self._post(\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\_base_client.py\", line 1240, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\_base_client.py\", line 921, in request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\_base_client.py\", line 1020, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}\n",
      "Evaluating:  65%|██████▌   | 26/40 [00:08<00:04,  2.81it/s]Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 78, in _aresults\n",
      "    r = await future\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 37, in sema_coro\n",
      "    return await coro\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 111, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 125, in ascore\n",
      "    raise e\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 121, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_correctness.py\", line 270, in _ascore\n",
      "    similarity_score = await self.answer_similarity.ascore(\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 125, in ascore\n",
      "    raise e\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 121, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_similarity.py\", line 65, in _ascore\n",
      "    embedding_1 = np.array(await self.embeddings.embed_text(ground_truth))\n",
      "AttributeError: 'OpenAIEmbeddings' object has no attribute 'embed_text'. Did you mean: 'embed_query'?\n",
      "Evaluating:  72%|███████▎  | 29/40 [00:09<00:03,  2.97it/s]Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 78, in _aresults\n",
      "    r = await future\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 37, in sema_coro\n",
      "    return await coro\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 111, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 125, in ascore\n",
      "    raise e\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 121, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_correctness.py\", line 270, in _ascore\n",
      "    similarity_score = await self.answer_similarity.ascore(\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 125, in ascore\n",
      "    raise e\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 121, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_similarity.py\", line 65, in _ascore\n",
      "    embedding_1 = np.array(await self.embeddings.embed_text(ground_truth))\n",
      "AttributeError: 'OpenAIEmbeddings' object has no attribute 'embed_text'. Did you mean: 'embed_query'?\n",
      "Evaluating:  78%|███████▊  | 31/40 [00:09<00:02,  4.46it/s]Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 78, in _aresults\n",
      "    r = await future\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 37, in sema_coro\n",
      "    return await coro\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 111, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 125, in ascore\n",
      "    raise e\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 121, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_correctness.py\", line 270, in _ascore\n",
      "    similarity_score = await self.answer_similarity.ascore(\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 125, in ascore\n",
      "    raise e\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 121, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_similarity.py\", line 65, in _ascore\n",
      "    embedding_1 = np.array(await self.embeddings.embed_text(ground_truth))\n",
      "AttributeError: 'OpenAIEmbeddings' object has no attribute 'embed_text'. Did you mean: 'embed_query'?\n",
      "Evaluating:  80%|████████  | 32/40 [00:09<00:01,  4.39it/s]Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 78, in _aresults\n",
      "    r = await future\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 37, in sema_coro\n",
      "    return await coro\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 111, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 125, in ascore\n",
      "    raise e\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 121, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_correctness.py\", line 270, in _ascore\n",
      "    similarity_score = await self.answer_similarity.ascore(\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 125, in ascore\n",
      "    raise e\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 121, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_similarity.py\", line 65, in _ascore\n",
      "    embedding_1 = np.array(await self.embeddings.embed_text(ground_truth))\n",
      "AttributeError: 'OpenAIEmbeddings' object has no attribute 'embed_text'. Did you mean: 'embed_query'?\n",
      "Evaluating:  82%|████████▎ | 33/40 [00:10<00:03,  2.27it/s]Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 78, in _aresults\n",
      "    r = await future\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 37, in sema_coro\n",
      "    return await coro\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 111, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 125, in ascore\n",
      "    raise e\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 121, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_correctness.py\", line 270, in _ascore\n",
      "    similarity_score = await self.answer_similarity.ascore(\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 125, in ascore\n",
      "    raise e\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 121, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_similarity.py\", line 65, in _ascore\n",
      "    embedding_1 = np.array(await self.embeddings.embed_text(ground_truth))\n",
      "AttributeError: 'OpenAIEmbeddings' object has no attribute 'embed_text'. Did you mean: 'embed_query'?\n",
      "Evaluating:  85%|████████▌ | 34/40 [00:10<00:02,  2.79it/s]Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 78, in _aresults\n",
      "    r = await future\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 37, in sema_coro\n",
      "    return await coro\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 111, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 125, in ascore\n",
      "    raise e\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 121, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_correctness.py\", line 270, in _ascore\n",
      "    similarity_score = await self.answer_similarity.ascore(\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 125, in ascore\n",
      "    raise e\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 121, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_similarity.py\", line 65, in _ascore\n",
      "    embedding_1 = np.array(await self.embeddings.embed_text(ground_truth))\n",
      "AttributeError: 'OpenAIEmbeddings' object has no attribute 'embed_text'. Did you mean: 'embed_query'?\n",
      "Evaluating:  88%|████████▊ | 35/40 [00:11<00:01,  2.76it/s]Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 78, in _aresults\n",
      "    r = await future\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 37, in sema_coro\n",
      "    return await coro\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 111, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 125, in ascore\n",
      "    raise e\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 121, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_correctness.py\", line 270, in _ascore\n",
      "    similarity_score = await self.answer_similarity.ascore(\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 125, in ascore\n",
      "    raise e\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 121, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_similarity.py\", line 65, in _ascore\n",
      "    embedding_1 = np.array(await self.embeddings.embed_text(ground_truth))\n",
      "AttributeError: 'OpenAIEmbeddings' object has no attribute 'embed_text'. Did you mean: 'embed_query'?\n",
      "Evaluating:  90%|█████████ | 36/40 [00:11<00:01,  3.25it/s]Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 78, in _aresults\n",
      "    r = await future\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 37, in sema_coro\n",
      "    return await coro\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 111, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 125, in ascore\n",
      "    raise e\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 121, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_correctness.py\", line 270, in _ascore\n",
      "    similarity_score = await self.answer_similarity.ascore(\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 125, in ascore\n",
      "    raise e\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 121, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_similarity.py\", line 65, in _ascore\n",
      "    embedding_1 = np.array(await self.embeddings.embed_text(ground_truth))\n",
      "AttributeError: 'OpenAIEmbeddings' object has no attribute 'embed_text'. Did you mean: 'embed_query'?\n",
      "Evaluating:  95%|█████████▌| 38/40 [00:12<00:01,  1.84it/s]Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 78, in _aresults\n",
      "    r = await future\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 37, in sema_coro\n",
      "    return await coro\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 111, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 125, in ascore\n",
      "    raise e\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 121, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_correctness.py\", line 270, in _ascore\n",
      "    similarity_score = await self.answer_similarity.ascore(\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 125, in ascore\n",
      "    raise e\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 121, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_similarity.py\", line 65, in _ascore\n",
      "    embedding_1 = np.array(await self.embeddings.embed_text(ground_truth))\n",
      "AttributeError: 'OpenAIEmbeddings' object has no attribute 'embed_text'. Did you mean: 'embed_query'?\n",
      "Evaluating: 100%|██████████| 40/40 [00:15<00:00,  2.58it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'faithfulness': 0.8750, 'answer_relevancy': 1.0000, 'answer_similarity': 0.5000, 'answer_correctness': 1.0000}"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_answers = evaluate(\n",
    "       dataset=dataset,\n",
    "       metrics=[faithfulness, answer_relevancy, answer_similarity, answer_correctness],\n",
    "       llm=ragas_azure_model,\n",
    "       raise_exceptions=False,\n",
    ")\n",
    "\n",
    "evaluate_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>context_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Qu'est-ce que le service Azure OpenAI ?</td>\n",
       "      <td>Le service Azure OpenAI est un outil puissant ...</td>\n",
       "      <td>[Le service Azure OpenAI : un outil puissant \\...</td>\n",
       "      <td>[\"Le service Azure OpenAI est un service cloud...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Comment fonctionne le service Azure OpenAI ?</td>\n",
       "      <td>Le service Azure OpenAI repose sur deux compos...</td>\n",
       "      <td>[Comment fonctionne le service Azure OpenAI ? ...</td>\n",
       "      <td>['Le service Azure OpenAI repose sur deux comp...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Quels sont les avantages du service Azure Open...</td>\n",
       "      <td>Le texte mentionne qu'il y a plusieurs avantag...</td>\n",
       "      <td>[Quels sont les avantages du service Azure Ope...</td>\n",
       "      <td>['Le service Azure OpenAI présente plusieurs a...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.876667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Quels sont les cas d'utilisation du service Az...</td>\n",
       "      <td>Le document ne fournit pas d'informations spéc...</td>\n",
       "      <td>[Quels sont les avantages du service Azure Ope...</td>\n",
       "      <td>[\"Le service Azure OpenAI peut être utilisé po...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.710000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Comment mettre en œuvre le service Azure OpenAI ?</td>\n",
       "      <td>Pour mettre en œuvre le service Azure OpenAI, ...</td>\n",
       "      <td>[Comment mettre en œuvre le service Azure Open...</td>\n",
       "      <td>['Pour mettre en œuvre le service Azure OpenAI...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Quels sont les composants principaux du servic...</td>\n",
       "      <td>Le service Azure OpenAI repose sur deux compos...</td>\n",
       "      <td>[Comment fonctionne le service Azure OpenAI ? ...</td>\n",
       "      <td>['Le service Azure OpenAI repose sur deux comp...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Quels sont les modèles d'IA accessibles via le...</td>\n",
       "      <td>Unfortunately, I do not have enough informatio...</td>\n",
       "      <td>[modèle, en utilisant les outils de formation ...</td>\n",
       "      <td>[\"Les API Azure OpenAI sont des interfaces de ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Quelles tâches les modèles d'IA du service Azu...</td>\n",
       "      <td>Je suis désolé, mais les informations fournies...</td>\n",
       "      <td>[modèle, en utilisant les outils de formation ...</td>\n",
       "      <td>[\"Il permet d'accéder à des modèles d'IA de po...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Quelle est l'infrastructure sur laquelle repos...</td>\n",
       "      <td>Le texte ne donne pas d'informations sur l'inf...</td>\n",
       "      <td>[Comment fonctionne le service Azure OpenAI ? ...</td>\n",
       "      <td>[\"Il garantit une haute performance, en s'appu...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Quels outils le service Azure OpenAI fournit-i...</td>\n",
       "      <td>Le service Azure OpenAI fournit des outils de ...</td>\n",
       "      <td>[modèle, en utilisant les outils de formation ...</td>\n",
       "      <td>[\"Choisir un modèle d'IA, parmi ceux proposés ...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0            Qu'est-ce que le service Azure OpenAI ?   \n",
       "1       Comment fonctionne le service Azure OpenAI ?   \n",
       "2  Quels sont les avantages du service Azure Open...   \n",
       "3  Quels sont les cas d'utilisation du service Az...   \n",
       "4  Comment mettre en œuvre le service Azure OpenAI ?   \n",
       "5  Quels sont les composants principaux du servic...   \n",
       "6  Quels sont les modèles d'IA accessibles via le...   \n",
       "7  Quelles tâches les modèles d'IA du service Azu...   \n",
       "8  Quelle est l'infrastructure sur laquelle repos...   \n",
       "9  Quels outils le service Azure OpenAI fournit-i...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  Le service Azure OpenAI est un outil puissant ...   \n",
       "1  Le service Azure OpenAI repose sur deux compos...   \n",
       "2  Le texte mentionne qu'il y a plusieurs avantag...   \n",
       "3  Le document ne fournit pas d'informations spéc...   \n",
       "4  Pour mettre en œuvre le service Azure OpenAI, ...   \n",
       "5  Le service Azure OpenAI repose sur deux compos...   \n",
       "6  Unfortunately, I do not have enough informatio...   \n",
       "7  Je suis désolé, mais les informations fournies...   \n",
       "8  Le texte ne donne pas d'informations sur l'inf...   \n",
       "9  Le service Azure OpenAI fournit des outils de ...   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [Le service Azure OpenAI : un outil puissant \\...   \n",
       "1  [Comment fonctionne le service Azure OpenAI ? ...   \n",
       "2  [Quels sont les avantages du service Azure Ope...   \n",
       "3  [Quels sont les avantages du service Azure Ope...   \n",
       "4  [Comment mettre en œuvre le service Azure Open...   \n",
       "5  [Comment fonctionne le service Azure OpenAI ? ...   \n",
       "6  [modèle, en utilisant les outils de formation ...   \n",
       "7  [modèle, en utilisant les outils de formation ...   \n",
       "8  [Comment fonctionne le service Azure OpenAI ? ...   \n",
       "9  [modèle, en utilisant les outils de formation ...   \n",
       "\n",
       "                                        ground_truth  context_recall  \\\n",
       "0  [\"Le service Azure OpenAI est un service cloud...             1.0   \n",
       "1  ['Le service Azure OpenAI repose sur deux comp...             1.0   \n",
       "2  ['Le service Azure OpenAI présente plusieurs a...             1.0   \n",
       "3  [\"Le service Azure OpenAI peut être utilisé po...             1.0   \n",
       "4  ['Pour mettre en œuvre le service Azure OpenAI...             0.5   \n",
       "5  ['Le service Azure OpenAI repose sur deux comp...             1.0   \n",
       "6  [\"Les API Azure OpenAI sont des interfaces de ...             1.0   \n",
       "7  [\"Il permet d'accéder à des modèles d'IA de po...             1.0   \n",
       "8  [\"Il garantit une haute performance, en s'appu...             1.0   \n",
       "9  [\"Choisir un modèle d'IA, parmi ceux proposés ...             0.5   \n",
       "\n",
       "   context_precision  \n",
       "0           1.000000  \n",
       "1           0.950000  \n",
       "2           0.876667  \n",
       "3           0.710000  \n",
       "4           0.450000  \n",
       "5           1.000000  \n",
       "6           0.966667  \n",
       "7           0.966667  \n",
       "8           1.000000  \n",
       "9           1.000000  "
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_answers_df = evaluate_contexts.to_pandas()\n",
    "evaluate_answers_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 50/50 [00:04<00:00, 10.74it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'harmfulness': 0.0000, 'maliciousness': 0.0000, 'coherence': 0.8000, 'correctness': 0.7000, 'conciseness': 0.8000}"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_aspect_critique = evaluate(\n",
    "       dataset=dataset,\n",
    "       metrics=[harmfulness, maliciousness, coherence, correctness, conciseness],\n",
    "       llm=ragas_azure_model,\n",
    "       raise_exceptions=False,\n",
    ")\n",
    "\n",
    "evaluate_aspect_critique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>harmfulness</th>\n",
       "      <th>maliciousness</th>\n",
       "      <th>coherence</th>\n",
       "      <th>correctness</th>\n",
       "      <th>conciseness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Qu'est-ce que le service Azure OpenAI ?</td>\n",
       "      <td>Le service Azure OpenAI est un outil puissant ...</td>\n",
       "      <td>[Le service Azure OpenAI : un outil puissant \\...</td>\n",
       "      <td>[\"Le service Azure OpenAI est un service cloud...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Comment fonctionne le service Azure OpenAI ?</td>\n",
       "      <td>Le service Azure OpenAI repose sur deux compos...</td>\n",
       "      <td>[Comment fonctionne le service Azure OpenAI ? ...</td>\n",
       "      <td>['Le service Azure OpenAI repose sur deux comp...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Quels sont les avantages du service Azure Open...</td>\n",
       "      <td>Le texte mentionne qu'il y a plusieurs avantag...</td>\n",
       "      <td>[Quels sont les avantages du service Azure Ope...</td>\n",
       "      <td>['Le service Azure OpenAI présente plusieurs a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Quels sont les cas d'utilisation du service Az...</td>\n",
       "      <td>Le document ne fournit pas d'informations spéc...</td>\n",
       "      <td>[Quels sont les avantages du service Azure Ope...</td>\n",
       "      <td>[\"Le service Azure OpenAI peut être utilisé po...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Comment mettre en œuvre le service Azure OpenAI ?</td>\n",
       "      <td>Pour mettre en œuvre le service Azure OpenAI, ...</td>\n",
       "      <td>[Comment mettre en œuvre le service Azure Open...</td>\n",
       "      <td>['Pour mettre en œuvre le service Azure OpenAI...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Quels sont les composants principaux du servic...</td>\n",
       "      <td>Le service Azure OpenAI repose sur deux compos...</td>\n",
       "      <td>[Comment fonctionne le service Azure OpenAI ? ...</td>\n",
       "      <td>['Le service Azure OpenAI repose sur deux comp...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Quels sont les modèles d'IA accessibles via le...</td>\n",
       "      <td>Unfortunately, I do not have enough informatio...</td>\n",
       "      <td>[modèle, en utilisant les outils de formation ...</td>\n",
       "      <td>[\"Les API Azure OpenAI sont des interfaces de ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Quelles tâches les modèles d'IA du service Azu...</td>\n",
       "      <td>Je suis désolé, mais les informations fournies...</td>\n",
       "      <td>[modèle, en utilisant les outils de formation ...</td>\n",
       "      <td>[\"Il permet d'accéder à des modèles d'IA de po...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Quelle est l'infrastructure sur laquelle repos...</td>\n",
       "      <td>Le texte ne donne pas d'informations sur l'inf...</td>\n",
       "      <td>[Comment fonctionne le service Azure OpenAI ? ...</td>\n",
       "      <td>[\"Il garantit une haute performance, en s'appu...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Quels outils le service Azure OpenAI fournit-i...</td>\n",
       "      <td>Le service Azure OpenAI fournit des outils de ...</td>\n",
       "      <td>[modèle, en utilisant les outils de formation ...</td>\n",
       "      <td>[\"Choisir un modèle d'IA, parmi ceux proposés ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0            Qu'est-ce que le service Azure OpenAI ?   \n",
       "1       Comment fonctionne le service Azure OpenAI ?   \n",
       "2  Quels sont les avantages du service Azure Open...   \n",
       "3  Quels sont les cas d'utilisation du service Az...   \n",
       "4  Comment mettre en œuvre le service Azure OpenAI ?   \n",
       "5  Quels sont les composants principaux du servic...   \n",
       "6  Quels sont les modèles d'IA accessibles via le...   \n",
       "7  Quelles tâches les modèles d'IA du service Azu...   \n",
       "8  Quelle est l'infrastructure sur laquelle repos...   \n",
       "9  Quels outils le service Azure OpenAI fournit-i...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  Le service Azure OpenAI est un outil puissant ...   \n",
       "1  Le service Azure OpenAI repose sur deux compos...   \n",
       "2  Le texte mentionne qu'il y a plusieurs avantag...   \n",
       "3  Le document ne fournit pas d'informations spéc...   \n",
       "4  Pour mettre en œuvre le service Azure OpenAI, ...   \n",
       "5  Le service Azure OpenAI repose sur deux compos...   \n",
       "6  Unfortunately, I do not have enough informatio...   \n",
       "7  Je suis désolé, mais les informations fournies...   \n",
       "8  Le texte ne donne pas d'informations sur l'inf...   \n",
       "9  Le service Azure OpenAI fournit des outils de ...   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [Le service Azure OpenAI : un outil puissant \\...   \n",
       "1  [Comment fonctionne le service Azure OpenAI ? ...   \n",
       "2  [Quels sont les avantages du service Azure Ope...   \n",
       "3  [Quels sont les avantages du service Azure Ope...   \n",
       "4  [Comment mettre en œuvre le service Azure Open...   \n",
       "5  [Comment fonctionne le service Azure OpenAI ? ...   \n",
       "6  [modèle, en utilisant les outils de formation ...   \n",
       "7  [modèle, en utilisant les outils de formation ...   \n",
       "8  [Comment fonctionne le service Azure OpenAI ? ...   \n",
       "9  [modèle, en utilisant les outils de formation ...   \n",
       "\n",
       "                                        ground_truth  harmfulness  \\\n",
       "0  [\"Le service Azure OpenAI est un service cloud...            0   \n",
       "1  ['Le service Azure OpenAI repose sur deux comp...            0   \n",
       "2  ['Le service Azure OpenAI présente plusieurs a...            0   \n",
       "3  [\"Le service Azure OpenAI peut être utilisé po...            0   \n",
       "4  ['Pour mettre en œuvre le service Azure OpenAI...            0   \n",
       "5  ['Le service Azure OpenAI repose sur deux comp...            0   \n",
       "6  [\"Les API Azure OpenAI sont des interfaces de ...            0   \n",
       "7  [\"Il permet d'accéder à des modèles d'IA de po...            0   \n",
       "8  [\"Il garantit une haute performance, en s'appu...            0   \n",
       "9  [\"Choisir un modèle d'IA, parmi ceux proposés ...            0   \n",
       "\n",
       "   maliciousness  coherence  correctness  conciseness  \n",
       "0              0          1            1            1  \n",
       "1              0          1            1            1  \n",
       "2              0          1            0            0  \n",
       "3              0          1            1            1  \n",
       "4              0          1            1            1  \n",
       "5              0          1            1            1  \n",
       "6              0          1            1            1  \n",
       "7              0          0            0            0  \n",
       "8              0          0            0            1  \n",
       "9              0          1            1            1  "
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_aspect_critique_df = evaluate_aspect_critique.to_pandas()\n",
    "evaluate_aspect_critique_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install evaluate bert_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, the best model is microsoft/deberta-xlarge-mnli, please consider using it instead of the default roberta-large in order to have the best correlation with human evaluation.\n",
    "\n",
    "https://docs.google.com/spreadsheets/d/1RKOVpselB98Nnh_EOC4A2BYn8_201tmPODpNWu4w7xI/edit#gid=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\paul.peton\\.cache\\huggingface\\hub\\models--roberta-large. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'precision': [1.0, 1.0000001192092896],\n",
       " 'recall': [1.0, 1.0000001192092896],\n",
       " 'f1': [1.0, 1.0000001192092896],\n",
       " 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.1)'}"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from evaluate import load\n",
    "\n",
    "bertscore = load(\"bertscore\")\n",
    "predictions = [\"hello there\", \"general kenobi\"]\n",
    "references = [\"hello there\", \"general kenobi\"]\n",
    "results = bertscore.compute(predictions=predictions, references=references, lang=\"en\")\n",
    "# default model lang=en: roberta-large\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "precision: The precision for each sentence from the predictions + references lists, which ranges from 0.0 to 1.0.\n",
    "\n",
    "recall: The recall for each sentence from the predictions + references lists, which ranges from 0.0 to 1.0.\n",
    "\n",
    "f1: The F1 score for each sentence from the predictions + references lists, which ranges from 0.0 to 1.0.\n",
    "\n",
    "hashcode: The hashcode of the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajuster les options pandas pour afficher les textes complets\n",
    "pd.set_option('display.max_colwidth', None)  # Désactive la troncation des colonnes\n",
    "pd.set_option('display.max_rows', None)      # Désactive la troncation des lignes\n",
    "pd.set_option('display.max_columns', None)   # Affiche toutes les colonnes\n",
    "pd.set_option('display.width', None)         # Ajuste automatiquement la largeur d'affichage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': [0.6400765180587769,\n",
       "  0.6021711826324463,\n",
       "  0.5712481141090393,\n",
       "  0.8315932154655457,\n",
       "  0.5764471888542175,\n",
       "  0.8881711959838867,\n",
       "  0.5767934322357178,\n",
       "  0.6516508460044861,\n",
       "  0.6587282419204712,\n",
       "  0.6696093678474426],\n",
       " 'recall': [0.8885854482650757,\n",
       "  0.8658581972122192,\n",
       "  0.8463621735572815,\n",
       "  0.9536949992179871,\n",
       "  0.7484399676322937,\n",
       "  0.8556890487670898,\n",
       "  0.6140142679214478,\n",
       "  0.7516512274742126,\n",
       "  0.6185110807418823,\n",
       "  0.7025548219680786],\n",
       " 'f1': [0.7441313862800598,\n",
       "  0.710332989692688,\n",
       "  0.6821095943450928,\n",
       "  0.8884686231613159,\n",
       "  0.6512798070907593,\n",
       "  0.8716276288032532,\n",
       "  0.594822108745575,\n",
       "  0.6980879902839661,\n",
       "  0.6379864811897278,\n",
       "  0.6856865286827087],\n",
       " 'hashcode': 'microsoft/deberta-xlarge-mnli_L40_no-idf_version=0.3.12(hug_trans=4.41.1)'}"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = answers\n",
    "references = [sublist[0] for sublist in ground_truths]\n",
    "results = bertscore.compute(predictions=predictions, references=references, lang=\"fr\", model_type=\"microsoft/deberta-xlarge-mnli\")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                                                                                                               predictions  \\\n",
      "2  Le service Azure OpenAI présente plusieurs avantages, parmi lesquels l'accès à des modèles d'IA de pointe, une grande flexibilité, une haute performance grâce à l'infrastructure cloud de Microsoft Azure, et une facilité d'intégration grâce à des API simples et des SDK dans différents langages de programmation.   \n",
      "4                  Pour mettre en œuvre le service Azure OpenAI, il faut créer un compte Microsoft Azure, souscrire à un abonnement au service, créer une clé d'API, choisir ou créer un modèle d'IA, créer un point de terminaison, envoyer des requêtes au point de terminaison, et analyser les résultats des requêtes.   \n",
      "6                                                                                                                                                                                                                          Les modèles d'IA accessibles via le service Azure OpenAI incluent GPT-3, Codex, DALL-E et CLIP.   \n",
      "\n",
      "                                                                                                                           references  \\\n",
      "2                                                              Le service Azure OpenAI présente plusieurs avantages, parmi lesquels :   \n",
      "4                                                 Pour mettre en œuvre le service Azure OpenAI, il faut suivre les étapes suivantes :   \n",
      "6  Les API Azure OpenAI sont des interfaces de programmation qui permettent d'interagir avec les modèles d'IA du service Azure OpenAI   \n",
      "\n",
      "   precision    recall        f1  \n",
      "2   0.571248  0.846362  0.682110  \n",
      "4   0.576447  0.748440  0.651280  \n",
      "6   0.576793  0.614014  0.594822  \n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    'predictions': predictions,\n",
    "    'references': references,\n",
    "    'precision': results['precision'],\n",
    "    'recall': results['recall'],\n",
    "    'f1': results['f1']\n",
    "}\n",
    "\n",
    "# Créer le DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Afficher le DataFrame\n",
    "print(df.sort_values(by='precision', ascending=True).head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictions    Pour mettre en œuvre le service Azure OpenAI, il faut créer un compte Microsoft Azure, souscrire à un abonnement au service, créer une clé d'API, choisir ou créer un modèle d'IA, créer un point de terminaison, envoyer des requêtes au point de terminaison, et analyser les résultats des requêtes.\n",
       "references                                                                                        [Pour mettre en œuvre le service Azure OpenAI, il faut suivre les étapes suivantes :, Analyser les résultats des requêtes, en utilisant les outils de suivi et de statistiques fournis par le portail Azure OpenAI.]\n",
       "precision                                                                                                                                                                                                                                                                                                     0.577569\n",
       "recall                                                                                                                                                                                                                                                                                                         0.74844\n",
       "f1                                                                                                                                                                                                                                                                                                             0.65128\n",
       "Name: 4, dtype: object"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                        predictions  \\\n",
      "5                                                                                           Les composants principaux du service Azure OpenAI sont le portail Azure OpenAI et les API Azure OpenAI.   \n",
      "3  Le service Azure OpenAI peut être utilisé pour de nombreux cas d'utilisation, dans différents domaines et secteurs d'activité, tels que l'éducation, la santé, le divertissement et le commerce.   \n",
      "9                                                                                          Le service Azure OpenAI fournit des outils de formation et de réglage pour créer son propre modèle d'IA.   \n",
      "\n",
      "                                                                                                                       references  \\\n",
      "5                Le service Azure OpenAI repose sur deux composants principaux : le portail Azure OpenAI et les API Azure OpenAI.   \n",
      "3  Le service Azure OpenAI peut être utilisé pour de nombreux cas d'utilisation, dans différents domaines et secteurs d'activité.   \n",
      "9                                                         Choisir un modèle d'IA, parmi ceux proposés par le service Azure OpenAI   \n",
      "\n",
      "   precision    recall        f1  \n",
      "5   0.888171  0.855689  0.871628  \n",
      "3   0.831593  0.953695  0.888469  \n",
      "9   0.669609  0.702555  0.685687  \n"
     ]
    }
   ],
   "source": [
    "print(df.sort_values(by='precision', ascending=False).head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictions             Les composants principaux du service Azure OpenAI sont le portail Azure OpenAI et les API Azure OpenAI.\n",
       "references     Le service Azure OpenAI repose sur deux composants principaux : le portail Azure OpenAI et les API Azure OpenAI.\n",
       "precision                                                                                                              0.888171\n",
       "recall                                                                                                                 0.855689\n",
       "f1                                                                                                                     0.871628\n",
       "Name: 5, dtype: object"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
