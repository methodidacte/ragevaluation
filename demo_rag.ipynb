{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain.llms import AzureOpenAI\n",
    "# from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.chat_models import openai\n",
    "from openai import AzureOpenAI\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "import requests\n",
    "import os\n",
    "from datetime import datetime\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_KEY\"] = \"***\"\n",
    "os.environ[\"AZURE_OPENAI_KEY\"] = \"***\"\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"***\"\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://lab-ai-openai-eus.openai.azure.com/\"\n",
    "os.environ[\"AZURE_OPENAI_API_VERSION\"] = \"2024-02-01\"\n",
    "os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"] = \"gpt-35-turbo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-9WKPLMNDuVbY3gXxqNCR05lwg44Sx', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Hello! I am an AI assistant and I'm here to help you find the information you need. Please let me know what you're looking for and I'll do my best to assist you.\", role='assistant', function_call=None, tool_calls=None), content_filter_results={'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}})], created=1717491971, model='gpt-35-turbo', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=39, prompt_tokens=19, total_tokens=58), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}])\n"
     ]
    }
   ],
   "source": [
    "client = AzureOpenAI(\n",
    "  azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"), \n",
    "  api_key=os.getenv(\"AZURE_OPENAI_KEY\"),  \n",
    "  api_version=\"2024-02-01\" #2024-02-15-preview ?\n",
    ")\n",
    "\n",
    "message_text = [{\"role\":\"system\",\"content\":\"You are an AI assistant that helps people find information.\"}]\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-35-turbo\", # model = \"deployment_name\"\n",
    "  messages = message_text,\n",
    "  temperature=0.7,\n",
    "  max_tokens=800,\n",
    "  top_p=0.95,\n",
    "  frequency_penalty=0,\n",
    "  presence_penalty=0,\n",
    "  stop=None\n",
    ")\n",
    "\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"J'aime programmer.\", response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 20, 'total_tokens': 25}, 'model_name': 'gpt-35-turbo', 'system_fingerprint': None, 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run-a696ddf5-dd63-47c4-b5e5-4ab7de86b4ed-0', usage_metadata={'input_tokens': 20, 'output_tokens': 5, 'total_tokens': 25})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = AzureChatOpenAI(\n",
    "    openai_api_version=\"2024-02-01\",\n",
    "    azure_deployment=\"gpt-35-turbo\",\n",
    ")\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "message = HumanMessage(\n",
    "    content=\"Translate this sentence from English to French. I love programming.\"\n",
    ")\n",
    "llm.invoke([message])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install unstructured\n",
    "# !pip install \"unstructured[pdf]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"Le service Azure OpenAI : un outil puissant pour l'intelligence artificielle\\n\\nUn document qui présente le service Azure OpenAI, ses caractéristiques, ses bénéfices et ses applications.\\n\\nQu'est-ce que le service Azure OpenAI ? Le service Azure OpenAI est un service cloud qui permet aux développeurs et aux chercheurs d'accéder à la plateforme OpenAI, une initiative qui vise à créer une intelligence artificielle générale (AGI) bénéfique pour l'humanité. Le service Azure OpenAI offre la possibilité d'utiliser des modèles d'IA pré-entraînés ou personnalisés, de les déployer sur le cloud ou en périphérie, et de les intégrer à des applications et des scénarios variés.\\n\\nComment fonctionne le service Azure OpenAI ? Le service Azure OpenAI repose sur deux composants principaux : le portail Azure OpenAI et les API Azure OpenAI. Le portail Azure OpenAI est une interface web qui permet de créer, de gérer et de surveiller les ressources du service Azure OpenAI, telles que les abonnements, les clés, les modèles, les points de terminaison et les requêtes. Les API Azure OpenAI sont des interfaces de programmation qui permettent d'interagir avec les modèles d'IA du service Azure OpenAI, tels que GPT-3, Codex, DALL-E ou CLIP. Les API Azure OpenAI sont accessibles via des requêtes HTTP ou des SDK dans différents langages de programmation, tels que Python, Java, C#, Node.js ou Ruby.\\n\\nQuels sont les avantages du service Azure OpenAI ? Le service Azure OpenAI présente plusieurs avantages, parmi lesquels :\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIl permet d'accéder à des modèles d'IA de pointe, capables de réaliser des tâches complexes et variées, telles que la génération de texte, la compréhension du langage naturel, la vision par ordinateur, la synthèse vocale ou la création de contenu multimédia. Il offre une grande flexibilité, en permettant de choisir entre des modèles pré-entraînés ou personnalisés, et de les adapter aux besoins spécifiques de chaque projet ou domaine. Il garantit une haute performance, en s'appuyant sur l'infrastructure cloud de Microsoft Azure, qui assure une disponibilité, une scalabilité et une sécurité optimales. Il facilite l'intégration, en proposant des API simples et des SDK dans différents langages de programmation, qui permettent de connecter les modèles d'IA du service Azure OpenAI à des applications et des scénarios existants ou nouveaux.\\n\\nQuels sont les cas d'utilisation du service Azure OpenAI ? Le service Azure OpenAI peut être utilisé pour de nombreux cas d'utilisation, dans différents domaines et secteurs d'activité. Voici quelques exemples :\\n\\nDans le domaine de l'éducation, le service Azure OpenAI peut être utilisé pour créer des outils\\n\\npédagogiques, tels que des assistants de rédaction, des générateurs de questions, des correcteurs automatiques ou des tuteurs virtuels.\\n\\nDans le domaine de la santé, le service Azure OpenAI peut être utilisé pour créer des outils d'aide au diagnostic, à la prescription, à la recherche ou à la prévention, en exploitant les données médicales et les connaissances scientifiques.\\n\\nDans le domaine du divertissement, le service Azure OpenAI peut être utilisé pour créer des outils de génération de contenu, tels que des scénarios, des dialogues, des personnages, des musiques ou des images.\\n\\nDans le domaine du commerce, le service Azure OpenAI peut être utilisé pour créer des outils\\n\\nd'optimisation, tels que des recommandations personnalisées, des prévisions de ventes, des analyses de marché ou des chatbots.\\n\\nComment mettre en œuvre le service Azure OpenAI ? Pour mettre en œuvre le service Azure OpenAI, il faut suivre les étapes suivantes :\\n\\nCréer un compte Microsoft Azure, si ce n'est pas déjà fait, et se connecter au portail Azure OpenAI.\\n\\nSouscrire à un abonnement au service Azure OpenAI, en choisissant le niveau de tarification\\n\\nadapté à ses besoins.\", metadata={'source': 'microsoft_learn\\\\Le service Azure OpenAI.pdf'}),\n",
       " Document(page_content=\"Souscrire à un abonnement au service Azure OpenAI, en choisissant le niveau de tarification\\n\\nadapté à ses besoins.\\n\\nCréer une clé d'API, qui permettra d'authentifier les requêtes au service Azure OpenAI. • Choisir un modèle d'IA, parmi ceux proposés par le service Azure OpenAI, ou créer son propre modèle, en utilisant les outils de formation et de réglage fournis par le service Azure OpenAI.\\n\\nCréer un point de terminaison, qui permettra d'exposer le modèle d'IA sur le cloud ou en\\n\\npériphérie, et de le rendre accessible via une URL.\\n\\nEnvoyer des requêtes au point de terminaison, en utilisant les API Azure OpenAI ou les SDK dans le langage de programmation de son choix.\\n\\nAnalyser les résultats des requêtes, en utilisant les outils de suivi et de statistiques fournis par le\\n\\nportail Azure OpenAI.\", metadata={'source': 'microsoft_learn\\\\Le service Azure OpenAI.pdf'})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# upload pdf \n",
    "\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "directory_path = \"microsoft_learn\"\n",
    "\n",
    "loader = DirectoryLoader(directory_path)\n",
    "pages = loader.load_and_split()\n",
    "pages\n",
    "\n",
    "# partition_pdf is not available. Install the pdf dependencies with pip install \"unstructured[pdf]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAG : recherche par similarité (en utilisant les embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract text from pdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pdfminer.six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF path file: microsoft_learn\\Le service Azure OpenAI.pdf\n"
     ]
    }
   ],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "dirs = os.listdir(directory_path)\n",
    "output = \"pdftotext.txt\"\n",
    "# supprimer le fichier s'il existe déjà\n",
    "if os.path.exists(output):  \n",
    "    os.remove(output) \n",
    "for file in dirs:\n",
    "    path_file = os.path.join(directory_path, file)\n",
    "    print(f\"PDF path file: {path_file}\")\n",
    "    text = extract_text(path_file)\n",
    "    with open(output, 'a', encoding='utf-8') as fichier:\n",
    "        fichier.write(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader, DirectoryLoader\n",
    "\n",
    "loader = TextLoader(\"pdftotext.txt\", encoding='utf-8')\n",
    "documents= loader.load()\n",
    "len(documents)\n",
    "\n",
    "# 'charmap' codec can't decode byte 0x9d in position 3802475: character maps to <undefined>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"Le service Azure OpenAI : un outil puissant \\npour l'intelligence artificielle \\n\\nUn document qui présente le service Azure OpenAI, ses caractéristiques, ses bénéfices et \\nses applications. \\n\\nQu'est-ce que le service Azure OpenAI ? \\nLe service Azure OpenAI est un service cloud qui permet aux développeurs et aux chercheurs d'accéder à \\nla plateforme OpenAI, une initiative qui vise à créer une intelligence artificielle générale (AGI) bénéfique \\npour l'humanité. Le service Azure OpenAI offre la possibilité d'utiliser des modèles d'IA pré-entraînés ou \\npersonnalisés, de les déployer sur le cloud ou en périphérie, et de les intégrer à des applications et des \\nscénarios variés. \\n\\nComment fonctionne le service Azure OpenAI ? \\nLe service Azure OpenAI repose sur deux composants principaux : le portail Azure OpenAI et les API \\nAzure OpenAI. Le portail Azure OpenAI est une interface web qui permet de créer, de gérer et de \\nsurveiller les ressources du service Azure OpenAI, telles que les abonnements, les clés, les modèles, les \\npoints de terminaison et les requêtes. Les API Azure OpenAI sont des interfaces de programmation qui \\npermettent d'interagir avec les modèles d'IA du service Azure OpenAI, tels que GPT-3, Codex, DALL-E ou \\nCLIP. Les API Azure OpenAI sont accessibles via des requêtes HTTP ou des SDK dans différents langages \\nde programmation, tels que Python, Java, C#, Node.js ou Ruby. \\n\\nQuels sont les avantages du service Azure OpenAI ? \\nLe service Azure OpenAI présente plusieurs avantages, parmi lesquels : \\n\\n• \\n\\n• \\n\\n• \\n\\n• \\n\\nIl permet d'accéder à des modèles d'IA de pointe, capables de réaliser des tâches complexes et \\nvariées, telles que la génération de texte, la compréhension du langage naturel, la vision par \\nordinateur, la synthèse vocale ou la création de contenu multimédia. \\nIl offre une grande flexibilité, en permettant de choisir entre des modèles pré-entraînés ou \\npersonnalisés, et de les adapter aux besoins spécifiques de chaque projet ou domaine. \\nIl garantit une haute performance, en s'appuyant sur l'infrastructure cloud de Microsoft Azure, \\nqui assure une disponibilité, une scalabilité et une sécurité optimales. \\nIl facilite l'intégration, en proposant des API simples et des SDK dans différents langages de \\nprogrammation, qui permettent de connecter les modèles d'IA du service Azure OpenAI à des \\napplications et des scénarios existants ou nouveaux. \\n\\nQuels sont les cas d'utilisation du service Azure OpenAI ? \\nLe service Azure OpenAI peut être utilisé pour de nombreux cas d'utilisation, dans différents domaines et \\nsecteurs d'activité. Voici quelques exemples : \\n\\n\\x0c•  Dans le domaine de l'éducation, le service Azure OpenAI peut être utilisé pour créer des outils \\n\\npédagogiques, tels que des assistants de rédaction, des générateurs de questions, des \\ncorrecteurs automatiques ou des tuteurs virtuels. \\n\\n•  Dans le domaine de la santé, le service Azure OpenAI peut être utilisé pour créer des outils \\nd'aide au diagnostic, à la prescription, à la recherche ou à la prévention, en exploitant les \\ndonnées médicales et les connaissances scientifiques. \\n\\n•  Dans le domaine du divertissement, le service Azure OpenAI peut être utilisé pour créer des \\noutils de génération de contenu, tels que des scénarios, des dialogues, des personnages, des \\nmusiques ou des images. \\n\\n•  Dans le domaine du commerce, le service Azure OpenAI peut être utilisé pour créer des outils \\n\\nd'optimisation, tels que des recommandations personnalisées, des prévisions de ventes, des \\nanalyses de marché ou des chatbots. \\n\\nComment mettre en œuvre le service Azure OpenAI ? \\nPour mettre en œuvre le service Azure OpenAI, il faut suivre les étapes suivantes : \\n\\n•  Créer un compte Microsoft Azure, si ce n'est pas déjà fait, et se connecter au portail Azure \\n\\nOpenAI. \\n\\n•  Souscrire à un abonnement au service Azure OpenAI, en choisissant le niveau de tarification \\n\\nadapté à ses besoins. \\n\\n•  Créer une clé d'API, qui permettra d'authentifier les requêtes au service Azure OpenAI. \\n•  Choisir un modèle d'IA, parmi ceux proposés par le service Azure OpenAI, ou créer son propre \\nmodèle, en utilisant les outils de formation et de réglage fournis par le service Azure OpenAI. \\n\\n•  Créer un point de terminaison, qui permettra d'exposer le modèle d'IA sur le cloud ou en \\n\\npériphérie, et de le rendre accessible via une URL. \\n\\n•  Envoyer des requêtes au point de terminaison, en utilisant les API Azure OpenAI ou les SDK dans \\n\\nle langage de programmation de son choix. \\n\\n•  Analyser les résultats des requêtes, en utilisant les outils de suivi et de statistiques fournis par le \\n\\nportail Azure OpenAI. \\n\\n \\n\\x0c\", metadata={'source': 'pdftotext.txt'})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk settings  \n",
    "chunk_size = 200\n",
    "chunk_overlap = 30 # duplication des informations entre les vecteurs coupés "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(len(chunks)) # 35 vecteurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'•  Analyser les résultats des requêtes, en utilisant les outils de suivi et de statistiques fournis par le \\n\\nportail Azure OpenAI.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[34].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain.embeddings import OpenAIEmbeddings\n",
    "#import numpy as np\n",
    "#from langchain.vectorstores import Chroma\n",
    "\n",
    "\n",
    "#embeddings = OpenAIEmbeddings(deployment_id=\"text-embedding-ada-002\",chunk_size=\"100\")\n",
    "\n",
    "#docsearch = Chroma.from_documents(chunks, embeddings)\n",
    "#for splitted_document in chunks:\n",
    " #   Chroma.from_documents(documents=[splitted_document], embedding=embeddings)\n",
    "  #  time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.embeddings import OpenAIEmbeddings\n",
    "# from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# # TODO : The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`\n",
    "\n",
    "# embeddings = OpenAIEmbeddings(\n",
    "#     deployment_id=\"text-embedding-ada-002\",\n",
    "#     api_key=os.getenv(\"AZURE_OPENAI_KEY\"),\n",
    "#     chunk_size=chunk_size\n",
    "#     )\n",
    "\n",
    "# # faiss_vectorstore= FAISS.from_documents(documents, embeddings) # Limite de tokens !!! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enregistrer les chunks en tant que vecteur en utilisant faiss  \n",
    "pour résoudre le problème des tokens appels nombreux API, on découpe les chunks\n",
    " en blocs , sleep \n",
    "save le tout en local pour ne pas relancer à chaque fois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U langchain-community faiss-cpu langchain-openai tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `AzureOpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import AzureOpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings import AzureOpenAIEmbeddings\n",
    "\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment= \"text-embedding-ada-002\",\n",
    "    api_key=os.environ[\"AZURE_OPENAI_KEY\"],\n",
    ")\n",
    "\n",
    "# Iterate over each chunk of documents\n",
    "bloc = 40\n",
    "for i in range(0, len(chunks), bloc):\n",
    "    print(i)\n",
    "    faiss_vectorstore= FAISS.from_documents(chunks[i:i+bloc], embeddings)\n",
    "    sleep(10)\n",
    "    # print(faiss_vectorstore)\n",
    "\n",
    "faiss_retriever = faiss_vectorstore.as_retriever()\n",
    "faiss_vectorstore.save_local(\"faiss_index\")\n",
    "\n",
    "\n",
    "# TODO : optimiser la taille des chunks plus tard\n",
    "# LangChainDeprecationWarning: The class `AzureOpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import AzureOpenAIEmbeddings`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrievers are used to find relevant documents or passages that contain the answer to a given query.\n",
    "The system \"retrieves\" any documents that could be relevant in answering the question, and then passes those documents (along with the original question) to the language model for a \"generation\" step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La question est posée en utilisant le chat,\n",
    "toujours en utilisant la similarité sématique \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test utilisant LCEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "template_context1 =  \"\"\"answer the question only according to the context  : {context}\n",
    "            Question: {question}\n",
    "        \"\"\"\n",
    "template_context2= \"\"\"answer the question only according to the 429 th line and the lines just after of the file  : {context}\n",
    "            Question: {question}\n",
    "        \"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template_context1)\n",
    "\n",
    "retrieval_chain = (\n",
    "    {\"context\": faiss_retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"quelles sont  les certifications Associate? et quel est leur nombre? \"\n",
    "question = \"qu'est ce que le service Azure OpenAI ? \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Le service Azure OpenAI est présenté comme un outil puissant pour l'intelligence artificielle dans le premier document et est décrit en détail dans les autres documents.\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RetrievalQA\n",
    "from langchain.chains import RetrievalQA\n",
    "import faiss\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "# TODO : trouver comment load faiss_retriever = index = faiss.read_index( \"faiss_index\") index.as_retriever()\n",
    "\n",
    "qa_stuff = RetrievalQA.from_chain_type( #creer une chain pour répondre à la question\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", # tous les documents d'un coup et les passe au LLM\n",
    "    retriever=faiss_retriever, \n",
    "    return_source_documents= True,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'Pourquoi utiliser Azure OpenAI ?',\n",
       " 'result': \"Le service Azure OpenAI présente plusieurs avantages pour l'intelligence artificielle, tels que la possibilité de créer des modèles en utilisant les outils de formation et de réglage fournis par le service, ainsi que des caractéristiques telles que la puissance et la flexibilité de la plateforme. Cependant, les raisons spécifiques pour lesquelles une personne ou une entreprise peut choisir d'utiliser Azure OpenAI peuvent varier en fonction de leurs besoins et objectifs en matière d'IA.\",\n",
       " 'source_documents': [Document(page_content='Quels sont les avantages du service Azure OpenAI ? \\nLe service Azure OpenAI présente plusieurs avantages, parmi lesquels : \\n\\n• \\n\\n• \\n\\n• \\n\\n•', metadata={'source': 'pdftotext.txt'}),\n",
       "  Document(page_content=\"Le service Azure OpenAI : un outil puissant \\npour l'intelligence artificielle \\n\\nUn document qui présente le service Azure OpenAI, ses caractéristiques, ses bénéfices et \\nses applications.\", metadata={'source': 'pdftotext.txt'}),\n",
       "  Document(page_content='modèle, en utilisant les outils de formation et de réglage fournis par le service Azure OpenAI.', metadata={'source': 'pdftotext.txt'}),\n",
       "  Document(page_content='Comment mettre en œuvre le service Azure OpenAI ? \\nPour mettre en œuvre le service Azure OpenAI, il faut suivre les étapes suivantes :', metadata={'source': 'pdftotext.txt'})]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "question = \"what is a voucher code?\"\n",
    "question = \"Pourquoi utiliser Azure OpenAI ?\"\n",
    "result = qa_stuff.invoke(question)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Préparer un golden dataset \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\"Quel est le nombre  des certifications Azure ? \",\"Quel est le nombre des certifications Associate?\", \"qu est ce qu un voucher en un mot\"]\n",
    "ground_truths = [[\"Il y a 15 certifications Azure\"],\n",
    "                 [\"Il y a 10 certifications Assiociate mentionnees\"], \n",
    "                 [\"une promotion\"]\n",
    "                ]\n",
    "contexts = []\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "\"Qu'est-ce que le service Azure OpenAI ?\",\n",
    "\"Comment fonctionne le service Azure OpenAI ?\",\n",
    "\"Quels sont les avantages du service Azure OpenAI ?\",\n",
    "\"Quels sont les cas d'utilisation du service Azure OpenAI ?\",\n",
    "\"Comment mettre en œuvre le service Azure OpenAI ?\",\n",
    "\"Quels sont les composants principaux du service Azure OpenAI ?\",\n",
    "\"Quels sont les modèles d'IA accessibles via le service Azure OpenAI ?\",\n",
    "\"Quelles tâches les modèles d'IA du service Azure OpenAI peuvent-ils accomplir ?\",\n",
    "\"Quelle est l'infrastructure sur laquelle repose le service Azure OpenAI ?\",\n",
    "\"Quels outils le service Azure OpenAI fournit-il pour la formation et le réglage des modèles d'IA ?\"\n",
    "]\n",
    "\n",
    "answers = [  \n",
    "\"Le service Azure OpenAI est un service cloud qui permet aux développeurs et aux chercheurs d'accéder à la plateforme OpenAI, une initiative qui vise à créer une intelligence artificielle générale (AGI) bénéfique pour l'humanité. Il offre la possibilité d'utiliser des modèles d'IA pré-entraînés ou personnalisés, de les déployer sur le cloud ou en périphérie, et de les intégrer à des applications et des scénarios variés.\",   \n",
    "\"Le service Azure OpenAI repose sur deux composants principaux : le portail Azure OpenAI et les API Azure OpenAI. Le portail est une interface web qui permet de créer, de gérer et de surveiller les ressources du service, tandis que les API sont des interfaces de programmation qui permettent d'interagir avec les modèles d'IA du service.\",   \n",
    "\"Le service Azure OpenAI présente plusieurs avantages, parmi lesquels l'accès à des modèles d'IA de pointe, une grande flexibilité, une haute performance grâce à l'infrastructure cloud de Microsoft Azure, et une facilité d'intégration grâce à des API simples et des SDK dans différents langages de programmation.\",   \n",
    "\"Le service Azure OpenAI peut être utilisé pour de nombreux cas d'utilisation, dans différents domaines et secteurs d'activité, tels que l'éducation, la santé, le divertissement et le commerce.\",   \n",
    "\"Pour mettre en œuvre le service Azure OpenAI, il faut créer un compte Microsoft Azure, souscrire à un abonnement au service, créer une clé d'API, choisir ou créer un modèle d'IA, créer un point de terminaison, envoyer des requêtes au point de terminaison, et analyser les résultats des requêtes.\",   \n",
    "\"Les composants principaux du service Azure OpenAI sont le portail Azure OpenAI et les API Azure OpenAI.\",   \n",
    "\"Les modèles d'IA accessibles via le service Azure OpenAI incluent GPT-3, Codex, DALL-E et CLIP.\",   \n",
    "\"Les modèles d'IA du service Azure OpenAI peuvent réaliser des tâches complexes et variées, telles que la génération de texte, la compréhension du langage naturel, la vision par ordinateur, la synthèse vocale ou la création de contenu multimédia.\",   \n",
    "\"L'infrastructure sur laquelle repose le service Azure OpenAI est l'infrastructure cloud de Microsoft Azure.\",   \n",
    "\"Le service Azure OpenAI fournit des outils de formation et de réglage pour créer son propre modèle d'IA.\"\n",
    "]  \n",
    "\n",
    "ground_truths = [  \n",
    "[\"Le service Azure OpenAI est un service cloud qui permet aux développeurs et aux chercheurs d'accéder à la plateforme OpenAI\", \"Le service Azure OpenAI offre la possibilité d'utiliser des modèles d'IA pré-entraînés ou personnalisés, de les déployer sur le cloud ou en périphérie, et de les intégrer à des applications et des scénarios variés.\"],  \n",
    "[\"Le service Azure OpenAI repose sur deux composants principaux : le portail Azure OpenAI et les API Azure OpenAI.\", \"Le portail Azure OpenAI est une interface web qui permet de créer, de gérer et de surveiller les ressources du service Azure OpenAI, telles que les abonnements, les clés, les modèles, les points de terminaison et les requêtes.\"],  \n",
    "[\"Le service Azure OpenAI présente plusieurs avantages, parmi lesquels :\", \"Il garantit une haute performance, en s'appuyant sur l'infrastructure cloud de Microsoft Azure, qui assure une disponibilité, une scalabilité et une sécurité optimales.\"],  \n",
    "[\"Le service Azure OpenAI peut être utilisé pour de nombreux cas d'utilisation, dans différents domaines et secteurs d'activité.\", \"Dans le domaine du commerce, le service Azure OpenAI peut être utilisé pour créer des outils d'optimisation, tels que des recommandations personnalisées, des prévisions de ventes, des analyses de marché ou des chatbots.\"],  \n",
    "[\"Pour mettre en œuvre le service Azure OpenAI, il faut suivre les étapes suivantes :\", \"Analyser les résultats des requêtes, en utilisant les outils de suivi et de statistiques fournis par le portail Azure OpenAI.\"],  \n",
    "[\"Le service Azure OpenAI repose sur deux composants principaux : le portail Azure OpenAI et les API Azure OpenAI.\", \"Les API Azure OpenAI sont des interfaces de programmation qui permettent d'interagir avec les modèles d'IA du service Azure OpenAI, tels que GPT-3, Codex, DALL-E ou CLIP.\"],  \n",
    "[\"Les API Azure OpenAI sont des interfaces de programmation qui permettent d'interagir avec les modèles d'IA du service Azure OpenAI\", \"tels que GPT-3, Codex, DALL-E ou CLIP.\"],  \n",
    "[\"Il permet d'accéder à des modèles d'IA de pointe, capables de réaliser des tâches complexes et variées\", \"telles que la génération de texte, la compréhension du langage naturel, la vision par ordinateur, la synthèse vocale ou la création de contenu multimédia.\"],  \n",
    "[\"Il garantit une haute performance, en s'appuyant sur l'infrastructure cloud de Microsoft Azure\", \"qui assure une disponibilité, une scalabilité et une sécurité optimales.\"],  \n",
    "[\"Choisir un modèle d'IA, parmi ceux proposés par le service Azure OpenAI\", \"ou créer son propre modèle, en utilisant les outils de formation et de réglage fournis par le service Azure OpenAI.\"]  \n",
    "]\n",
    "\n",
    "results = []\n",
    "contexts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "for query in questions:\n",
    "    result = qa_stuff({\"query\": query})\n",
    "    results.append(result['result']) \n",
    "    sources = result[\"source_documents\"]\n",
    "    contents = [source.page_content for source in sources]  \n",
    "\n",
    "    processed_contents = []\n",
    "    for content in contents:\n",
    "        if isinstance(content, str):  \n",
    "            processed_content = content.split('.')[:2]  \n",
    "            processed_contents.extend(processed_content)  \n",
    "        else:\n",
    "            print(f\"Expected a string but got {type(content)}\") \n",
    "    contexts.append(processed_contents)\n",
    "\n",
    "print(len(questions))\n",
    "print(len(ground_truths))\n",
    "print(len(results))\n",
    "print(len(contexts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            question  \\\n",
      "0            Qu'est-ce que le service Azure OpenAI ?   \n",
      "1       Comment fonctionne le service Azure OpenAI ?   \n",
      "2  Quels sont les avantages du service Azure Open...   \n",
      "3  Quels sont les cas d'utilisation du service Az...   \n",
      "4  Comment mettre en œuvre le service Azure OpenAI ?   \n",
      "\n",
      "                                              answer  \\\n",
      "0  Le service Azure OpenAI est un outil pour l'in...   \n",
      "1  Le service Azure OpenAI repose sur deux compos...   \n",
      "2  Le texte mentionne que le service Azure OpenAI...   \n",
      "3  Le document ne fournit pas de détails sur les ...   \n",
      "4  Pour mettre en œuvre le service Azure OpenAI, ...   \n",
      "\n",
      "                                            contexts  \\\n",
      "0  [Le service Azure OpenAI : un outil puissant \\...   \n",
      "1  [Comment fonctionne le service Azure OpenAI ? ...   \n",
      "2  [Quels sont les avantages du service Azure Ope...   \n",
      "3  [Quels sont les avantages du service Azure Ope...   \n",
      "4  [Comment mettre en œuvre le service Azure Open...   \n",
      "\n",
      "                                        ground_truth  \n",
      "0  [Le service Azure OpenAI est un service cloud ...  \n",
      "1  [Le service Azure OpenAI repose sur deux compo...  \n",
      "2  [Le service Azure OpenAI présente plusieurs av...  \n",
      "3  [Le service Azure OpenAI peut être utilisé pou...  \n",
      "4  [Pour mettre en œuvre le service Azure OpenAI,...  \n"
     ]
    }
   ],
   "source": [
    "# Creating a DataFrame\n",
    "data = {\n",
    "    'question': questions,\n",
    "    'answer': results,\n",
    "    'contexts': contexts,\n",
    "    'ground_truth': ground_truths\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df.head())\n",
    "\n",
    "# Save as .csv\n",
    "df.to_csv('ragas_aoai_dataset.csv', index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://blog.cellenza.com/securite-2/utiliser-azure-openai-langchain-et-ragas-pour-la-classification-des-documents-confidentiels-et-la-protection-des-donnees-sensibles/ \n",
    "\n",
    "# from ragas.testset.generator import TestsetGenerator\n",
    "# from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "\n",
    "# testset = generator.generate_with_langchain_docs(documents, test_size=10)\n",
    "\n",
    "# print(testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ragas datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "import pyarrow as py\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as ds\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import  faithfulness, answer_relevancy, context_recall, context_precision #, harmfulness\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from datasets import Dataset, Features, Sequence, Value, load_dataset\n",
    "\n",
    "\n",
    "# Définir le dataset explicitement \n",
    "features = Features({\n",
    "    \"question\": Value(\"string\"),\n",
    "    \"answer\": Value(\"string\"),\n",
    "    \"contexts\": Sequence(Value(\"string\")),\n",
    "    \"ground_truth\": Value(\"string\")\n",
    "})\n",
    "dataset = Dataset.from_dict(df, features=features)\n",
    "\n",
    "# ragas_llm = LangchainLLMWrapper(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "\n",
    "# wrapper around azure_model \n",
    "ragas_azure_model = LangchainLLMWrapper(llm)\n",
    "# patch the new RagasLLM instance\n",
    "faithfulness.llm = ragas_azure_model\n",
    "answer_relevancy.llm = ragas_azure_model\n",
    "context_precision.llm = ragas_azure_model\n",
    "context_recall.llm = ragas_azure_model\n",
    "# harmfulness.llm = ragas_azure_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n",
      "c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py:336: UserWarning: If you have openai>=1.0.0 installed and are using Azure, please use the `AzureOpenAIEmbeddings` class.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# init and change the embeddings\n",
    "# only for answer_relevancy\n",
    "azure_embeddings = OpenAIEmbeddings(\n",
    "    # deployment=\"text-embedding-ada-002\",\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    openai_api_base=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    openai_api_key=os.getenv(\"AZURE_OPENAI_KEY\"),\n",
    "    openai_api_type=\"azure\"\n",
    ")\n",
    "answer_relevancy.embeddings = azure_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 78, in _aresults\n",
      "    r = await future\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 37, in sema_coro\n",
      "    return await coro\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 111, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 125, in ascore\n",
      "    raise e\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 121, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 167, in _ascore\n",
      "    return self._calculate_score(answers, row)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 139, in _calculate_score\n",
      "    cosine_sim = self.calculate_similarity(question, gen_questions)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 113, in calculate_similarity\n",
      "    question_vec = np.asarray(self.embeddings.embed_query(question)).reshape(1, -1)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 697, in embed_query\n",
      "    return self.embed_documents([text])[0]\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 668, in embed_documents\n",
      "    return self._get_len_safe_embeddings(texts, engine=engine)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 494, in _get_len_safe_embeddings\n",
      "    response = embed_with_retry(\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 116, in embed_with_retry\n",
      "    return embeddings.client.create(**kwargs)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\resources\\embeddings.py\", line 114, in create\n",
      "    return self._post(\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\_base_client.py\", line 1240, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\_base_client.py\", line 921, in request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\_base_client.py\", line 1020, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}\n",
      "Evaluating:   2%|▎         | 1/40 [00:01<00:53,  1.37s/it]Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 78, in _aresults\n",
      "    r = await future\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 37, in sema_coro\n",
      "    return await coro\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 111, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 125, in ascore\n",
      "    raise e\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 121, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 167, in _ascore\n",
      "    return self._calculate_score(answers, row)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 139, in _calculate_score\n",
      "    cosine_sim = self.calculate_similarity(question, gen_questions)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 113, in calculate_similarity\n",
      "    question_vec = np.asarray(self.embeddings.embed_query(question)).reshape(1, -1)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 697, in embed_query\n",
      "    return self.embed_documents([text])[0]\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 668, in embed_documents\n",
      "    return self._get_len_safe_embeddings(texts, engine=engine)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 494, in _get_len_safe_embeddings\n",
      "    response = embed_with_retry(\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 116, in embed_with_retry\n",
      "    return embeddings.client.create(**kwargs)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\resources\\embeddings.py\", line 114, in create\n",
      "    return self._post(\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\_base_client.py\", line 1240, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\_base_client.py\", line 921, in request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\_base_client.py\", line 1020, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}\n",
      "Evaluating:   5%|▌         | 2/40 [00:01<00:28,  1.33it/s]Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 78, in _aresults\n",
      "    r = await future\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 37, in sema_coro\n",
      "    return await coro\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 111, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 125, in ascore\n",
      "    raise e\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 121, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 167, in _ascore\n",
      "    return self._calculate_score(answers, row)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 139, in _calculate_score\n",
      "    cosine_sim = self.calculate_similarity(question, gen_questions)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 113, in calculate_similarity\n",
      "    question_vec = np.asarray(self.embeddings.embed_query(question)).reshape(1, -1)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 697, in embed_query\n",
      "    return self.embed_documents([text])[0]\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 668, in embed_documents\n",
      "    return self._get_len_safe_embeddings(texts, engine=engine)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 494, in _get_len_safe_embeddings\n",
      "    response = embed_with_retry(\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 116, in embed_with_retry\n",
      "    return embeddings.client.create(**kwargs)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\resources\\embeddings.py\", line 114, in create\n",
      "    return self._post(\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\_base_client.py\", line 1240, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\_base_client.py\", line 921, in request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\_base_client.py\", line 1020, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}\n",
      "Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 78, in _aresults\n",
      "    r = await future\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 37, in sema_coro\n",
      "    return await coro\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 111, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 125, in ascore\n",
      "    raise e\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 121, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 167, in _ascore\n",
      "    return self._calculate_score(answers, row)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 139, in _calculate_score\n",
      "    cosine_sim = self.calculate_similarity(question, gen_questions)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 113, in calculate_similarity\n",
      "    question_vec = np.asarray(self.embeddings.embed_query(question)).reshape(1, -1)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 697, in embed_query\n",
      "    return self.embed_documents([text])[0]\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 668, in embed_documents\n",
      "    return self._get_len_safe_embeddings(texts, engine=engine)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 494, in _get_len_safe_embeddings\n",
      "    response = embed_with_retry(\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 116, in embed_with_retry\n",
      "    return embeddings.client.create(**kwargs)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\resources\\embeddings.py\", line 114, in create\n",
      "    return self._post(\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\_base_client.py\", line 1240, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\_base_client.py\", line 921, in request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\_base_client.py\", line 1020, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}\n",
      "Evaluating:  12%|█▎        | 5/40 [00:02<00:11,  3.00it/s]Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 78, in _aresults\n",
      "    r = await future\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 37, in sema_coro\n",
      "    return await coro\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 111, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 125, in ascore\n",
      "    raise e\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 121, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 167, in _ascore\n",
      "    return self._calculate_score(answers, row)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 139, in _calculate_score\n",
      "    cosine_sim = self.calculate_similarity(question, gen_questions)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 113, in calculate_similarity\n",
      "    question_vec = np.asarray(self.embeddings.embed_query(question)).reshape(1, -1)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 697, in embed_query\n",
      "    return self.embed_documents([text])[0]\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 668, in embed_documents\n",
      "    return self._get_len_safe_embeddings(texts, engine=engine)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 494, in _get_len_safe_embeddings\n",
      "    response = embed_with_retry(\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 116, in embed_with_retry\n",
      "    return embeddings.client.create(**kwargs)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\resources\\embeddings.py\", line 114, in create\n",
      "    return self._post(\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\_base_client.py\", line 1240, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\_base_client.py\", line 921, in request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\_base_client.py\", line 1020, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}\n",
      "Evaluating:  20%|██        | 8/40 [00:02<00:08,  3.90it/s]Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 78, in _aresults\n",
      "    r = await future\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 37, in sema_coro\n",
      "    return await coro\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 111, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 125, in ascore\n",
      "    raise e\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 121, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 167, in _ascore\n",
      "    return self._calculate_score(answers, row)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 139, in _calculate_score\n",
      "    cosine_sim = self.calculate_similarity(question, gen_questions)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 113, in calculate_similarity\n",
      "    question_vec = np.asarray(self.embeddings.embed_query(question)).reshape(1, -1)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 697, in embed_query\n",
      "    return self.embed_documents([text])[0]\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 668, in embed_documents\n",
      "    return self._get_len_safe_embeddings(texts, engine=engine)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 494, in _get_len_safe_embeddings\n",
      "    response = embed_with_retry(\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 116, in embed_with_retry\n",
      "    return embeddings.client.create(**kwargs)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\resources\\embeddings.py\", line 114, in create\n",
      "    return self._post(\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\_base_client.py\", line 1240, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\_base_client.py\", line 921, in request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\_base_client.py\", line 1020, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}\n",
      "Evaluating:  32%|███▎      | 13/40 [00:03<00:05,  5.32it/s]Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 78, in _aresults\n",
      "    r = await future\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 37, in sema_coro\n",
      "    return await coro\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 111, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 125, in ascore\n",
      "    raise e\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 121, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 167, in _ascore\n",
      "    return self._calculate_score(answers, row)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 139, in _calculate_score\n",
      "    cosine_sim = self.calculate_similarity(question, gen_questions)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 113, in calculate_similarity\n",
      "    question_vec = np.asarray(self.embeddings.embed_query(question)).reshape(1, -1)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 697, in embed_query\n",
      "    return self.embed_documents([text])[0]\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 668, in embed_documents\n",
      "    return self._get_len_safe_embeddings(texts, engine=engine)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 494, in _get_len_safe_embeddings\n",
      "    response = embed_with_retry(\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 116, in embed_with_retry\n",
      "    return embeddings.client.create(**kwargs)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\resources\\embeddings.py\", line 114, in create\n",
      "    return self._post(\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\_base_client.py\", line 1240, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\_base_client.py\", line 921, in request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\_base_client.py\", line 1020, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}\n",
      "Evaluating:  38%|███▊      | 15/40 [00:03<00:04,  5.97it/s]Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 78, in _aresults\n",
      "    r = await future\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 37, in sema_coro\n",
      "    return await coro\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 111, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 125, in ascore\n",
      "    raise e\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 121, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 167, in _ascore\n",
      "    return self._calculate_score(answers, row)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 139, in _calculate_score\n",
      "    cosine_sim = self.calculate_similarity(question, gen_questions)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 113, in calculate_similarity\n",
      "    question_vec = np.asarray(self.embeddings.embed_query(question)).reshape(1, -1)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 697, in embed_query\n",
      "    return self.embed_documents([text])[0]\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 668, in embed_documents\n",
      "    return self._get_len_safe_embeddings(texts, engine=engine)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 494, in _get_len_safe_embeddings\n",
      "    response = embed_with_retry(\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 116, in embed_with_retry\n",
      "    return embeddings.client.create(**kwargs)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\resources\\embeddings.py\", line 114, in create\n",
      "    return self._post(\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\_base_client.py\", line 1240, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\_base_client.py\", line 921, in request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\_base_client.py\", line 1020, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}\n",
      "Evaluating:  45%|████▌     | 18/40 [00:05<00:06,  3.65it/s]Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 78, in _aresults\n",
      "    r = await future\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 37, in sema_coro\n",
      "    return await coro\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 111, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 125, in ascore\n",
      "    raise e\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 121, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 167, in _ascore\n",
      "    return self._calculate_score(answers, row)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 139, in _calculate_score\n",
      "    cosine_sim = self.calculate_similarity(question, gen_questions)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 113, in calculate_similarity\n",
      "    question_vec = np.asarray(self.embeddings.embed_query(question)).reshape(1, -1)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 697, in embed_query\n",
      "    return self.embed_documents([text])[0]\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 668, in embed_documents\n",
      "    return self._get_len_safe_embeddings(texts, engine=engine)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 494, in _get_len_safe_embeddings\n",
      "    response = embed_with_retry(\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 116, in embed_with_retry\n",
      "    return embeddings.client.create(**kwargs)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\resources\\embeddings.py\", line 114, in create\n",
      "    return self._post(\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\_base_client.py\", line 1240, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\_base_client.py\", line 921, in request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\_base_client.py\", line 1020, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}\n",
      "Evaluating:  62%|██████▎   | 25/40 [00:06<00:02,  5.45it/s]Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 78, in _aresults\n",
      "    r = await future\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 37, in sema_coro\n",
      "    return await coro\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\executor.py\", line 111, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 125, in ascore\n",
      "    raise e\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\base.py\", line 121, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 167, in _ascore\n",
      "    return self._calculate_score(answers, row)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 139, in _calculate_score\n",
      "    cosine_sim = self.calculate_similarity(question, gen_questions)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 113, in calculate_similarity\n",
      "    question_vec = np.asarray(self.embeddings.embed_query(question)).reshape(1, -1)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 697, in embed_query\n",
      "    return self.embed_documents([text])[0]\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 668, in embed_documents\n",
      "    return self._get_len_safe_embeddings(texts, engine=engine)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 494, in _get_len_safe_embeddings\n",
      "    response = embed_with_retry(\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py\", line 116, in embed_with_retry\n",
      "    return embeddings.client.create(**kwargs)\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\resources\\embeddings.py\", line 114, in create\n",
      "    return self._post(\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\_base_client.py\", line 1240, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\_base_client.py\", line 921, in request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\paul.peton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\_base_client.py\", line 1020, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}\n",
      "Evaluating: 100%|██████████| 40/40 [00:14<00:00,  2.70it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'faithfulness': 0.8738, 'answer_relevancy': 0.9586, 'context_recall': 0.7875, 'context_precision': 0.8687}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_result = evaluate(\n",
    "       dataset=dataset,\n",
    "       metrics=[faithfulness, answer_relevancy,  context_recall, context_precision],\n",
    "       llm=ragas_azure_model,\n",
    "       raise_exceptions=False,\n",
    ")\n",
    "\n",
    "evaluate_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
